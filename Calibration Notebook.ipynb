{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac53736-3a00-443d-9120-a37ee038a592",
   "metadata": {},
   "source": [
    "# MaSim Country Calibration\n",
    "\n",
    "This note book is used for running the country calibration processes for eventual use in the experimental simulation process where various response strategies are modeled. This notebook and the accompanying toolkit was developed by [James Brodovsky](https://github.com/jbrodovsky) and Sarit Adhikari as part of the calibration efforts for Burkino Faso and Mozambique in 2025.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Installation\n",
    "\n",
    "Installation is in two parts. First, clone this repository to your local machine. Second, install the `masim_analysis` package into a virtual environment, e.g., using `pip` or `uv`.\n",
    "\n",
    "```bash\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "If you are using `uv` (and you should!) install via\n",
    "\n",
    "```bash\n",
    "uv pip install -e .\n",
    "```\n",
    "\n",
    "This will install the package in an editable mode, allowing you to make changes to the code and have them reflected in your local environment. If for some reason you are not seeing the changes reflected, try re-installing the package locally using the above commands again.\n",
    "\n",
    "Alternatively, and this should be done only if you are developing additional toolbox features, you can run this notebook from the root (top level) directory and direct import calls to the toolbox via \n",
    "\n",
    "```python \n",
    "from src.masim_analysis import *\n",
    "```\n",
    "where `*` is the specific module you are interested in.\n",
    "\n",
    "### Package and repo structure\n",
    "\n",
    "Please make note of the following directory structures: `conf` and `data`. These are the primary two directories for experimental country data and configuration files. The `conf` directory contains the configuration (.yml) files for the simulation, while the `data` directory contains the data files used in the simulation (typically raster `.asc` files).\n",
    "\n",
    "Each of these folders is organized by country. For example, if you are working with Mozambique, which we abrieviate as `moz`, the directory structure would look like this:\n",
    "\n",
    "```\n",
    "data/\n",
    "    moz/\n",
    "        calibration/\n",
    "        ...\n",
    "conf/\n",
    "    moz/\n",
    "        calibration/\n",
    "        ...\n",
    "```\n",
    "Additionally the templates folder contains the template files for the configuration files. These are used to generate the .yml files for the simulation. This templating system is gradually being phased out in favor of a more structured approach using Python data classes in the `configure` module. Ultimately, this package communicates with the MaSim simulation through a `.yml` configuration file that can still be created manually if you desire.\n",
    "\n",
    "### Style guide\n",
    "\n",
    "This package follows pretty strict styling guidelines for clarity and consistency. The code is formatted and linted using `ruff` and `pyright`. In particular, `pyright` is configured in standard mode, which means that it will check for type errors and other issues in the code. Warnings and errors reported by either of these tools should be addressed before submitting a pull request. In some cases, this may result in somewhat more verbose code, but it is done to ensure that the code is clear and easy to understand. The goal is to make the code as readable and maintainable as possible.\n",
    "\n",
    "### How to use this notebook\n",
    "\n",
    "This notebook should be thought of as a structured interactive prompt that guides you through the process of calibrating and validating a country. The code is generally organized into sections that can be run independently. The notebook breaks up individual workflows by using markdown headings and note blocks. Due to the some what long-running nature of the tasks that calibration and validation involve, you'll sometimes have to wait and shutdown the kernel the notebook is running on and pick up where you left off another time (or wait for them to complete on a cluster).\n",
    "\n",
    "To that end, this notebook is designed to make things organized but also segmented. Calibration constants (name, population, etc.) are handled through the `CountryParams` dataclass found in `masim_analysis.configure` and the initial setup workflow has you define this and save it to a file. Module and library imports should be handled in the workflow segment you are currently working on. It is recommend (for speed of execution) to separate out imports from code execution to save time on re-importing.\n",
    "\n",
    "Workflows are generally separated by a horizontal rule and when you reach one, you can consider that segment complete.\n",
    "\n",
    "--- \n",
    "\n",
    "## A note on calibration efforts\n",
    "\n",
    "The primary calibration point is to relate the beta parameter (rate of infection/biting) with the actual reported prevelence, given the population size of a given map pixel and treatment access rate. This involves a few steps. As a preliminary step, obtain the relevant raster files that contain population data, district mapping values, treatment access, and prevalence (pfpr2-10, or a similar name) and place it under `data/<country>`. Fictitious calibration data will be stored under `data/<country>/calibration`. Create a new branch in the git repository for the calibration process. This is important to keep track of changes and to avoid conflicts with the main branch. The main branch should be reserved as a branching off point for strategy and treatment analysis or new calibration efforts. The workflow there should be to branch off from the main branch, implement the strategies and treatments, and then merge back in any useful changes. Strategy and treament analysis shouldn't be a main contribution to the main branch. The calibration branch name should be descriptive and include the country name, e.g., `calibration-moz`.\n",
    "\n",
    "Calibration then occurs in two phases and should be done on a separate git branch. The first phase is generating the simulated data for beta calibration. This creates the fictitious configuration and data files. This concludes with several command and job files to be run on a cluster. At the moment this is configured to work on Temple University's OwlsNest cluster, but the resulting `*_cmds.txt` files simply contain a list of shell commands that execute the simulation and should be generalizable to whatever parallel computing cluster system you are using.\n",
    "\n",
    "The second phase is started when the batch processing is completed and downloaded locally to the `output/<country>/calibration` directory. These files are then summarized and the prevalence and beta values are fit using a log-sigmoid curve fit when broken down by pixel population and treatment access rate. These fits are then used to generate the beta map for eventual use in the experimental simulation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d72e28",
   "metadata": {},
   "source": [
    "## Basic parameters\n",
    "\n",
    "There are a few country specific parameters that we need to develop and organize. These parameters are frequently used and are listed below:\n",
    "\n",
    "- `name`: The country code name. Usually two or three letters. Examples: `moz` for Mozambique, `bf` for Burkino Faso, `rwa` for Rwanda, `tz` for Tanzania, etc.\n",
    "- `birth_rate`: The birth rate of the country. This is used to calculate the population growth rate. This is usually a constant value for the country. Data is typically given in births per 1000 people. Normalize that data to a decimal value.\n",
    "- `target_population`: The population of the country in the calibration target year.\n",
    "- `initial_age_structure`: The initial age bins of the country. \n",
    "- `age_distribution`: The age distribution of the country as percentages corresponding to the age bins.\n",
    "- `death_rate`: The death rate of the country corresponding to the age bins.\n",
    "- `access_rates`: the treatment access rates for the country. This is determined by the unique values in the raster file typically called `<name>_treatementseeking.asc`\n",
    "- `target_case_count`: The target year case count for the country.\n",
    "- `lower_bound_case_count`: The lower bound of the target year case count for the country.\n",
    "- `upper_bound_case_count`: The upper bound of the target year case count for the country.\n",
    "\n",
    "Some of these are configuration parameters that are fed into the `MaSim` simulation. Others are simply descriptive (e.g. `name`) are are used to organize files. It is useful to have these parameters as variables. Once we have gone through the initial tuning, these parameters are stable and it is useful to simply keep them in a Python source file and import them as neccessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7c299d",
   "metadata": {},
   "source": [
    "### Directory Setup\n",
    "\n",
    "This repo is setup assuming a specific file directory structure. The main directories of interest are `data` and `conf`. Each of these directories should contain a subdirectory for the country you are working with. These directories and subdirectories are based off of the name of the country. We will create them now.\n",
    "\n",
    "**What is the long-form name of the country your are working with?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc251d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_name = \"Mozambique\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69362cce",
   "metadata": {},
   "source": [
    "**What is the short-form country code?** (usually two or three letters, e.g., `moz` for Mozambique, `bf` for Burkino Faso, `rwa` for Rwanda, `tz` for Tanzania, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd087fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"moz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afd971f",
   "metadata": {},
   "source": [
    "Now we'll do a basic file system configuration for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad969d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from masim_analysis import configure\n",
    "\n",
    "configure.setup_directories(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b25b57",
   "metadata": {},
   "source": [
    "After this initial folder configuration is set up copy the data from the country folder on the dropbox into `data/<name>`. This should be a collection of raster (`.asc`) and `.csv` files plus some other miscellaneous files and spreadsheets. At a minimum these should include: \n",
    "- `<name>_districts.asc` (mapping of individual raster pixels to larger scale provinces or health districts)\n",
    "- `<name>_population.asc` (calibration year population)\n",
    "- `<name>_initialpopulation.asc` (backed out initial population)\n",
    "- `<name>_pfpr210.asc` (pfpr210 raster)\n",
    "- `<name>_traveltime.asc` (travel time raster)\n",
    "- `<name>_treatmentseeking.asc` (treatment seeking raster)\n",
    "- `<name>_drugdistribution.csv` (drug distribution data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb02686",
   "metadata": {},
   "source": [
    "### Raster data exploration\n",
    "\n",
    "It can be useful to explore the raster data to get a sense of the population distribution, treatment access rates, and prevalence, as well as to familiarize yourself with this codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47944fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from masim_analysis import utils\n",
    "\n",
    "districts, _ = utils.read_raster(os.path.join(\"data\", name, f\"{name}_districts.asc\"))\n",
    "population, _ = utils.read_raster(os.path.join(\"data\", name, f\"{name}_initialpopulation.asc\"))\n",
    "prevalence, _ = utils.read_raster(os.path.join(\"data\", name, f\"{name}_pfpr210.asc\"))\n",
    "district_names = pd.read_csv(os.path.join(\"data\", name, f\"{name}_mapping.csv\"), index_col=\"ID\")\n",
    "names = district_names.to_dict()[district_names.columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_fig = utils.plot_districts(districts, names, \"Mozambique\", fig_size=(12, 6), loc=\"lower right\")\n",
    "pop_fig = utils.plot_population(population, \"Mozambique\", fig_size=(12, 6), population_upper_limit=5000)\n",
    "pfpr_plot = utils.plot_prevalence(prevalence, \"Mozambique\", fig_size=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778892ee",
   "metadata": {},
   "source": [
    "### Drug distribution rates\n",
    "\n",
    "Each country has a different drug distribution rate. This will factor into the calibration file. Outside of this notebook you should process whatever raw or summarized data you have into a year by year table where year row is a year and each column is a drug with its distribution rate. Read in the raw file and process it to remove any unnecessary columns or rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bcc7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_distribution = pd.read_csv(\n",
    "    os.path.join(\"data\", name, f\"{name}_drugdistribution.csv\"), index_col=0, na_values=\"-99\"\n",
    ")\n",
    "drug_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809cbe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any columns that are all NaN\n",
    "drug_distribution.dropna(axis=1, how=\"all\", inplace=True)\n",
    "# Drop any rows that are all NaN\n",
    "drug_distribution.dropna(axis=0, how=\"all\", inplace=True)\n",
    "# Fill any remaining NaN values with 0\n",
    "drug_distribution.fillna(0, inplace=True)\n",
    "# convert all columns to lowercase\n",
    "drug_distribution.columns = [col.lower() for col in drug_distribution.columns]\n",
    "# Convert to percentages [0, 1]\n",
    "drug_distribution = drug_distribution.div(100)\n",
    "drug_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72a5c4c",
   "metadata": {},
   "source": [
    "As a sanity check, the sum of the distribution rates should equal 1.0 for each year, but it might not be. The percentages given aren't necessarily exclusive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba9d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_distribution[\"sum\"] = drug_distribution.sum(axis=1)\n",
    "drug_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3e1010",
   "metadata": {},
   "source": [
    "If in the case that the sum of the distribution rates does not equal 1.0, then you will need to normalize the values so that they do for the purposes of the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d004d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Need to actually confirm this. I'm not sure if the percentages MUST add up to one\n",
    "\n",
    "# drug_distribution.rename(columns={\"other\": \"al\"}, inplace=True)\n",
    "# drug_distribution.drop(columns=[\"sum\", \"aspirin\"], inplace=True)\n",
    "# drug_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036a87c5",
   "metadata": {},
   "source": [
    "We then relate this to the `DRUG_DB` and `THERAPY_DB` dictionary in `masim_analysis.configure`. Note that the drug distribution may be either a single drug or a combination of drugs. For instance, `al` is a combination of artemether and lumefantrine. This would correspond to a *therapy* entry in `THERAPY_DB` that uses both the *artemether* and *lumefantrine* drugs from `DRUG_DB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68b4dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from masim_analysis.configure import DRUG_DB, THERAPY_DB\n",
    "\n",
    "print(\"DRUGS:\")\n",
    "for idx in DRUG_DB.keys():\n",
    "    print(f\"Drug {idx}: {DRUG_DB[idx]['name']}\")\n",
    "print(\"===========================\")\n",
    "print(\"THERAPIES:\")\n",
    "for idx in THERAPY_DB.keys():\n",
    "    drug_ids = THERAPY_DB[idx][\"drug_id\"]\n",
    "    print(f\"Therapy {idx}: {[DRUG_DB[drug_id]['name'] for drug_id in drug_ids]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34913486",
   "metadata": {},
   "source": [
    "We now create a mapping between the drug distribution table columns to the therapy ID numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2e44f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping: list[int] = [7, 8, 6, 4, 5, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8152967",
   "metadata": {},
   "source": [
    "Now remove any un-needed columns from the drug distribution table (i.e. 'sum', 'aspirin', 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9545ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_distribution.drop(columns=[\"sum\", \"aspirin\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1c6ce8",
   "metadata": {},
   "source": [
    "### Public/Private Distribution\n",
    "\n",
    "For countries that have a public and private sector split, the overall strategy will be constructed using a MFT. For the overall distribution percentage, we use a weighted value based on the proportion of the split between public and private sector distributions. For example for a country with a 70/30 public/private split a therapy that has a 25% distribution rate in the public sector and a 50% distribution in the private sector would have an overall strategy distribution of 0.7 * 0.25 + 0.3 * 0.50 = 0.325. The private market, however, can be difficult to ascertain in some countries, particularly in the distribution of specific therapies. \n",
    "\n",
    "If there is no private market or no information on the private market, then we simply use the public sector distribution rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5909ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "market_distribution = pd.DataFrame(\n",
    "    {\n",
    "        \"private\":  0.895 * np.ones_like(drug_distribution.index),\n",
    "        \"public\": 0.105 * np.ones_like(drug_distribution.index)\n",
    "    },\n",
    "    index= drug_distribution.index\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0899b4",
   "metadata": {},
   "source": [
    "We now apply the public/private market distribution to the drug distribution rates to get the overall distribution rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0dcca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "private_market = drug_distribution.mul(market_distribution[\"private\"], axis=0)\n",
    "public_market = drug_distribution.mul(market_distribution[\"public\"], axis=0)\n",
    "combined_distribution = private_market.add(public_market, fill_value=0)\n",
    "combined_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de975cac",
   "metadata": {},
   "source": [
    "### Start and End Dates\n",
    "\n",
    "Next we need to establish the calibration year, starting date, and ending date of the simulation. The calibration year is the year of the most recent data available, typically 2-3 years old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9774cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_year = 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024a7308",
   "metadata": {},
   "source": [
    "Typical procedure is to back out ten years from the calibration year to establish the starting date for the simulation to permit sufficient \"burn in\" time and then to run the simulation for an additional year after the calibration year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969532d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "starting_date = date(calibration_year - 11, 1, 1) \n",
    "ending_date = date(calibration_year + 1, 12, 31)  \n",
    "start_of_comparison_period = date(calibration_year, 1, 1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8c3643",
   "metadata": {},
   "source": [
    "### Baseline Strategy and Event Databases\n",
    "\n",
    "From this information we must manually construct the baseline strategy(ies) and event(s) databases. Similar to the drug and therapy \"databases\" we store the strategies in a dictionary with integer keys. Each key corresponds to a strategy which itself is a dictionary with the following keys:\n",
    "- `name`: The name of the strategy. This is used to identify the strategy in the simulation.\n",
    "- `type`: The type of strategy. Leave this as \"MFT\" until further notice.\n",
    "- `therapy_ids`: The therapy ids used in the strategy. This is a list of integers corresponding to the therapy ids in the `THERAPY_DB` dictionary.\n",
    "- `distribution`: The distribution of the strategy. This is a list of floats corresponding to the distribution of the strategy that we derived from the drug distribution rates from DHS data.\n",
    "\n",
    "Next create a list that provides the mapping between the column names in the combined distribution table to the numeric index of the therapy in the `THERAPY_DB` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b487a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruamel.yaml import YAML\n",
    "\n",
    "yaml = YAML()\n",
    "\n",
    "os.makedirs(os.path.join(\"conf\", name, \"test\"), exist_ok=True)\n",
    "\n",
    "strategy_db = {}\n",
    "events = []\n",
    "\n",
    "i = 0\n",
    "for _, row in combined_distribution.iterrows():\n",
    "    strategy_db[i] = {\n",
    "        \"name\": f\"{row.name}_pub\",\n",
    "        \"type\": \"MFT\",\n",
    "        \"therapy_ids\": column_mapping,\n",
    "        \"distribution\": row.values.tolist(),\n",
    "    }\n",
    "    events.append({\n",
    "        \"name\": f\"{row.name}_strategy\",\n",
    "        \"info\": [{\"day\": date(int(row.name), 1, 1).strftime(\"%Y/%m/%d\"), \"strategy_id\": i}],\n",
    "    })\n",
    "    i += 1\n",
    "    \n",
    "yaml.dump(events, open(os.path.join(\"conf\", name, \"test\", \"events.yaml\"), \"w\"))\n",
    "yaml.dump(strategy_db, open(os.path.join(\"conf\", name, \"test\", \"strategy_db.yaml\"), \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f6e22",
   "metadata": {},
   "source": [
    "## Birth Rate Check\n",
    "\n",
    "This is all then put together to develop an initial configuration to test these baseline parameters for the growth rate. From the team, data files, and some additional research you need to identify the following parameters:\n",
    "\n",
    "- `birth_rate`: the country's \"crude\" birth rate (we will be verifying this number in the step)\n",
    "- `initial_age_structure`: the distribution of the population across different age groups at the start of the simulation\n",
    "- `age_distribution`: the corresponding age brackets of the initial age distribution\n",
    "- `death_rate`: the death rate of the country by age bracket.\n",
    "- `target_population`: the calibration year's population value of the country\n",
    "\n",
    "The important calibration point is the final year population. We back out the population data from this using a birth rate. To check that the initialization raster is correct, we need to run an initial simulation run to verify this growth rate considering malaria deaths as well. This is really just a sanity check to make sure the input data is correct.\n",
    "\n",
    "Run an initial check to make sure the growth rate is working correctly. Given the input file for whatever year, run a single simulation and check that the population for the target year is correct. Make sure to scale the ending population by the simulation scale factor as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb43dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_distribution = [ 0.037, 0.132, 0.161, 0.142, 0.090, 0.086, 0.070, 0.052, 0.044, 0.044, 0.031, 0.041, 0.024, 0.017, 0.013, 0.017]\n",
    "birth_rate = 55.5 / 1000\n",
    "death_rate = [0.048140, 0.041605, 0.052070, 0.048057, 0.048057, 0.00497, 0.00497, 0.00497, 0.00497, 0.003540, 0.00354, 0.00758, 0.01113, 0.01113, 0.01113]\n",
    "initial_age_structure = [1, 5, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 69, 100]\n",
    "target_population = 33_635_160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58129977",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(age_distribution) == len(initial_age_structure), \"Please check to make sure that the values and lengths of age_distribution and initial_age_structure are consistent.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a335cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "initial_population = np.nansum(utils.read_raster(os.path.join(\"data\", name, f\"{name}_initialpopulation.asc\"))[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a463f50c",
   "metadata": {},
   "source": [
    "Next we'll create a simple simulation run to verify the population growth rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d2e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from masim_analysis import calibrate, configure\n",
    "\n",
    "pop = 100_000\n",
    "params = configure.configure(\n",
    "    country_code=name,\n",
    "    birth_rate=birth_rate,\n",
    "    initial_age_structure=initial_age_structure,\n",
    "    age_distribution=age_distribution,\n",
    "    death_rates=death_rate,\n",
    "    starting_date=starting_date,\n",
    "    start_of_comparison_period=start_of_comparison_period,\n",
    "    ending_date=ending_date,\n",
    "    strategy_db=strategy_db,\n",
    "    calibration_str=f\"growth_validation_{pop}\",\n",
    "    beta_override=0.00,\n",
    "    population_scalar=1.0,\n",
    "    calibration=True,\n",
    ")\n",
    "params[\"events\"].extend(events)\n",
    "\n",
    "yaml.dump(params, open(os.path.join(\"conf\", name, \"test\", f\"{name}_growth_validation_{pop}.yaml\"), \"w\"))\n",
    "\n",
    "calibrate.write_pixel_data_files(params[\"raster_db\"], pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb6f91",
   "metadata": {},
   "source": [
    "Now we'll run the simulation to verify the population growth rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ed93d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(\"conf\", name, \"test\", f\"{name}_growth_validation_{pop}.yaml\")\n",
    "os.makedirs(os.path.join(\"output\", name, \"test\"), exist_ok=True)\n",
    "output_file = os.path.join(\"output\", name, \"test\", f\"{name}_growth_validation_{pop}\")\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f\"./bin/MaSim -i ./{filename} -o ./{output_file} -r SQLiteDistrictReporter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4490f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from masim_analysis import analysis\n",
    "\n",
    "data = analysis.get_table(f\"{output_file}monthly_data_0.db\", \"monthlysitedata\")\n",
    "starting_pop = data[data[\"monthlydataid\"] == 1][\"population\"].sum()\n",
    "last_month = data[\"monthlydataid\"].unique()[-13]\n",
    "population_by_month = [\n",
    "    data[data[\"monthlydataid\"] == month][\"population\"].sum() for month in data[\"monthlydataid\"].unique()\n",
    "]\n",
    "population_by_month = np.array(population_by_month)\n",
    "ending_population = population_by_month[-13]\n",
    "growth_rate = (ending_population - starting_pop) / starting_pop\n",
    "projected_population = initial_population * (1 + growth_rate)\n",
    "print(f\"Starting population: {starting_pop}\")\n",
    "print(f\"Ending population: {ending_population}\")\n",
    "print(f\"Growth rate: {growth_rate}\")\n",
    "print(f\"Projected {calibration_year} population: {projected_population}\")\n",
    "print(f\"Percent error: {100 * (projected_population - target_population) / target_population:.4f}%\")\n",
    "\n",
    "# plots\n",
    "population_scalar = population_by_month / starting_pop\n",
    "plt.plot(data[\"monthlydataid\"].unique(), (initial_population * population_scalar) / 1_000_000, linestyle=\"-\", color=\"b\")\n",
    "plt.axhline(y=target_population / 1_000_000, color=\"r\", linestyle=\"--\", label=\"Target Population (32.64 million)\")\n",
    "plt.axvline(x=last_month, color=\"g\", linestyle=\"--\", label=\"Calibration year (2023)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Population (millions)\")\n",
    "plt.title(\"Population by Month\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(\"images\", name, f\"{name}_population_by_month.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7b96f4",
   "metadata": {},
   "source": [
    "You should shoot for an absolute error of less than 2%. Finally we'll add in the target case information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c21f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_case_count = 18_600_000\n",
    "lower_bound = 18_000_000\n",
    "upper_bound = 19_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e72607",
   "metadata": {},
   "source": [
    "With that achieved, we can now save off these initial configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21248b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from masim_analysis.configure import CountryParams\n",
    "\n",
    "country = CountryParams(\n",
    "    name,\n",
    "    long_name,\n",
    "    age_distribution,\n",
    "    birth_rate,\n",
    "    calibration_year,\n",
    "    death_rate,\n",
    "    initial_age_structure,\n",
    "    target_population,\n",
    "    starting_date,\n",
    "    ending_date,\n",
    "    start_of_comparison_period,\n",
    "    target_case_count,\n",
    "    upper_bound,\n",
    "    lower_bound,\n",
    ")\n",
    "with open(os.path.join(\"conf\", name, \"test\", f\"{name}_country_params.json\"), \"w\") as f:\n",
    "    f.write(json.dumps(country.to_dict(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e20ffde",
   "metadata": {},
   "source": [
    "Now you can use the below code block to load these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7585b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from masim_analysis.configure import CountryParams\n",
    "\n",
    "country = CountryParams.load(name=\"moz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaf6e5f",
   "metadata": {},
   "source": [
    "You can now use the data in `country` to access various parameters, namely the country code and long-form name. This is used so that instead of having to run a boilerplate block of code every time you open and run this notebook, you can simply use the single line above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b5d675",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Seasonality Calibration\n",
    "\n",
    "Seasonality is a something of a manual process to fit precisely. The goal is to fit a curve that gives a scalar parameter that modifies the incidence rate for each day of the year. This is written to a `.csv` file historically called \"adjustment\" or \"rainfall\" and is a simple row-wise list of scalar values that effect the overall incidence or biting rate. This notebook choose to save this output as `seasonality.csv` in the `data/<country>/calibration` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfffd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from masim_analysis.configure import CountryParams\n",
    "\n",
    "country = CountryParams.load(name=\"moz\")\n",
    "\n",
    "cuamba = pd.read_csv(os.path.join(\"data\", country.country_code, \"calibration\", \"incidence cuamba.csv\"))\n",
    "inharrime = pd.read_csv(os.path.join(\"data\", country.country_code, \"calibration\", \"incidence inharrime.csv\"))\n",
    "c = cuamba[[\"month_num\", \"cases/thousand\"]].copy()\n",
    "i = inharrime[[\"month_num\", \"cases/thousand\"]].copy()\n",
    "s = pd.concat([c, i], axis=0)\n",
    "s = s.dropna()\n",
    "\n",
    "x = s[\"month_num\"].to_numpy()\n",
    "y = s[\"cases/thousand\"].to_numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 2))\n",
    "ax.plot(x, y, \".b\", label=\"Data\")\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"Cases/thousand\")\n",
    "ax.set_title(\"Seasonal signal\")\n",
    "plt.savefig(os.path.join(\"images\", country.country_code, f\"{country.country_code}_seasonal_signal.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9cc04b",
   "metadata": {},
   "source": [
    "As we know from previous work, the seasonality of malaria (and most diseases) is roughly sinusoidal. The goal is to fit a sinusoidal curve to the data. With that as well the working assumption of the Boni Lab is to use the positive half of the sine curve. There is not a strong stance on this as seasonal incidence modeling is not a major research concern of the lab. Simply put, taking the positive half of the sine curve is what previous publications from the lab has done and to avoid unneccesary problems in the peer review process, we will continue to do this. This notebook will walk through the process for the full sine curve for completeness' sake.\n",
    "\n",
    "The fitting is done using the `scipy.optimize.curve_fit` function. The function takes in a model function and the data to fit. The model function is a sinusoidal function with a phase shift, period, amplitude, and offset.\n",
    "\n",
    "$$\n",
    "s(x) = A \\sin\\left(\\frac{2 \\pi x}{P} + \\phi\\right) + B\n",
    "$$\n",
    "\n",
    "We are looking to fit a scalar multiplier of the base incidence data not a curve over the specific incidence. We can do this by normalizing the incidence data about the median, mean, or mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acecdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode as spmode\n",
    "import numpy as np\n",
    "\n",
    "median = float(np.median(y))\n",
    "mean = float(np.mean(y))\n",
    "mode = float(spmode(y).mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3609386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 2))\n",
    "ax.plot(x, y, \".b\", label=\"Data\")\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"Cases/thousand\")\n",
    "ax.set_title(\"Seasonal signal\")\n",
    "ax.axhline(median, color=\"r\", linestyle=\"--\", label=\"Median\")\n",
    "ax.axhline(mean, color=\"g\", linestyle=\"--\", label=\"Mean\")\n",
    "ax.axhline(mode, color=\"y\", linestyle=\"--\", label=\"Mode\")\n",
    "ax.legend(loc=\"upper right\", bbox_to_anchor=(1.12, 1))\n",
    "plt.savefig(os.path.join(\"images\", country.country_code, f\"{country.country_code}_seasonal_signal_means.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ef990",
   "metadata": {},
   "source": [
    "Normailze the data with respect to the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113224f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_norm = y / mean\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 2))\n",
    "ax.plot(x, y_norm, \".b\", label=\"Normalized Data\")\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"Cases / thousand\")\n",
    "ax.set_title(\"Seasonal signal\")\n",
    "ax.axhline(median / mean, color=\"r\", linestyle=\"--\", label=\"Normalized Median\")\n",
    "ax.axhline(mean / mean, color=\"g\", linestyle=\"--\", label=\"Normalized Mean\")\n",
    "ax.axhline(mode / mean, color=\"y\", linestyle=\"--\", label=\"Normalized Mode\")\n",
    "ax.legend()\n",
    "plt.savefig(os.path.join(\"images\", country.country_code, f\"{country.country_code}_seasonal_signal_normalized.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fa5d3e",
   "metadata": {},
   "source": [
    "Run the curve fit function and analyze with resepct to both the full sinusoid and the positive half of the sine curve. Models of these functions are found in the `calibrate` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e797f054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "from masim_analysis.calibrate import sinusoidal, positive_sinusoidal\n",
    "\n",
    "fit = curve_fit(sinusoidal, x, y_norm, p0=[1, 365, 0, 1], maxfev=10000)\n",
    "\n",
    "coefs = fit[0]\n",
    "\n",
    "t = np.linspace(1, 12, 1000)\n",
    "\n",
    "full_sine = sinusoidal(x, *coefs)\n",
    "r2 = 1 - (np.sum((y_norm - full_sine) ** 2) / np.sum((y_norm - np.mean(y_norm)) ** 2))\n",
    "std_dev = np.std(y_norm - full_sine)\n",
    "\n",
    "positive_sine = positive_sinusoidal(x, *coefs)\n",
    "r2_positive = 1 - (np.sum((y_norm - positive_sine) ** 2) / np.sum((y_norm - np.mean(y_norm)) ** 2))\n",
    "std_dev_positive = np.std(y_norm - positive_sine)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 2))\n",
    "ax.plot(x, y_norm, \".b\", label=\"Data (scaled)\")\n",
    "ax.plot(t, sinusoidal(t, *coefs), \"r-\", label=f\"Fitted curve\\n$r^2$ = {r2:0.2f}\\n$\\sigma$: {std_dev:0.2f}\")\n",
    "ax.plot(\n",
    "    t,\n",
    "    positive_sinusoidal(t, *coefs),\n",
    "    \"g--\",\n",
    "    label=f\"Fitted curve (positive sine)\\n$r^2$ = {r2_positive:0.2f}\\n$\\sigma$: {std_dev_positive:0.2f}\",\n",
    ")\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"Incidence Scalar\")\n",
    "ax.legend(loc=\"upper right\", bbox_to_anchor=(0.65, -0.35))\n",
    "plt.title(\"Seasonal signal with fitted curves\")\n",
    "plt.savefig(os.path.join(\"images\", country.country_code, f\"{country.country_code}_seasonal_signal_fitted.png\"), bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd788099",
   "metadata": {},
   "source": [
    "The fit is not great, but this is a relatively minor aspect of the simulation. The point is to introduce some degree of seasonal variation that is reasonable. Now let's transform the fit back to the incidence rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f871e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "ax.plot(x, y, \".b\", label=\"Data (scaled)\")\n",
    "ax.plot(t, positive_sinusoidal(t, *coefs) * mean, \"g--\", label=\"Fitted curve; $r^2$ = %.2f\" % r2)\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"Cases/thousand\")\n",
    "ax.set_title(\"Seasonal signal with fitted positive sine curve\")\n",
    "fig.savefig(os.path.join(\"images\", country.country_code, f\"{country.country_code}_seasonal_signal_fitted_positive.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e4f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the fit to a file for 365 days\n",
    "fit_x = np.arange(1, 366)\n",
    "fit_y = positive_sinusoidal(fit_x, *fit[0])\n",
    "data = pd.DataFrame({\"day\": fit_x, \"cases/thousand\": fit_y})\n",
    "data = data.set_index(\"day\")\n",
    "data.to_csv(os.path.join(\"data\", country.country_code, f\"{country.country_code}_seasonality.csv\"), index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df0af7",
   "metadata": {},
   "source": [
    "===========================\n",
    "\n",
    "Begin push-button automatic country model generation. At this point you've done all the manual configuration that needs to be done. This is a work in progress but the next few sections are slowly being ported over to a single CLI tool. This tool or set of tools should eventually replace the need for the manual process described in this notebook.\n",
    "\n",
    "***Calibration Tool***\n",
    "\n",
    "Calibration is now automated through a single command line tool: `masim_analysis.calibrate:calibrate`! This is a CLI tool installed via the `masim_analysis` package. If using `uv` to manage your virtual environments, you can run the tool via:\n",
    "\n",
    "```bash\n",
    "uv run calibrate --help\n",
    "```\n",
    "\n",
    "This tool automatically generates the configuration files, the commands, batch processes the commands, summarizes the results, and generates the log-sigmoid fit. It does this by using baked-in population and beta value bins (found in `masim_analysis.calibrate` as `POPULATION_BINS` and `BETAS` respectively) and calculates the access rate variation by reading in the the `<country_code>_treatementseeking.asc` raster file.\n",
    "\n",
    "You can use this on the command line as:\n",
    "\n",
    "```bash\n",
    "uv run calibrate <country_code> -r 20 -o output\n",
    "```\n",
    "\n",
    "where `<country_code>` is the two or three letter country code, `-r` is the number of repetitions per beta value (default is 10), and `-o` is the output directory (default is `output`). This will generate the calibration data files in `data/<country_code>/calibration` and the configuration files in `conf/<country_code>/calibration`, and the output files in `<output>/<country_code>/calibration`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ca7ce",
   "metadata": {},
   "source": [
    "## Run calibration data generation\n",
    "\n",
    "The unknown that we are trying to solve for is the beta value(s). We have _real_ pixel-wise _prevalence_ (pfpr2-10) data that arrises from a given beta. The goal is to generate data that matches closely the real prevalence data by varying the beta value, population size, and access rate for a simulated single pixel. We will first generate the configuration files for the calibration runs here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893310f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country calibration script\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from ruamel.yaml import YAML\n",
    "\n",
    "from masim_analysis import calibrate, utils\n",
    "from masim_analysis.configure import CountryParams\n",
    "\n",
    "yaml = YAML()\n",
    "\n",
    "name = \"moz\"\n",
    "country = CountryParams.load(os.path.join(\"conf\", name, \"test\", f\"{name}_country_params.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449c63df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique treatment access rates\n",
    "treatment, _ = utils.read_raster(os.path.join(\"data\", country.country_code, f\"{country.country_code}_treatmentseeking.asc\"))\n",
    "treatment = np.unique(treatment)\n",
    "treatment = treatment[~np.isnan(treatment)]\n",
    "treatment = np.sort(treatment)\n",
    "access_rates = [float(t) for t in treatment]  # Convert to float for consistency and to make pyright happy\n",
    "\n",
    "events = yaml.load(open(os.path.join(\"conf\", country.country_code, \"test\", \"events.yaml\"), \"r\"))\n",
    "strategy_db = yaml.load(open(os.path.join(\"conf\", country.country_code, \"test\", \"strategy_db.yaml\"), \"r\"))\n",
    "\n",
    "calibrate.generate_configuration_files(\n",
    "    country.country_code,\n",
    "    country.start_of_comparison_period.year,\n",
    "    access_rates,\n",
    "    country.birth_rate,\n",
    "    country.death_rate,\n",
    "    country.initial_age_structure,\n",
    "    country.age_distribution,\n",
    "    strategy_db=strategy_db,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd31fb6d",
   "metadata": {},
   "source": [
    "Create the command and job files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrate.generate_command_and_job_files(\n",
    "    country.country_code,\n",
    "    access_rates,\n",
    "    cores=28,\n",
    "    nodes=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392c525b",
   "metadata": {},
   "source": [
    "At this point you should have all the data and configuration files you need in order to run the calibration. The next step is to commit the changes (added files) to the local branch and push to the remote branch. Log into the cluster and switch to the branch you just created:\n",
    "\n",
    "```bash\n",
    "git checkout <branch_name>\n",
    "git pull\n",
    "```\n",
    "Create the required output directories on the cluster: \n",
    "\n",
    "```bash\n",
    "mkdir -p output/<country>/calibration\n",
    "```\n",
    "\n",
    "Ensure that the jobs files (typically `.sh`) are only the specific jobs that you want to run. Delete any other job or `.sh` files. Keep in mind that any given user may only have at most 50 jobs queued on Owl's Nest at a time. Queue up the calibration jobs using the following command:\n",
    "\n",
    "```bash\n",
    "for i in $(ls *.sh); do\n",
    "    echo \"Submitting job $i\"\n",
    "    qsub $i\n",
    "done\n",
    "```\n",
    "\n",
    "This will submit all the jobs in the current directory. Give it a few minutes to queue up and then log into your cluster account and check the status of the jobs. At steady state, each job should be running approximately 28 processess simultaneously. The lower population size pixels do not take very long to run when spread out over several nodes (4-8, approximately 5-10 minutes). The larger population sizes (10k-20k) take much longer (about an hour).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cfc524",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Check for missing data\n",
    "\n",
    "Once the jobs are completed, you will need to download the output files from the cluster. This can be done using `scp` or `rsync`. For example, to download the output files from Owl's Nest to your local machine, you can use the following command:\n",
    "\n",
    "```bash\n",
    "scp -r <username>@<cluster_address>:Temple-Malaria-Simulation-Analysis/output <local_path>\n",
    "```\n",
    "This will download the output files to the specified local path. I recommend simply copying them to your desktop. Once you have downloaded the output files, copy them to this repo (or download directly) and place them in the `output/<country>/calibration` directory. This will allow you to run the analysis on the simulated data locally.\n",
    "\n",
    "Assuming that all or most of the calibration simulation executed successfull, delete all job and commands files in the local repository. Do not delete the files on the cluster! This will cause a git tracking issue as the repository branches will be out of sync.\n",
    "\n",
    "Prior to running the full calibration analysis, check for any missing data files. Sometimes Owl's Nest gets hung, the database file didn't write correctly, the job times out, or some other bug happened. First, a sanity check. We should have the following number of output calibration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec4eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from masim_analysis.calibrate import POPULATION_BINS, BETAS\n",
    "from masim_analysis.configure import CountryParams\n",
    "\n",
    "name = \"moz\"\n",
    "country = CountryParams.load(os.path.join(\"conf\", name, \"test\", f\"{name}_country_params.json\"))\n",
    "\n",
    "needed_files = len(POPULATION_BINS) * len(access_rates) * len(BETAS) * 20\n",
    "output_dir = os.path.join(\"output\", country.country_code, \"calibration\")\n",
    "output_files = os.listdir(output_dir)\n",
    "completed_files = len(output_files)\n",
    "\n",
    "print(f\"Total calibration files: {needed_files}\")\n",
    "print(f\"Completed calibration files: {completed_files}\")\n",
    "print(f\"Missing calibration files: {needed_files - completed_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68726cfa",
   "metadata": {},
   "source": [
    "If there are missing files use the below block to check for the missing output and create the appropriate job and commands files. You should also double check that each permutation is completed using `process_missing_jobs` prior to moving onto the calibration curve fitting. If there are no missing files proceed to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7753b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "base_file_path = os.path.join(\"output\", country.country_code, \"calibration\")\n",
    "summary = pd.DataFrame(columns=[\"population\", \"access_rate\", \"beta\", \"iteration\", \"pfprunder5\", \"pfpr2to10\", \"pfprall\"])\n",
    "\n",
    "comparison = date(country.calibration_year, 1, 1)\n",
    "year_start = comparison.strftime(\"%Y-%m-%d\")\n",
    "year_end = (comparison + pd.DateOffset(years=1)).strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f428e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrate.process_missing_jobs(\n",
    "    name,\n",
    "    access_rates,\n",
    "    os.path.join(\"output\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e642286f",
   "metadata": {},
   "source": [
    "If there are any missing data files run them on the cluster. This can be done by running the following command:\n",
    "\n",
    "```bash\n",
    "qsub missing_calibration_runs_<pop>_job.sh\n",
    "```\n",
    "\n",
    "As an alternative, if there aren't too many missing files you can run them in parallel locally. I like to use [NuShell](https://www.nushell.sh/) to do this because it makes tasks like this much easier to execute. Plus it's fast and written in Rust. Keep in mind that NuShell cannot directly execute strings in scripts like `sh`, `bash`, `zsh`, or normal interaction with `nushell` on the terminal. When `nushell` reads in a text file it assumes the file contains valid `nu` commands. \n",
    "\n",
    "Using `nushell` this task can be accomplished by running the following command:\n",
    "\n",
    "```nushell\n",
    "let max_parallel = (sys cpu | length) # - 1; # If you wish to leave a few cores free for other tasks\n",
    "open missing_calibration_runs_<pop>.txt | lines | par-each --threads $max_parallel { |cmd| ^sh -c $cmd }\n",
    "```\n",
    "\n",
    "or, alternatively, if you are using `bash` or `zsh` you can run the following command:\n",
    "\n",
    "```bash\n",
    "while IFS= read -r line || [[ -n \"$line\" ]]; do\n",
    "    if [[ -n \"$line\" ]]; then\n",
    "        eval \"$line\" &\n",
    "    fi\n",
    "done < ./missing_calibration_runs_<pop>.txt\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8ab387",
   "metadata": {},
   "source": [
    "### Summarize data\n",
    "\n",
    "Once all the appropriate calibration data has been collected we need to summarize across all the individual data files. This will write the summarized results to `calibration_summary.csv` file in the `output/<country>/calibration` directory. The summary file will contain the following columns:\n",
    "- `beta`: the beta value used in the simulation\n",
    "- `population`: the population size of the pixel\n",
    "- `access_rate`: the treatment access rate of the pixel\n",
    "- `pfpr2_10`: the pfpr2-10 value of the pixel\n",
    "- `pfprunder5`: the mean pfpr under 5 value of the pixel\n",
    "- `pfprall`: the mean pfpr value of the pixel\n",
    "- `iteration` : the iteration number of the simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b087e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from masim_analysis import analysis, calibrate\n",
    "from masim_analysis.calibrate import POPULATION_BINS, BETAS\n",
    "\n",
    "name = \"moz\"\n",
    "country = CountryParams.load(os.path.join(\"conf\", name, \"test\", f\"{name}_country_params.json\"))\n",
    "\n",
    "files = glob(os.path.join(\"output\", country.country_code, \"calibration\", \"*.db\"))\n",
    "\n",
    "data = analysis.get_table(files[0], \"monthlysitedata\")\n",
    "end_month = data[\"monthlydataid\"].unique()[-13]\n",
    "base_file_path = os.path.join(\"output\", country.country_code, \"calibration\")\n",
    "\n",
    "summary = calibrate.summarize_calibration_results(\n",
    "    country.country_code,\n",
    "    access_rates,\n",
    "    end_month-12,\n",
    "    end_month,\n",
    "    os.path.join(\"output\")\n",
    ")\n",
    "summary[\"pfprunder5\"] = summary[\"pfprunder5\"].div(100)\n",
    "summary[\"pfpr2to10\"] = summary[\"pfpr2to10\"].div(100)\n",
    "summary[\"pfprall\"] = summary[\"pfprall\"].div(100)\n",
    "summary = summary.drop(columns=[\"iteration\"])\n",
    "summary = summary.groupby([\"population\", \"access_rate\", \"beta\"]).mean().reset_index()\n",
    "summary.to_csv(f\"{base_file_path}/calibration_means.csv\", index=False)\n",
    "summary.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09abc4e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Curve fitting\n",
    "\n",
    "Now that we have summarized data that connects population size, treatment access, transmission rate, and prevelence, we can fit the data to a linear and log-sigmoid curve. Generally, the log-sigmoid appears to model the relationship better, but both methods are here for reference. This will allow us to generate a beta map for the experimental simulation. The beta map is generated by taking the beta value that corresponds to the population size and treatment access rate of each pixel. This is done using the `generate_beta_map` function, which takes the summarized data and generates a beta map for the experimental simulation.\n",
    "\n",
    "Again, the point of this fit is to relate the measured prevalence to the beta value by way of population size and treatment access rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from masim_analysis.configure import CountryParams\n",
    "name = \"moz\"\n",
    "country = CountryParams.load(os.path.join(\"conf\", name, \"test\", f\"{name}_country_params.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b79cb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from masim_analysis import calibrate\n",
    "\n",
    "base_file_path = os.path.join(\"output\", country.country_code, \"calibration\")\n",
    "means = pd.read_csv(f\"{base_file_path}/calibration_means.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5886365",
   "metadata": {},
   "source": [
    "### Linear fit\n",
    "\n",
    "We're looking to fit the pfpr to beta relationship so that we can then use the real pfpr value from the raster data to determine the beta value. So, given a specific pixel's population, pfpr, and access rate (treatmentseeking?) calculate the beta value from this fitting method. We also don't have a decent way to serialize the linear models returned from sklearn, so at the moment this is just here for demonstration purposes.\n",
    "\n",
    "Start using linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from masim_analysis.calibrate import POPULATION_BINS, BETAS\n",
    "from masim_analysis import utils\n",
    "\n",
    "# Get the unique treatment access rates\n",
    "treatment, _ = utils.read_raster(os.path.join(\"data\", country.country_code, f\"{country.country_code}_treatmentseeking.asc\"))\n",
    "treatment = np.unique(treatment)\n",
    "treatment = treatment[~np.isnan(treatment)]\n",
    "treatment = np.sort(treatment)\n",
    "access_rates = [float(t) for t in treatment]  # Convert to float for consistency and to make pyright happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de2753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine grid size\n",
    "num_rows = len(POPULATION_BINS)\n",
    "num_cols = len(access_rates)\n",
    "\n",
    "# Create subplots grid\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(4 * num_cols, 4 * num_rows), sharex=True, sharey=True)\n",
    "\n",
    "# Ensure axes is always a 2D list for consistency\n",
    "if num_rows == 1:\n",
    "    axes = np.array([axes])  # Convert to 2D array\n",
    "if num_cols == 1:\n",
    "    axes = np.array([[ax] for ax in axes])  # Convert to 2D array\n",
    "\n",
    "poly_fit = {\n",
    "    access_rate: {population: None for population in POPULATION_BINS} for access_rate in access_rates\n",
    "}\n",
    "# Perform regression for each (Population, TreatmentAccess) group\n",
    "for i, population in enumerate(POPULATION_BINS):\n",
    "    for j, treatment_access in enumerate(access_rates):\n",
    "        ax = axes[i, j]  # Select subplot location\n",
    "\n",
    "        # Filter the data for the current Population and TreatmentAccess\n",
    "        group = means[(means[\"population\"] == population) & (means[\"access_rate\"] == treatment_access)]\n",
    "\n",
    "        if group.empty:\n",
    "            ax.set_visible(False)  # Hide empty plots\n",
    "            continue\n",
    "\n",
    "        group = group.dropna(axis=0)  # drop any row in a nan column\n",
    "\n",
    "        X = group[[\"beta\"]].values\n",
    "        y = group[\"pfpr2to10\"].values\n",
    "\n",
    "        # 1. Linear Regression\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)  # type: ignore\n",
    "\n",
    "        # 2. Polynomial Regression (degree 3)\n",
    "        poly_model3 = make_pipeline(PolynomialFeatures(3), LinearRegression())\n",
    "        poly_model3.fit(X, y)\n",
    "\n",
    "        # 3. Polynomial Regression (degree 5)\n",
    "        poly_model5 = make_pipeline(PolynomialFeatures(5), LinearRegression())\n",
    "        poly_model5.fit(X, y)\n",
    "\n",
    "        # 4. Spline Regression\n",
    "        # spline_model = UnivariateSpline(group['beta'], group['pfpr2to10'], s=50)\n",
    "\n",
    "        # Plot regression\n",
    "        sns.scatterplot(x=group[\"beta\"], y=group[\"pfpr2to10\"], ax=ax, label=\"Data\", color=\"black\")\n",
    "        ax.plot(group[\"beta\"], model.predict(X), color=\"red\", linestyle=\"dashed\", label=\"Linear\")\n",
    "        ax.plot(group[\"beta\"], poly_model3.predict(X), color=\"blue\", linestyle=\"dashed\", label=\"Poly (3)\")\n",
    "        ax.plot(group[\"beta\"], poly_model5.predict(X), color=\"green\", linestyle=\"dashed\", label=\"Poly (5)\")\n",
    "        # ax.plot(group['Beta'], spline_model(X), color='purple', linestyle=\"dashed\", label=\"Spline\")\n",
    "        poly_fit[treatment_access][population] = {\n",
    "            \"linear\": model,\n",
    "            \"poly3\": poly_model3,\n",
    "            \"poly5\": poly_model5,\n",
    "            # \"spline\": spline_model\n",
    "        }\n",
    "        # Setting titles & labels\n",
    "        ax.set_title(f\"Population : {population}, Access : {treatment_access}\")\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(\"pfpr2to10\")\n",
    "        if i == num_rows - 1:\n",
    "            ax.set_xlabel(\"Beta\")\n",
    "        ax.legend(fontsize=7)\n",
    "\n",
    "# Adjust layout\n",
    "plt.suptitle(\"Curve Fitting by Population & Treatment Access\", fontsize=16)\n",
    "plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.96))\n",
    "plt.savefig(os.path.join(\"images\", country.country_code, f\"{country.country_code}_linear_calibration_curve_fitting.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3310a37",
   "metadata": {},
   "source": [
    "Now we'll do a logarithemic fit for the beta value and attempt to fit a sigmoid curve. Again this is the model that typically works best for the data. The fit returned is a list of the parameters for the sigmoid function which can be easily serialized and saved to a file. The sigmoid function is defined as:\n",
    "\n",
    "$$s = \\frac{a}{1 + e^{-b(x - c)}}$$\n",
    "\n",
    "which in code is:\n",
    "\n",
    "```python\n",
    "def sigmoid(x, a, b, c):\n",
    "    return a / (1 + np.exp(-b * (x - c)))\n",
    "```\n",
    "where `a`, `b`, and `c` are the parameters of the sigmoid function. The `x` value is the beta value. The `a` parameter is the maximum value of the sigmoid function, the `b` parameter is the steepness of the curve, and the `c` parameter is the x-value of the sigmoid's midpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df70f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_map = calibrate.get_beta_models(\n",
    "    populations=POPULATION_BINS,\n",
    "    access_rates=access_rates,\n",
    "    means=means,\n",
    "    pfpr_cutoff=0.0,\n",
    ")\n",
    "models_map_filename = \"models_map.json\"\n",
    "with open(os.path.join(\"data\", country.country_code, \"calibration\", models_map_filename), \"w\") as f:  # noqa: F811, ruff disabled\n",
    "    json.dump(models_map, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823cdb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all the model data, fits, and inverse fits on the same figure\n",
    "num_rows = len(POPULATION_BINS)\n",
    "num_cols = len(access_rates)\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(4 * num_cols, 4 * num_rows), sharex=True, sharey=True)\n",
    "for i, population in enumerate(POPULATION_BINS):\n",
    "    for j, treatment_access in enumerate(access_rates):\n",
    "        ax = axes[i, j]  # Select subplot location\n",
    "        coefs = models_map[treatment_access][population]\n",
    "        group = means[(means[\"population\"] == population) & (means[\"access_rate\"] == treatment_access)]\n",
    "        betas = group[\"beta\"].to_numpy()\n",
    "        pfpr = group[\"pfpr2to10\"].to_numpy()\n",
    "        \n",
    "        ax.plot(betas, pfpr, \".\", label=\"Data\", color=\"black\")\n",
    "        X = np.linspace(1e-4, 10, 10000)\n",
    "        try:\n",
    "            Y = calibrate.sigmoid(np.log10(X), *coefs)\n",
    "            ax.plot(X, Y, color=\"red\", label=\"Fitted Curve\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting sigmoid for Population: {population}, Access: {treatment_access} - {e}\")\n",
    "        \n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_xlabel(\"Beta\")\n",
    "        ax.set_ylabel(\"pfpr2to10\")\n",
    "        ax.set_title(f\"Population : {population}, Access : {treatment_access}\")\n",
    "        ax.legend(fontsize=7)\n",
    "        ax.set_xlim(1e-3, 10)\n",
    "        ax.set_ylim(0, 1)\n",
    "        \n",
    "# Adjust layout\n",
    "plt.suptitle(\"pfPr vs. Beta Data and Curve Fits by Population & Treatment Access\", fontsize=24)\n",
    "plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.96))\n",
    "plt.savefig(os.path.join(\"images\", country.country_code, f\"{country.country_code}_log_sigmoid_fit.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855507fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Keep in mind that the built in JSON serialization and deserialization reads in all the values as strings. To get around this use `load_beta_model` from the `calibrate` module instead of the native `json.load()` function. This will convert the values back to numeric values and lists where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a8c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from masim_analysis import calibrate, utils\n",
    "from masim_analysis.configure import CountryParams\n",
    "\n",
    "country = CountryParams.load(os.path.join(\"conf\", \"moz\", \"test\", f\"moz_country_params.json\"))\n",
    "\n",
    "models_map_filename = \"models_map.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b090131",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_map = calibrate.load_beta_model(os.path.join(\"data\", country.country_code, \"calibration\", models_map_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f370043",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_map[0.451]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f721fd",
   "metadata": {},
   "source": [
    "We can then use this model to generate the beta map for the experimental simulation. The beta map is generated by taking the beta value that corresponds to the population size and treatment access rate of each pixel. This is done using the `create_beta_map` function, which takes the summarized data and generates a beta map for the experimental simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca9812",
   "metadata": {},
   "outputs": [],
   "source": [
    "population, raster_params = utils.read_raster(os.path.join(\"data\", country.country_code, f\"{country.country_code}_population.asc\"))\n",
    "prevalence, _ = utils.read_raster(os.path.join(\"data\", country.country_code, f\"{country.country_code}_pfpr210.asc\"))\n",
    "treatment, _ = utils.read_raster(os.path.join(\"data\", country.country_code, f\"{country.country_code}_treatmentseeking.asc\"))\n",
    "districts, _ = utils.read_raster(os.path.join(\"data\", country.country_code, f\"{country.country_code}_districts.asc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1eeb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_map = calibrate.create_beta_map(models_map, population, treatment, prevalence)\n",
    "pfpr_predicted = calibrate.predicted_prevalence(models_map, population, treatment, beta_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37277b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(18, 7))\n",
    "im0 = ax[0].imshow(beta_map, cmap=\"turbo\")\n",
    "fig.colorbar(im0, ax=ax[0], shrink=0.5, label=\"Beta\")\n",
    "ax[0].set_title(\"Pixel-wise Beta Map\")\n",
    "im0.set_clim(0, 0.5)  # Set color limits for better visualization\n",
    "\n",
    "im1 = ax[1].imshow(prevalence * 100, cmap=\"turbo\", interpolation=\"nearest\")\n",
    "fig.colorbar(im1, ax=ax[1], shrink=0.5, label=\"PfPR %\")\n",
    "ax[1].set_title(\"Pixel-wise PfPR Map\")\n",
    "im1.set_clim(0, np.nanmax(prevalence)*100)  # Set color limits for better visualization\n",
    "#plt.savefig(f\"{name}_beta_map_3.png\", dpi=600, bbox_inches=\"tight\")\n",
    "\n",
    "im2 = ax[2].imshow(pfpr_predicted * 100 * 12, cmap=\"turbo\", interpolation=\"nearest\")\n",
    "fig.colorbar(im2, ax=ax[2], shrink=0.5, label=\"PfPR %\")\n",
    "ax[2].set_title(\"Pixel-wise Predicted PfPR Map\")\n",
    "im2.set_clim(0, 50)  # Set color limits\n",
    "plt.tight_layout()\n",
    "\n",
    "im3 = ax[3].imshow(population, cmap=\"turbo\", interpolation=\"nearest\")\n",
    "fig.colorbar(im3, ax=ax[3], shrink=0.5, label=\"Population\")\n",
    "ax[3].set_title(\"Pixel-wise Population Map\")\n",
    "im3.set_clim(0, 5000)  # Set color limits for better visualization\n",
    "fig.suptitle(\"Beta and PfPR Model Comparison Maps\", fontsize=16)\n",
    "\n",
    "plt.savefig(os.path.join(\"images\", country.country_code, f\"{country.country_code}_comparison_maps.png\"), dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175a9081",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### Subsectioning\n",
    "\n",
    "This was done as a sanity check to for debugging the validation analysis. It can probably be excluded in the full pipeline but it is being left here for now for the sake of returning to it should issues arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0da8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from masim_analysis import utils\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from masim_analysis.configure import CountryParams\n",
    "\n",
    "country = CountryParams.load(os.path.join(\"conf\", \"moz\", \"test\", f\"moz_country_params.json\"))\n",
    "\n",
    "beta_map = utils.read_raster(os.path.join(\"data\", country.country_code, f\"{country.country_code}_beta.asc\"))[0]\n",
    "\n",
    "beta_sub = beta_map[125:150, 150:175]\n",
    "prevalence_sub = prevalence[125:150, 150:175]\n",
    "population_sub = population[125:150, 150:175]\n",
    "treatment_sub = treatment[125:150, 150:175]\n",
    "district_sub = np.zeros_like(districts[125:150, 150:175])\n",
    "\n",
    "utils.write_raster(beta_sub, \"moz_beta_sub.asc\", 0, 0)\n",
    "utils.write_raster(prevalence_sub, \"moz_pfpr_sub.asc\", 0, 0)\n",
    "utils.write_raster(treatment_sub, \"moz_treatment_sub.asc\", 0, 0)\n",
    "utils.write_raster(population_sub, \"moz_population_sub.asc\", 0, 0)\n",
    "utils.write_raster(district_sub, \"moz_districts_sub.asc\", 0, 0)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 6))\n",
    "im0 = ax[0].imshow(beta_sub, cmap=\"turbo\", interpolation=\"nearest\")\n",
    "plt.colorbar(im0, ax=ax[0], label=\"Beta\")\n",
    "ax[0].set_title(\"Pixel-wise Beta Map\")\n",
    "#im0.set_clim(0, np.nanmax(beta_map))  # Set color limits for better visualization\n",
    "im0.set_clim(0, 1)  # Set color limits for better visualization\n",
    "\n",
    "im1 = ax[1].imshow(prevalence_sub * 100, cmap=\"turbo\", interpolation=\"nearest\")\n",
    "plt.colorbar(im1, ax=ax[1], label=\"PfPR %\")\n",
    "ax[1].set_title(\"Pixel-wise PfPR Map\")\n",
    "im1.set_clim(0, np.nanmax(prevalence_sub)*100)  # Set color limits for better visualization\n",
    "#plt.savefig(f\"{name}_beta_map_3.png\", dpi=600, bbox_inches=\"tight\")\n",
    "\n",
    "im2 = ax[2].imshow(population_sub, cmap=\"turbo\", interpolation=\"nearest\")\n",
    "plt.colorbar(im2, ax=ax[2], label=\"Population\")\n",
    "ax[2].set_title(\"Pixel-wise Population Map\")\n",
    "#im2.set_clim(0, np.nanmax(population))  # Set color limits\n",
    "im2.set_clim(0, 5000)  # Set color limits for better visualization\n",
    "plt.savefig(os.path.join(\"images\", country.country_code, f\"{country.country_code}_subsection.png\"), dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213d5deb",
   "metadata": {},
   "source": [
    "=======================\n",
    "\n",
    "***End Calibration Tool***\n",
    "\n",
    "Begin ***Validation Tool***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a878ac6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Validation\n",
    "\n",
    "The validation process is similar to the calibration process except that we now use the fitted modelled beta map to attempt to recreate the prevelance map. For debugging purposes, the subsection sanity check validation has been left in place. It probably does not need to be permanently included in the full pipeline but it is being left here for now for the sake of returning to it should issues arise. It's also a quicker check than running the full country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00354546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "\n",
    "from masim_analysis import configure\n",
    "from masim_analysis.configure import CountryParams\n",
    "\n",
    "country = CountryParams.load(name=\"moz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d033df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruamel.yaml import YAML\n",
    "\n",
    "yaml = YAML()\n",
    "\n",
    "strategy_db = yaml.load(open(os.path.join(\"conf\", country.country_code, \"test\", \"strategy_db.yaml\"), \"r\"))\n",
    "events = yaml.load(open(os.path.join(\"conf\", country.country_code, \"test\", \"events.yaml\"), \"r\"))\n",
    "params = configure.configure(\n",
    "    country_code=country.country_code,\n",
    "    birth_rate=country.birth_rate,\n",
    "    initial_age_structure=country.initial_age_structure,\n",
    "    age_distribution=country.age_distribution,\n",
    "    death_rates=country.death_rate,\n",
    "    starting_date=country.starting_date,\n",
    "    start_of_comparison_period=country.start_of_comparison_period,\n",
    "    ending_date=country.ending_date,\n",
    "    strategy_db=strategy_db,\n",
    "    calibration_str=\"\",\n",
    "    calibration=False,\n",
    ")\n",
    "params[\"events\"].extend(events)\n",
    "\n",
    "params['raster_db']['population_raster'] = \"moz_population_sub.asc\"\n",
    "params['raster_db']['pf_treatment_under5'] = \"moz_treatment_sub.asc\"\n",
    "params['raster_db']['pf_treatment_over5'] = \"moz_treatment_sub.asc\"\n",
    "params['raster_db']['prevalence_raster'] = \"moz_pfpr_sub.asc\"\n",
    "params['raster_db']['district_raster'] = \"moz_districts_sub.asc\"\n",
    "params['raster_db']['beta_raster'] = \"moz_beta_sub.asc\"\n",
    "\n",
    "yaml.dump(params, open(os.path.join(\"conf\", country.country_code, \"test\", f\"{country.country_code}_subsection.yml\"), \"w\"))\n",
    "\n",
    "itr = '4e' # Reset or increment this when running a new iteration or country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b013ee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f\"./bin/MaSim -i {os.path.join('conf', country.country_code, 'test', f'{country.country_code}_subsection.yml')} -o {os.path.join('output', country.country_code, f'{country.country_code}_subsection_{itr}_')} -r SQLitePixelReporter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd86f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from masim_analysis import analysis\n",
    "\n",
    "data = analysis.get_table(os.path.join('output', country.country_code, f'{country.country_code}_subsection_{itr}_monthly_data_0.db'), \"monthlysitedata\")\n",
    "months = data[\"monthlydataid\"].unique()\n",
    "ending_month = months[-13]\n",
    "starting_month = ending_month - 12\n",
    "year = data.loc[data[\"monthlydataid\"].between(starting_month, ending_month, inclusive='left')]\n",
    "\n",
    "prev = year[['locationid', 'pfprunder5', 'pfpr2to10', 'pfprall']].groupby('locationid').mean()\n",
    "population = year[['locationid', 'population']].groupby('locationid').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc7a9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def linear(x, a, b):\n",
    "    \"\"\"\n",
    "    Linear function for curve fitting.\n",
    "    \"\"\"\n",
    "    return a * x + b\n",
    "\n",
    "X = prevalence_sub.reshape(-1)\n",
    "Y = prev['pfpr2to10'].to_numpy() / 100\n",
    "\n",
    "linear_trend, _ = curve_fit(linear, X, Y, p0=[1, 0], maxfev=10000)\n",
    "\n",
    "plt.scatter(X, Y, s=population['population'] / population['population'].max() * 100, marker=\"o\", alpha=0.35, cmap=\"viridis\", c=population['population'], label=\"Predicted PfPR\")\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(\"Population\", rotation=270, labelpad=15)\n",
    "x = np.linspace(0, 1, 1000)\n",
    "plt.plot(x, x, color=\"red\", linestyle=\"--\")\n",
    "plt.plot(x, linear_trend[0] * x + linear_trend[1], color=\"blue\", linestyle=\"--\", label=f\"Predicted trend: {linear_trend[0]:.2f}x + {linear_trend[1]:.2f}\")\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel(\"Observed PfPR\")\n",
    "plt.ylabel(\"Predicted PfPR\")\n",
    "plt.title(\"Observed vs Predicted PfPR\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(\"images\", country.country_code, f\"{name}_subsection_observed_vs_predicted_pfpr_{itr}.png\"), dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea367b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = prev['pfpr2to10'].to_numpy().reshape(25, 25) / 100 - prevalence_sub\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(16, 8))\n",
    "\n",
    "im0 = ax[0, 0].imshow(beta_sub, cmap=\"turbo\")\n",
    "fig.colorbar(im0, ax=ax[0, 0], label=\"Beta\")\n",
    "ax[0, 0].set_title(\"Pixel-wise Beta Map\")\n",
    "im0.set_clim(0, 0.5)\n",
    "\n",
    "im1 = ax[0, 1].imshow(prev['pfpr2to10'].to_numpy().reshape(25,25), cmap=\"turbo\")\n",
    "fig.colorbar(im1, ax=ax[0, 1], label=\"PfPR %\")\n",
    "ax[0, 1].set_title(\"Predicted Prevalence Map\")\n",
    "im1.set_clim(0, np.nanmax(prevalence_sub)*100)\n",
    "\n",
    "im2 = ax[0, 2].imshow(population[\"population\"].to_numpy().reshape(25,25) * 4, cmap=\"turbo\")\n",
    "fig.colorbar(im2, ax=ax[0, 2], label=\"Population\")\n",
    "ax[0, 2].set_title(\"Predicted Population Map\")\n",
    "im2.set_clim(0, 5000)\n",
    "color='blue'\n",
    "im3 = ax[1,0].imshow(diff, cmap=\"PiYG\", vmin=-0.25, vmax=0.25)\n",
    "fig.colorbar(im3, ax=ax[1,0], label=\"Difference (Predicted - Observed)\")\n",
    "ax[1, 0].set_title(\"Prevalence Difference Map\")\n",
    "\n",
    "im4 = ax[1, 1].imshow(prevalence_sub * 100, cmap=\"turbo\", interpolation=\"nearest\")\n",
    "fig.colorbar(im4, ax=ax[1, 1], label=\"PfPR %\")\n",
    "ax[1, 1].set_title(\"Observed PfPR Map\")\n",
    "im4.set_clim(0, np.nanmax(prevalence_sub)*100)\n",
    "\n",
    "im5 = ax[1, 2].imshow(population_sub, cmap=\"turbo\", interpolation=\"nearest\")\n",
    "fig.colorbar(im5, ax=ax[1, 2], label=\"Population\")\n",
    "ax[1, 2].set_title(\"Observed Population Map\")\n",
    "im5.set_clim(0, 5000)\n",
    "\n",
    "fig.suptitle(\"Subsection Predicted vs Observed Maps\", fontsize=16)\n",
    "plt.savefig(os.path.join(\"images\", country.country_code, f\"{name}_subsection_prediction_maps_{itr}.png\"), dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673253f1",
   "metadata": {},
   "source": [
    "If the trend line looks close to y=x, write the full beta map and proceed to country-wide validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab34982",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.write_raster(beta_map, \n",
    "    os.path.join(\"data\", country.country_code, f\"{country.country_code}_beta.asc\"), \n",
    "    raster_params['xllcorner'], \n",
    "    raster_params['yllcorner'], \n",
    "    raster_params['cellsize']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a303c3",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Whole country validation\n",
    "\n",
    "Now we'll repeat this process for the whole country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5921d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from masim_analysis import commands, configure, utils\n",
    "from masim_analysis.configure import CountryParams\n",
    "\n",
    "country = CountryParams.load(name=\"moz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3121ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "file, cmds = commands.generate_commands(\n",
    "    os.path.join(\"conf\", country.country_code, f\"{country.country_code}_validation.yml\"), \n",
    "    os.path.join(\"output\", country.country_code, f\"validation\"), \n",
    "    20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d6763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_db = yaml.load(open(os.path.join(\"conf\", country.country_code, \"test\", \"strategy_db.yaml\"), \"r\"))\n",
    "events = yaml.load(open(os.path.join(\"conf\", country.country_code, \"test\", \"events.yaml\"), \"r\"))\n",
    "params = configure.configure(\n",
    "    country_code=country.country_code,\n",
    "    birth_rate=country.birth_rate,\n",
    "    initial_age_structure=country.initial_age_structure,\n",
    "    age_distribution=country.age_distribution,\n",
    "    death_rates=country.death_rate,\n",
    "    starting_date=country.starting_date,\n",
    "    start_of_comparison_period=country.start_of_comparison_period,\n",
    "    ending_date=country.ending_date,\n",
    "    strategy_db=strategy_db,\n",
    "    calibration_str=\"\",\n",
    "    calibration=False,\n",
    ")\n",
    "params[\"events\"].extend(events)\n",
    "\n",
    "with open(os.path.join(\"conf\", name, f\"{name}_validation.yml\"), \"w\") as f:\n",
    "    yaml.dump(params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58e2335",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{name}_validation.txt\", \"w\") as f:\n",
    "    for cmd in cmds:\n",
    "        f.write(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93ebba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "commands.generate_job_file(\n",
    "    \"moz_validation.txt\",\n",
    "    \"validation\",\n",
    "    cores_override=4,\n",
    "    nodes_override=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4230988",
   "metadata": {},
   "source": [
    "Due to the size of the validation simulation (outside of small countries like Rwanda) the simulation usually needs a larger chunk of RAM and a longer time to run than what is permitted or available on OwlsNest. It is recommended to run the validation simulation on the Nessun Dorma cluster instead. Set up and connection to the cluster is part of the onboarding process to the lab.\n",
    "\n",
    "Once connected to the cluster, use ssh to connect:\n",
    "\n",
    "```bash\n",
    "ssh <username>@10.10.100.2\n",
    "```\n",
    "\n",
    "Once connected and logged in, navigate to `~/work`, clone this repository there or pull in the latest branch changes and submit the `.pbs` job.\n",
    "\n",
    "(Automatic generation of Nessun Dorma job files is not yet implemented, but templates are included in this repository.)\n",
    "\n",
    "Once up to date, ssh to one of the cluster nodes and submit the validation job:\n",
    "\n",
    "```bash\n",
    "ssh nd01\n",
    "cd ~/work/Temple-Malaria-Simulation-Analysis\n",
    "qsub validation.pbs\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7856312",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Once the runs have complete use `scp` as described above to copy the database files locally. Next up plot the predicted vs true prevalence values to check the fit of the model. There will likely be some outliers along the y- and x-axis. The Malaria Atlas project sometimes project prevalence where there is no population and our fits particularly for low-population areas sometimes project prevalence where no observed prevalence exists. Use median simulation validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f824cb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from masim_analysis import analysis, utils, calibrate\n",
    "from masim_analysis.configure import CountryParams\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "country = CountryParams.load(name=\"moz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352fb6a5",
   "metadata": {},
   "source": [
    "To check that we actually get the correct date range, let's look at the unique monthly data ids. From this, we can derive the correct index to use for the validation mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bc39b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "population, raster_params = utils.read_raster(os.path.join(\"data\", country.country_code, f\"{country.country_code}_population.asc\"))\n",
    "prevalence_obs, _ = utils.read_raster(os.path.join(\"data\", country.country_code, f\"{country.country_code}_pfpr210.asc\"))\n",
    "beta = utils.read_raster(os.path.join(\"data\", country.country_code, f\"{country.country_code}_beta.asc\"))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77942ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ave_population, ave_cases, ave_prevalence_2_to_10, ave_cases_2_to_10, ave_prevalence_under_5, ave_cases_under_5 = analysis.get_average_summary_statistics(\"output/moz/validation\", country)\n",
    "ave_population.to_csv(os.path.join(\"output\", country.country_code, \"validation\", \"ave_population.csv\"))\n",
    "ave_cases.to_csv(os.path.join(\"output\", country.country_code,      \"validation\", \"ave_cases.csv\"))\n",
    "ave_prevalence_2_to_10.to_csv(os.path.join(\"output\", country.country_code, \"validation\", \"ave_prevalence_2_to_10.csv\"))\n",
    "ave_cases_2_to_10.to_csv(os.path.join(\"output\", country.country_code, \"validation\", \"ave_cases_2_to_10.csv\"))\n",
    "ave_prevalence_under_5.to_csv(os.path.join(\"output\", country.country_code, \"validation\", \"ave_prevalence_under_5.csv\"))\n",
    "ave_cases_under_5.to_csv(os.path.join(\"output\", country.country_code, \"validation\", \"ave_cases_under_5.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052f60e2",
   "metadata": {},
   "source": [
    "**Total case count verification**\n",
    "\n",
    "Mozambique has a reported 2023 case count ('clinical episodes') between 12.87 and 13.23 million cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c92c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ave_cases = pd.read_csv(os.path.join(\"output\", country.country_code, \"validation\", \"ave_cases.csv\"), index_col=0)\n",
    "ave_prevalence_2_to_10 = pd.read_csv(os.path.join(\"output\", country.country_code, \"validation\", \"ave_prevalence_2_to_10.csv\"), index_col=0)\n",
    "ave_prevalence_under_5 = pd.read_csv(os.path.join(\"output\", country.country_code, \"validation\", \"ave_prevalence_under_5.csv\"), index_col=0)\n",
    "ave_population = pd.read_csv(os.path.join(\"output\", country.country_code, \"validation\", \"ave_population.csv\"), index_col=0)\n",
    "ave_cases_2_to_10 = pd.read_csv(os.path.join(\"output\", country.country_code, \"validation\", \"ave_cases_2_to_10.csv\"), index_col=0)\n",
    "ave_cases_under_5 = pd.read_csv(os.path.join(\"output\", country.country_code, \"validation\", \"ave_cases_under_5.csv\"), index_col=0)\n",
    "\n",
    "mean_cases, mean_prevalence_2_to_10, mean_prevalence_under_5, mean_population = calibrate.get_last_year_statistics(ave_cases, ave_prevalence_2_to_10, ave_prevalence_under_5, ave_population)\n",
    "\n",
    "print(f\"{mean_cases['mean'].sum(): ,.0f} clinical episodes | SCALED: {mean_cases['mean'].sum() * 4 : ,.0f}\")\n",
    "print(f\"{mean_population['mean'].sum(): ,.0f} population | SCALED: {mean_population['mean'].sum() * 4 : ,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478638fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ave_cases = ave_cases.drop(columns=\"clinicalepisodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656d5c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ave_cases[\"monthlydataid\"].unique()\n",
    "ending_month = months[-13]\n",
    "ave_cases_year = ave_cases[ave_cases[\"monthlydataid\"].between(ending_month-12, ending_month, inclusive='left')].groupby(\"locationid\").sum().drop(columns=\"monthlydataid\")\n",
    "ave_cases_year[\"mean\"] = ave_cases_year.mean(axis=1)\n",
    "\n",
    "prevalence_obs = prevalence_obs.reshape(-1)\n",
    "prevalence_comp = mean_prevalence_2_to_10[[\"mean\"]].copy().div(100).rename(columns={\"mean\": \"mean_2_to_10\"})\n",
    "prevalence_comp[\"mean_under_5\"] = mean_prevalence_under_5[[\"mean\"]].copy().div(100).rename(columns={\"mean\": \"mean_under_5\"})\n",
    "prev_obs = pd.DataFrame({\n",
    "    \"obs\": prevalence_obs[~np.isnan(prevalence_obs)]\n",
    "    },\n",
    "    index = np.arange(len(prevalence_obs[~np.isnan(prevalence_obs)]))\n",
    "    )\n",
    "PREV = prevalence_comp.merge(prev_obs, left_index=True, right_index=True, how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a784b8",
   "metadata": {},
   "source": [
    "Plot the observed vs predicted prevalence values. If the fit looks good (close to Y=X), then the calibration is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb6310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from masim_analysis.analysis import plot_prevalence_trend\n",
    "\n",
    "num_locations = mean_cases.index.unique()[-1] + 1\n",
    "\n",
    "age_str = \"2_to_10\"\n",
    "\n",
    "X = PREV[\"obs\"].to_numpy()\n",
    "Y = PREV[f\"mean_{age_str}\"].to_numpy()\n",
    "\n",
    "pop = np.zeros(num_locations)\n",
    "pop[mean_population.index - 1] = mean_population[\"mean\"].to_numpy()\n",
    "\n",
    "fig = plot_prevalence_trend(X, Y, pop, age_str)\n",
    "fig.savefig(os.path.join(\"images\", country.country_code, f\"{country.country_code}_observed_vs_predicted_pfpr_FULL_COUNTRY_{age_str}.png\"), dpi=600, bbox_inches=\"tight\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7690cc5d",
   "metadata": {},
   "source": [
    "We'll create some additional analysis plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccec258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "pfpr_diff = Y - X\n",
    "pfpr_diff = pfpr_diff[~np.isnan(pfpr_diff)]\n",
    "(mu, sigma) = norm.fit(pfpr_diff)\n",
    "\n",
    "bins = 100\n",
    "\n",
    "hist = np.histogram(pfpr_diff, bins=bins, density=True)\n",
    "y = norm.pdf(hist[1][:-1], loc=mu, scale=sigma)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "ax.hist(pfpr_diff, bins=bins, color=\"blue\", alpha=0.7, density=True, edgecolor=\"black\", label=\"Differences\")\n",
    "ax.plot(hist[1][:-1], y, color=\"red\", linewidth=2, linestyle=\"--\", label=\"Trend\")\n",
    "ax.set_xlabel(\"pfPr difference\")\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "ax.legend()\n",
    "ax.set_title(f\"Prevalence Differences ({age_str.replace('_', ' ')})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_pfpr = X\n",
    "pred_pfpr = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bbef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfpr = pd.DataFrame({\n",
    "    \"observed\": obs_pfpr,\n",
    "    \"predicted\": pred_pfpr,\n",
    "    \"population\": pop\n",
    "})\n",
    "pfpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a6935b",
   "metadata": {},
   "source": [
    "Now lets break down the above scatter plot by population values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfbdfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize=(24, 10))\n",
    "x = np.linspace(0, 1, 1000)\n",
    "for i, pop in enumerate([10, 50, 100, 500, 1000, 2500, 5000, 10000]):\n",
    "    df = pfpr.loc[pfpr['population'] >= pop]\n",
    "    ax[i // 4, i % 4].scatter(df['observed'], df['predicted'], s=df[\"population\"] / df[\"population\"].max() * 100, alpha=0.5, cmap=\"viridis\", c=df[\"population\"])\n",
    "    ax[i // 4, i % 4].set_title(f'Minimum Population: {pop}')\n",
    "    ax[i // 4, i % 4].set_xlabel('Observed')\n",
    "    ax[i // 4, i % 4].set_ylabel('Predicted')\n",
    "    ax[i // 4, i % 4].plot(x,x, color='red', linestyle='--')\n",
    "    ax[i // 4, i % 4].set_xlim(0, 0.6)\n",
    "    ax[i // 4, i % 4].set_ylim(0, 0.6)\n",
    "    # add a colorbar for the entire figure\n",
    "cbar = fig.colorbar(ax[0, 0].collections[0], ax=ax.ravel().tolist(), orientation='vertical')\n",
    "cbar.set_label(\"Population\", rotation=90, labelpad=15)\n",
    "plt.suptitle(f'Observed vs Predicted PfPR ({age_str.replace('_', ' ')}) by Minimum Population', fontsize=16)\n",
    "plt.savefig(os.path.join(\"images\", country.country_code, f\"{country.country_code}_observed_vs_predicted_pfpr_by_population_{age_str}.png\"), dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b42de6",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Incidence Validation\n",
    "\n",
    "As a baseline calibration, check the whole-country incidence values against the reported incidence values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04076ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "722db6d8",
   "metadata": {},
   "source": [
    "The next calibration step is to tune the model to match the true incidence data. The true incidence data can be downloaded from the [Malaria Atlas Project](https://www.malariaatlas.org/) (MAP) [data website](https://data.malariaatlas.org/). You will want to select Pf data, the subnational breakdown, and then the provinces or districts you need for your given country. The province selection can be a bit cumberson (there isn't a way to select all provinces from a given country at once as of this writing in 2025) so you will likely want to have the district list handy and manually type in the names of the districts in order to select the correct data.\n",
    "\n",
    "The data is provided in a a .csv format. There is some extra columns in the data that we don't need. What we do need is the `Year`, `Name`, and `Value` columns. Additionally, you'll want to map the district numbering to the district names in the .csv as the `Id` column. \n",
    "\n",
    "Sometimes the names might not match exactly. Primarily is our mapping using an underscore instead of a space, but sometimes things like \"City\" will be present in the local language ex: \"Mexico City\" in English vs. \"Ciudad de Mxico\" in Spanish. At the moment, the most straightforward way of solving this and doing the mapping is manual entry. If you can think of a better way submit it via a PR. \n",
    "\n",
    "The first step is to create a mapping from the district rasters to the pixels and a mapping from the MAP data to the district numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d4a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "country = CountryParams.load(os.path.join(\"conf\", \"moz\", \"test\", f\"moz_country_params.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac79e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_incidence = pd.read_csv(\"data/moz/moz_incidenceperthousand.csv\")\n",
    "true_incidence = true_incidence.loc[true_incidence['Year'] == country.calibration_year].copy()\n",
    "true_incidence = true_incidence.set_index('Id')\n",
    "# sort by Id\n",
    "true_incidence = true_incidence.sort_index()\n",
    "true_incidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = pd.read_csv(os.path.join(\"output\", country.country_code, \"validation\", \"ave_cases.csv\"), index_col=0)\n",
    "population = pd.read_csv(os.path.join(\"output\", country.country_code, \"validation\", \"ave_population.csv\"), index_col=0)\n",
    "prevalence = pd.read_csv(os.path.join(\"output\", country.country_code, \"validation\", \"ave_prevalence.csv\"), index_col=0)\n",
    "districts = utils.read_raster(os.path.join(\"data\", country.country_code, f\"{country.country_code}_districts.asc\"))[0]\n",
    "\n",
    "mean_cases, mean_prevalence, mean_population = calibrate.get_last_year_statistics(cases, prevalence, population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ebf022",
   "metadata": {},
   "outputs": [],
   "source": [
    "incidence = pd.DataFrame()\n",
    "incidence[\"cases\"] = mean_cases[\"mean\"].copy()\n",
    "incidence[\"prevalence\"] = mean_prevalence[\"mean\"].copy()\n",
    "incidence[\"population\"] = mean_population[\"mean\"].copy()\n",
    "\n",
    "num_locations = incidence.index.unique()[-1] + 1\n",
    "district_encoding = np.zeros(num_locations)\n",
    "districts = districts.reshape(-1)\n",
    "districts = districts[~np.isnan(districts)]\n",
    "\n",
    "incidence[\"district\"] = districts[incidence.index.to_numpy()]\n",
    "incidence = incidence.groupby(\"district\").sum()\n",
    "incidence = incidence.drop(columns=\"prevalence\")\n",
    "incidence[\"cases_per_thousand\"] = incidence[\"cases\"] / 1000\n",
    "\n",
    "incidence[\"true_cases_per_thousand\"] = true_incidence[\"Value\"].copy()\n",
    "incidence[\"incidence_diff\"] = incidence[\"cases_per_thousand\"] - incidence[\"true_cases_per_thousand\"]\n",
    "incidence[\"percent_diff\"] = incidence[\"incidence_diff\"] / incidence[\"true_cases_per_thousand\"]\n",
    "\n",
    "incidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f89af3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
