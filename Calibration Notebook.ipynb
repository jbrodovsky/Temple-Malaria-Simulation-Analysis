{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac53736-3a00-443d-9120-a37ee038a592",
   "metadata": {},
   "source": [
    "# MaSim Country Calibration\n",
    "\n",
    "This note book is used for running the country calibration processes for eventual use in the experimental simulation process where various response strategies are modeled. This notebook and the accompanying toolkit was developed by [James Brodovsky](https://github.com/jbrodovsky) and Sarit Adhikari as part of the calibration efforts for Burkino Faso and Mozambique in 2025.\n",
    "\n",
    "## Installation\n",
    "\n",
    "Installation is in two parts. First, clone this repository to your local machine. Second, install the `masim_analysis` package into a virtual environment, e.g., using `pip` or `uv`.\n",
    "\n",
    "```bash\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "If you are using `uv` (and you should!) install via\n",
    "\n",
    "```bash\n",
    "uv pip install -e .\n",
    "```\n",
    "\n",
    "This will install the package in an editable mode, allowing you to make changes to the code and have them reflected in your local environment. If for some reason you are not seeing the changes reflected, try re-installing the package locally using the above commands again.\n",
    "\n",
    "Alternatively, and this should be done only if you are developing additional toolbox features, you can run this notebook from the root (top level) directory and direct import calls to the toolbox via \n",
    "\n",
    "```python \n",
    "from src.masim_analysis import *\n",
    "```\n",
    "where `*` is the specific module you are interested in.\n",
    "\n",
    "## Package and repo structure\n",
    "\n",
    "Please make note of the following directory structures: `conf` and `data`. These are the primary two directories for experimental country data and configuration files. The `conf` directory contains the configuration (.yml) files for the simulation, while the `data` directory contains the data files used in the simulation (typically raster `.asc` files).\n",
    "\n",
    "Each of these folders is organized by country. For example, if you are working with Mozambique, which we abrieviate as `moz`, the directory structure would look like this:\n",
    "\n",
    "```\n",
    "data/\n",
    "    moz/\n",
    "        calibration/\n",
    "        ...\n",
    "conf/\n",
    "    moz/\n",
    "        calibration/\n",
    "        ...\n",
    "```\n",
    "Additionally the templates folder contains the template files for the configuration files. These are used to generate the .yml files for the simulation. This templating system is gradually being phased out in favor of a more structured approach using Python data classes in the `configure` module. Ultimately, this package communicates with the MaSim simulation through a `.yml` configuration file that can still be created manually if you desire.\n",
    "\n",
    "## Style guide\n",
    "\n",
    "This package follows pretty strict styling guidelines for clarity and consistency. The code is formatted and linted using `ruff` and `pyright`. In particular, `pyright` is configured in standard mode, which means that it will check for type errors and other issues in the code. Warnings and errors reported by either of these tools should be addressed before submitting a pull request. In some cases, this may result in somewhat more verbose code, but it is done to ensure that the code is clear and easy to understand. The goal is to make the code as readable and maintainable as possible.\n",
    "\n",
    "## How to use this notebook\n",
    "\n",
    "This notebook should be thought of as a structured interactive prompt that guides you through the process of calibrating and validating a country. The code sections are generally organized into sections that can be run independently. The notebook breaks up individual workflows by using markdown headings and note blocks. Due to the some what long-running nature of the tasks that calibration and validation involve, you'll sometimes have to wait and shutdown the kernel the notebook is running on and pick up where you left off another time.\n",
    "\n",
    "To that end, this notebook is designed to make things organized but also segmented. Keep calibration constants (name, population, etc.) in a block below this and make sure to run that block every time you start the notebook. Module and library imports should be handled in the workflow segment you are currently working on. It is recommend (for speed of execution) to separate out imports from code execution to save time on re-importing.\n",
    "\n",
    "Workflows are generally separated by a horizontal rule:\n",
    "\n",
    "---\n",
    "\n",
    "## Calibration efforts\n",
    "\n",
    "The primary calibration point is to relate the beta parameter (rate of infection/biting) with the actual reported prevelence, given the population size of a given map pixel and treatment access rate. This involves a few steps. As a preliminary step, obtain the relevant raster files that contain population data, district mapping values, treatment access, and prevalence (pfpr2-10, or a similar name) and place it under `data/<country>`. Fictitious calibration data will be stored under `data/<country>/calibration`. Create a new branch in the git repository for the calibration process. This is important to keep track of changes and to avoid conflicts with the main branch. The main branch should be reserved as a branching off point for strategy and treatment analysis or new calibration efforts. The workflow there should be to branch off from the main branch, implement the strategies and treatments, and then merge back in any useful changes. Strategy and treament analysis shouldn't be a main contribution to the main branch. The calibration branch name should be descriptive and include the country name, e.g., `calibration-moz`.\n",
    "\n",
    "Calibration then occurs in two phases and should be done on a separate git branch. The first phase is generating the simulated data for beta calibration. This creates the fictitious configuration and data files. This concludes with several command and job files to be run on a cluster. At the moment this is configured to work on Temple University's OwlsNest cluster, but the resulting `*_cmds.txt` files simply contain a list of shell commands that execute the simulation and should be generalizable to whatever parallel computing cluster system you are using.\n",
    "\n",
    "The second phase is started when the batch processing is completed and downloaded locally to the `output/<country>/calibration` directory. These files are then summarized and the prevalence and beta values are fit using a log-sigmoid curve fit when broken down by pixel population and treatment access rate. These fits are then used to generate the beta map for eventual use in the experimental simulation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d72e28",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "These are the country specific parameters you are working within. Please run the below code block every time you start this notebook.\n",
    "\n",
    "- `name`: The country code name. Usually two or three letters. Examples: `moz` for Mozambique, `bf` for Burkino Faso, `rwa` for Rwanda, `tz` for Tanzania, etc.\n",
    "- `birth_rate`: The birth rate of the country. This is used to calculate the population growth rate. This is usually a constant value for the country. Data is typically given in births per 1000 people. Normalize that data to a decimal value.\n",
    "- `target_population`: The population of the country in the calibration target year.\n",
    "- `initial_age_structure`: The initial age bins of the country. \n",
    "- `age_distribution`: The age distribution of the country as percentages corresponding to the age bins.\n",
    "- `death_rate`: The death rate of the country corresponding to the age bins.\n",
    "- `access_rates`: the treatment access rates for the country. This is determined by the unique values in the raster file typically called `<name>_treatementseeking.asc`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb1055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ruamel.yaml import YAML\n",
    "\n",
    "from masim_analysis import configure, utils\n",
    "\n",
    "yaml = YAML()\n",
    "\n",
    "# --- Input parameters ---\n",
    "# These are theconstants for each calibration permutation that we know from population data\n",
    "name = \"moz\"\n",
    "\n",
    "# Population structure\n",
    "initial_age_structure = [\n",
    "    1,\n",
    "    5,\n",
    "    9,\n",
    "    14,\n",
    "    19,\n",
    "    24,\n",
    "    29,\n",
    "    34,\n",
    "    39,\n",
    "    44,\n",
    "    49,\n",
    "    54,\n",
    "    59,\n",
    "    64,\n",
    "    69,\n",
    "    100,\n",
    "]  # Different from the MaSim age_structure\n",
    "age_distribution = [\n",
    "    0.037,\n",
    "    0.132,\n",
    "    0.161,\n",
    "    0.142,\n",
    "    0.090,\n",
    "    0.086,\n",
    "    0.070,\n",
    "    0.052,\n",
    "    0.044,\n",
    "    0.044,\n",
    "    0.031,\n",
    "    0.041,\n",
    "    0.024,\n",
    "    0.017,\n",
    "    0.013,\n",
    "    0.017,\n",
    "]\n",
    "\n",
    "# Growth rate and population parameters\n",
    "birth_rate = 55.5 / 1000\n",
    "target_population = 33_635_160  # Total population for the current year (2023)\n",
    "initial_population, _ = utils.read_raster(os.path.join(\"data\", name, f\"{name}_initialpopulation.asc\"))\n",
    "initial_population = np.nansum(initial_population)  # Initial population for the target year\n",
    "target_growth_rate = (target_population - initial_population) / initial_population\n",
    "# death_rate = [0.049744, 0.064331, 0.064331, 0.064331, 0.064331, 0.00359, 0.00361, 0.00365, 0.00379, 0.00379, 0.133, 0.133, 0.0174, 0.0174, 0.0174] # original\n",
    "death_rate = [\n",
    "    0.048140,\n",
    "    0.041605,\n",
    "    0.052070,\n",
    "    0.048057,\n",
    "    0.048057,\n",
    "    0.00497,\n",
    "    0.00497,\n",
    "    0.00497,\n",
    "    0.00497,\n",
    "    0.003540,\n",
    "    0.00354,\n",
    "    0.00758,\n",
    "    0.01113,\n",
    "    0.01113,\n",
    "    0.01113,\n",
    "]\n",
    "\n",
    "# Get the unique treatment access rates\n",
    "treatment, _ = utils.read_raster(os.path.join(\"data\", name, f\"{name}_treatmentseeking.asc\"))\n",
    "treatment = np.unique(treatment)\n",
    "treatment = treatment[~np.isnan(treatment)]\n",
    "treatment = np.sort(treatment)\n",
    "access_rates = [float(t) for t in treatment]  # Convert to float for consistency and to make pyright happy\n",
    "\n",
    "# --- Tuneable parameters ---\n",
    "# These are the parameters that we will be tuning in the calibration process to develop a model that\n",
    "# relates the prevelence data to population, access rates, and betas.\n",
    "betas = [0.001, 0.005, 0.01, 0.0125, 0.015, 0.02, 0.03, 0.04, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.8, 1]\n",
    "population_bins = [10, 20, 30, 40, 50, 75, 100, 250, 500, 1000, 2000, 5000, 10000, 15000, 20000]\n",
    "\n",
    "# --- Calibration parameters ---\n",
    "# These are the target year for the data available as well as the number\n",
    "# of repetitions for each calibration combination. These variables\n",
    "# are constant for each calibration permutation\n",
    "calibration_year = 2022\n",
    "reps = 20\n",
    "\n",
    "# --- Derived dates ---\n",
    "starting_date = date(calibration_year - 11, 1, 1).strftime(\"%Y/%m/%d\")\n",
    "ending_date = date(calibration_year + 1, 12, 31).strftime(\"%Y/%m/%d\")\n",
    "start_of_comparison_period = date(calibration_year, 1, 1).strftime(\"%Y/%m/%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb02686",
   "metadata": {},
   "source": [
    "### Raster data exploration\n",
    "\n",
    "The first step is to read in the relevant rasters to get a sense of some of the big picture data. The rasters are stored in the `data/<country>` directory. This is then used to get some basic statistics to run calibration. Of primary concern is getting the value of the initial population, calibration year population, `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47944fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts, _ = utils.read_raster(os.path.join(\"data\", name, f\"{name}_districts.asc\"))\n",
    "population, _ = utils.read_raster(os.path.join(\"data\", name, f\"{name}_initialpopulation.asc\"))\n",
    "prevalence, _ = utils.read_raster(os.path.join(\"data\", name, f\"{name}_pfpr210.asc\"))\n",
    "district_names = pd.read_csv(os.path.join(\"data\", name, f\"{name}_mapping.csv\"), index_col=\"ID\")\n",
    "names = district_names.to_dict()[\"DISTRICT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_fig = utils.plot_districts(districts, names, \"Mozambique\", fig_size=(12, 6), loc=\"lower right\")\n",
    "pop_fig = utils.plot_population(population, \"Mozambique\", fig_size=(12, 6))\n",
    "pfpr_plot = utils.plot_prevalence(prevalence, \"Mozambique\", fig_size=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778892ee",
   "metadata": {},
   "source": [
    "### Drug distribution rates\n",
    "\n",
    "Each country has a different drug distribution rate. This will factor into the calibration file. For instances, Mozambique's distribution is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bcc7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_distribution = pd.read_csv(\n",
    "    os.path.join(\"data\", name, f\"{name}_drugdistribution.csv\"), index_col=0, na_values=\"-99\"\n",
    ")\n",
    "drug_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809cbe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_distribution.drop(2003, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f15d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_distribution.drop(columns=[\"AL\", \"aspirin\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2d4741",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72a5c4c",
   "metadata": {},
   "source": [
    "As a santiy check, the sum of the distribution rates should equal 1.0 for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba9d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_distribution[\"sum\"] = drug_distribution.sum(axis=1)\n",
    "drug_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d004d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = drug_distribution[\"sum\"]\n",
    "drug_distribution.drop(columns=[\"sum\"], inplace=True)\n",
    "drug_distribution = drug_distribution.div(totals, axis=0)\n",
    "drug_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036a87c5",
   "metadata": {},
   "source": [
    "We then relate this to the `DRUG_DB` and `THERAPY_DB` dictionary in `masim_analysis.configure`. Note that the drug distribution may be either a single drug or a combination of drugs. For instance, `al` is a combination of artemether and lumefantrine. This would correspond to a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68b4dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from masim_analysis.configure import DRUG_DB, THERAPY_DB\n",
    "\n",
    "print(\"Drugs:\")\n",
    "for idx in DRUG_DB.keys():\n",
    "    print(f\"Drug {idx}: {DRUG_DB[idx]['name']}\")\n",
    "print(\"Therapies:\")\n",
    "for idx in THERAPY_DB.keys():\n",
    "    drug_ids = THERAPY_DB[idx][\"drug_id\"]\n",
    "    print(f\"Therapy {idx}: {[DRUG_DB[drug_id]['name'] for drug_id in drug_ids]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be18a449",
   "metadata": {},
   "source": [
    "From this information we must manually construct the baseline strategy. Similar to the drug and therapy \"databases\" we store the strategies in a dictionary with integer keys. Each key corresponds to a strategy which itself is a dictionary with the following keys:\n",
    "- `name`: The name of the strategy. This is used to identify the strategy in the simulation.\n",
    "- `type`: The type of strategy. Leave this as \"MFT\" until further notice.\n",
    "- `therapy_ids`: The therapy ids used in the strategy. This is a list of integers corresponding to the therapy ids in the `THERAPY_DB` dictionary.\n",
    "- `distribution`: The distribution of the strategy. This is a list of floats corresponding to the distribution of the strategy that we derived from the drug distribution rates from DHS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b487a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: maciek thinks that ASAQ should really be AL; going to queue up this for a temporary calibration (i.e. therapy 1 instead of 0)\n",
    "os.makedirs(os.path.join(\"conf\", name, \"test\"), exist_ok=True)\n",
    "\n",
    "strategy_db = {\n",
    "    0: {\n",
    "        \"name\": \"2011\",\n",
    "        \"type\": \"MFT\",\n",
    "        \"therapy_ids\": [7, 8, 6, 4, 5, 1],\n",
    "        \"distribution\": [0.083661, 0.234252, 0.027559, 0.019685, 0, 0.589567],\n",
    "    },\n",
    "    1: {\n",
    "        \"name\": \"2015\",\n",
    "        \"type\": \"MFT\",\n",
    "        \"therapy_ids\": [7, 8, 6, 4, 5, 1],\n",
    "        \"distribution\": [0.003914, 0.07045, 0.002935, 0.001957, 0, 0.014677, 0.906067],\n",
    "    },\n",
    "    2: {\n",
    "        \"name\": \"2018\",\n",
    "        \"type\": \"MFT\",\n",
    "        \"therapy_ids\": [7, 8, 6, 4, 5, 1],\n",
    "        \"distribution\": [0.3005970, 0.006965, 0.002985, 0.00199, 0.004975, 0.00199, 0.975124],\n",
    "    },\n",
    "    3: {\n",
    "        \"name\": \"2022\",\n",
    "        \"type\": \"MFT\",\n",
    "        \"therapy_ids\": [7, 8, 6, 4, 5, 1],\n",
    "        \"distribution\": [0.025819, 0.028798, 0, 0.027805, 0.059583, 0.013903, 0.844091],\n",
    "    },\n",
    "    4: {\n",
    "        \"name\": \"2023\",\n",
    "        \"type\": \"MFT\",\n",
    "        \"therapy_ids\": [7, 8, 6, 4, 5, 1],\n",
    "        \"distribution\": [0.025819, 0.028798, 0, 0.027805, 0.059583, 0.013903, 0.844091],\n",
    "    },\n",
    "}\n",
    "yaml.dump(strategy_db, open(os.path.join(\"conf\", name, \"test\", \"strategy_db.yaml\"), \"w\"))\n",
    "\n",
    "events = [\n",
    "    {\"name\": \"2011_strategy\", \"info\": [{\"day\": starting_date, \"strategy_id\": 0}]},\n",
    "    {\"name\": \"2015_strategy\", \"info\": [{\"day\": date(2015, 1, 1).strftime(\"%Y/%m/%d\"), \"strategy_id\": 1}]},\n",
    "    {\"name\": \"2018_strategy\", \"info\": [{\"day\": date(2018, 1, 1).strftime(\"%Y/%m/%d\"), \"strategy_id\": 2}]},\n",
    "    {\"name\": \"2022_strategy\", \"info\": [{\"day\": date(2022, 1, 1).strftime(\"%Y/%m/%d\"), \"strategy_id\": 3}]},\n",
    "    {\"name\": \"2023_strategy\", \"info\": [{\"day\": date(2023, 1, 1).strftime(\"%Y/%m/%d\"), \"strategy_id\": 4}]},\n",
    "]\n",
    "yaml.dump(events, open(os.path.join(\"conf\", name, \"test\", \"events.yaml\"), \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f6e22",
   "metadata": {},
   "source": [
    "## Birth Rate Check\n",
    "\n",
    "The important calibration point is the final year population. We back out the population data from this using a birth rate. To check that the initialization raster is correct, we need to run an initial simulation run to verify this growth rate considering malria deaths as well. This is really just a sanity check to make sure the input data is correct.\n",
    "\n",
    "Run an initial check to make sure the growth rate is working correctly. Given the input file for whatever year, run a single simulation and check that the population for the target year is correct. Make sure to scale the ending population by the simulation scale factor as well. Ball park the initial beta value around 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079577a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(access_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a335cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from masim_analysis import analysis, calibrate\n",
    "\n",
    "# load the previously saved strategy and event files\n",
    "strategy_db = yaml.load(open(os.path.join(\"conf\", name, \"test\", \"strategy_db.yaml\"), \"r\"))\n",
    "events = yaml.load(open(os.path.join(\"conf\", name, \"test\", \"events.yaml\"), \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a463f50c",
   "metadata": {},
   "source": [
    "Next we'll create a simple simulation run to verify the population growth rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d2e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = 100_000\n",
    "params = configure.configure(\n",
    "    country_code=name,\n",
    "    birth_rate=birth_rate,\n",
    "    initial_age_structure=initial_age_structure,\n",
    "    age_distribution=age_distribution,\n",
    "    death_rates=death_rate,\n",
    "    starting_date=date(calibration_year - 11, 1, 1),\n",
    "    start_of_comparison_period=date(calibration_year, 1, 1),\n",
    "    ending_date=date(calibration_year + 1, 1, 1),\n",
    "    strategy_db=strategy_db,\n",
    "    calibration_str=f\"growth_validation_{pop}\",\n",
    "    beta_override=0.00,\n",
    "    population_override=pop,\n",
    "    calibration=True,\n",
    ")\n",
    "params[\"events\"].extend(events)\n",
    "params[\"artificial_rescaling_of_population_size\"] = 1.0\n",
    "\n",
    "yaml.dump(params, open(os.path.join(\"conf\", name, \"test\", f\"{name}_growth_validation_{pop}.yaml\"), \"w\"))\n",
    "\n",
    "calibrate.write_pixel_data_files(params[\"raster_db\"], pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb6f91",
   "metadata": {},
   "source": [
    "Now we'll run the simulation to verify the population growth rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ed93d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(\"conf\", name, \"test\", f\"{name}_growth_validation_{pop}.yaml\")\n",
    "os.makedirs(os.path.join(\"output\", name, \"test\"), exist_ok=True)\n",
    "output_file = os.path.join(\"output\", name, \"test\", f\"{name}_growth_validation_{pop}\")\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f\"./bin/MaSim -i ./{filename} -o ./{output_file} -r SQLiteDistrictReporter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd61af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4490f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = analysis.get_table(f\"{output_file}monthly_data_0.db\", \"monthlysitedata\")\n",
    "starting_pop = data[data[\"monthlydataid\"] == 1][\"population\"].sum()\n",
    "last_month = data[\"monthlydataid\"].unique()[-1]\n",
    "population_by_month = [\n",
    "    data[data[\"monthlydataid\"] == month][\"population\"].sum() for month in data[\"monthlydataid\"].unique()\n",
    "]\n",
    "population_by_month = np.array(population_by_month)\n",
    "ending_population = population_by_month[-1]\n",
    "growth_rate = (ending_population - starting_pop) / starting_pop\n",
    "projected_population = initial_population * (1 + growth_rate)\n",
    "print(f\"Starting population: {starting_pop}\")\n",
    "print(f\"Ending population: {ending_population}\")\n",
    "print(f\"Growth rate: {growth_rate}\")\n",
    "print(f\"Projected 2023 population: {projected_population}\")\n",
    "print(f\"Percent error: {100 * (projected_population - target_population) / target_population:.4f}%\")\n",
    "\n",
    "# plots\n",
    "population_scalar = population_by_month / starting_pop\n",
    "plt.plot(data[\"monthlydataid\"].unique(), (initial_population * population_scalar) / 1_000_000, linestyle=\"-\", color=\"b\")\n",
    "plt.axhline(y=target_population / 1_000_000, color=\"r\", linestyle=\"--\", label=\"Target Population (32.64 million)\")\n",
    "plt.axvline(x=last_month, color=\"g\", linestyle=\"--\", label=\"Calibration year (2023)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Population (millions)\")\n",
    "plt.title(\"Population by Month\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b5d675",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Seasonality Calibration\n",
    "\n",
    "Seasonality is a something of a manual process to fit precisely. The goal is to fit a curve that gives a scalar parameter that modifies the incidence rate for each day of the year. This is written to a `.csv` file historically called \"adjustment\" or \"rainfall\" and is a simple row-wise list of scalar values that effect the overall incidence or biting rate. This notebook choose to save this output as `seasonality.csv` in the `data/<country>/calibration` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfffd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "cuamba = pd.read_csv(os.path.join(\"data\", name, \"calibration\", \"incidence cuamba.csv\"))\n",
    "inharrime = pd.read_csv(os.path.join(\"data\", name, \"calibration\", \"incidence inharrime.csv\"))\n",
    "c = cuamba[[\"month_num\", \"cases/thousand\"]].copy()\n",
    "i = inharrime[[\"month_num\", \"cases/thousand\"]].copy()\n",
    "s = pd.concat([c, i], axis=0)\n",
    "s = s.dropna()\n",
    "\n",
    "x = s[\"month_num\"].to_numpy()\n",
    "y = s[\"cases/thousand\"].to_numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 2))\n",
    "ax.plot(x, y, \".b\", label=\"Data\")\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"Cases/thousand\")\n",
    "ax.set_title(\"Seasonal signal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9cc04b",
   "metadata": {},
   "source": [
    "As we know from previous work, the seasonality of malaria (and most diseases) is roughly sinusoidal. The goal is to fit a sinusoidal curve to the data. With that as well the working assumption of the Boni Lab is to use the positive half of the sine curve. There is not a strong stance on this as seasonal incidence modeling is not a major research concern of the lab. Simply put, taking the positive half of the sine curve is what previous publications from the lab has done and to avoid unneccesary problems in the peer review process, we will continue to do this. This notebook will walk through the process for the full sine curve for completeness' sake.\n",
    "\n",
    "The fitting is done using the `scipy.optimize.curve_fit` function. The function takes in a model function and the data to fit. The model function is a sinusoidal function with a phase shift, period, amplitude, and offset.\n",
    "\n",
    "$$\n",
    "s(x) = A \\sin\\left(\\frac{2 \\pi x}{P} + \\phi\\right) + B\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74f4f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonality(x, amplitude, period, phase, offset):\n",
    "    \"\"\"\n",
    "    Generate a seasonal signal according to a sinusoidal model.\n",
    "    \"\"\"\n",
    "    return amplitude * np.sin((2 * np.pi / period) * (x - phase)) + offset\n",
    "\n",
    "\n",
    "def seasonality_positive_sine(x, amplitude, period, phase, offset):\n",
    "    \"\"\"\n",
    "    Generate a seasonal signal according to a sinusoidal model.\n",
    "    \"\"\"\n",
    "    s = seasonality(x, amplitude, period, phase, offset)\n",
    "    s[s <= offset] = offset\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b690dc97",
   "metadata": {},
   "source": [
    "We are looking to fit a scalar multiplier of the base incidence data not a curve over the specific incidence. We can do this by normalizing the incidence data about the median, mean, or mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acecdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode as spmode\n",
    "\n",
    "median = float(np.median(y))\n",
    "mean = float(np.mean(y))\n",
    "mode = float(spmode(y).mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3609386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 2))\n",
    "ax.plot(x, y, \".b\", label=\"Data\")\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"Cases/thousand\")\n",
    "ax.set_title(\"Seasonal signal\")\n",
    "ax.axhline(median, color=\"r\", linestyle=\"--\", label=\"Median\")\n",
    "ax.axhline(mean, color=\"g\", linestyle=\"--\", label=\"Mean\")\n",
    "ax.axhline(mode, color=\"y\", linestyle=\"--\", label=\"Mode\")\n",
    "ax.legend(loc=\"upper right\", bbox_to_anchor=(1.12, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ef990",
   "metadata": {},
   "source": [
    "Normailze the data with respect to the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113224f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_norm = y / mean\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 2))\n",
    "ax.plot(x, y_norm, \".b\", label=\"Normalized Data\")\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"Cases / thousand\")\n",
    "ax.set_title(\"Seasonal signal\")\n",
    "ax.axhline(median / mean, color=\"r\", linestyle=\"--\", label=\"Normalized Median\")\n",
    "ax.axhline(mean / mean, color=\"g\", linestyle=\"--\", label=\"Normalized Mean\")\n",
    "ax.axhline(mode / mean, color=\"y\", linestyle=\"--\", label=\"Normalized Mode\")\n",
    "ax.legend(loc=\"upper right\", bbox_to_anchor=(1.22, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fa5d3e",
   "metadata": {},
   "source": [
    "Run the curve fit function and analyze with resepct to both the full sinusoid and the positive half of the sine curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e797f054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "fit = curve_fit(seasonality, x, y_norm, p0=[1, 365, 0, 1], maxfev=10000)\n",
    "\n",
    "coefs = fit[0]\n",
    "\n",
    "t = np.linspace(1, 12, 1000)\n",
    "\n",
    "full_sine = seasonality(x, *coefs)\n",
    "r2 = 1 - (np.sum((y_norm - full_sine) ** 2) / np.sum((y_norm - np.mean(y_norm)) ** 2))\n",
    "std_dev = np.std(y_norm - full_sine)\n",
    "\n",
    "positive_sine = seasonality_positive_sine(x, *coefs)\n",
    "r2_positive = 1 - (np.sum((y_norm - positive_sine) ** 2) / np.sum((y_norm - np.mean(y_norm)) ** 2))\n",
    "std_dev_positive = np.std(y_norm - positive_sine)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 2))\n",
    "ax.plot(x, y_norm, \".b\", label=\"Data (scaled)\")\n",
    "ax.plot(t, seasonality(t, *coefs), \"r-\", label=rf\"Fitted curve\\n$r^2$ = {r2:0.2f}\\n$\\sigma$: {std_dev:0.2f}\")\n",
    "ax.plot(\n",
    "    t,\n",
    "    seasonality_positive_sine(t, *coefs),\n",
    "    \"g--\",\n",
    "    label=rf\"Fitted curve (positive sine)\\n$r^2$ = {r2_positive:0.2f}\\n$\\sigma$: {std_dev_positive:0.2f}\",\n",
    ")\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"Incidence Scalar\")\n",
    "ax.legend(loc=\"upper right\", bbox_to_anchor=(1.28, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd788099",
   "metadata": {},
   "source": [
    "The fit is not great, but this is a relatively minor aspect of the simulation. The point is to introduce some degree of seasonal variation that is reasonable. Now let's transform the fit back to the incidence rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f871e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "ax.plot(x, y, \".b\", label=\"Data (scaled)\")\n",
    "ax.plot(t, seasonality_positive_sine(t, *coefs) * mean, \"g--\", label=\"Fitted curve; $r^2$ = %.2f\" % r2)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e4f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the fit to a file for 365 days\n",
    "fit_x = np.arange(1, 366)\n",
    "fit_y = seasonality_positive_sine(fit_x, *fit[0])\n",
    "data = pd.DataFrame({\"day\": fit_x, \"cases/thousand\": fit_y})\n",
    "data = data.set_index(\"day\")\n",
    "data.to_csv(os.path.join(\"data\", name, f\"{name}_seasonality.csv\"), index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ca7ce",
   "metadata": {},
   "source": [
    "## Run calibration data generation\n",
    "\n",
    "The unknown that we are trying to solve for is the beta value(s). We have _real_ pixel-wise _prevalence_ (pfpr2-10) data that arrises from a given beta. The goal is to generate data that matches closely the real prevalence data by varying the beta value, population size, and access rate for a simulated single pixel. We will first generate the configuration files for the calibration runs here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893310f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country calibration script\n",
    "import os\n",
    "from datetime import date\n",
    "\n",
    "from ruamel.yaml import YAML\n",
    "\n",
    "from masim_analysis import calibrate\n",
    "\n",
    "yaml = YAML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449c63df",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrate.generate_configuration_files(\n",
    "    name,\n",
    "    calibration_year,\n",
    "    population_bins,\n",
    "    access_rates,\n",
    "    betas,\n",
    "    birth_rate,\n",
    "    death_rate,\n",
    "    initial_age_structure,\n",
    "    age_distribution,\n",
    "    strategy_db=strategy_db,\n",
    "    events=events,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd31fb6d",
   "metadata": {},
   "source": [
    "Create the command and job files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrate.generate_command_and_job_files(\n",
    "    name,\n",
    "    population_bins,\n",
    "    access_rates,\n",
    "    betas,\n",
    "    reps,\n",
    "    cores=28,\n",
    "    nodes=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392c525b",
   "metadata": {},
   "source": [
    "At this point you should have all the data and configuration files you need in order to run the calibration. The next step is to commit the changes (added files) to the local branch and push to the remote branch. Log into the cluster and switch to the branch you just created:\n",
    "\n",
    "```bash\n",
    "git checkout <branch_name>\n",
    "git pull\n",
    "```\n",
    "Create the required output directories on the cluster: \n",
    "\n",
    "```bash\n",
    "mkdir -p output/<country>/calibration\n",
    "```\n",
    "\n",
    "Ensure that the jobs files (typically `.sh`) are only the specific jobs that you want to run. Delete any other job or `.sh` files. Keep in mind that any given user may only have at most 50 jobs queued on Owl's Nest at a time. Queue up the calibration jobs using the following command:\n",
    "\n",
    "```bash\n",
    "for i in $(ls *.sh); do\n",
    "    echo \"Submitting job $i\"\n",
    "    qsub $i\n",
    "done\n",
    "```\n",
    "\n",
    "This will submit all the jobs in the current directory. Give it a few minutes to queue up and then log into your cluster account and check the status of the jobs. At steady state, each job should be running approximately 28 processess simultaneously. The lower population size pixels do not take very long to run when spread out over several nodes (4-8, approximately 15-60 minutes). The larger population sizes (10k-20k) take much longer (10-12 hours).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cfc524",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Check for missing data\n",
    "\n",
    "Once the jobs are completed, you will need to download the output files from the cluster. This can be done using `scp` or `rsync`. For example, to download the output files from Owl's Nest to your local machine, you can use the following command:\n",
    "\n",
    "```bash\n",
    "scp -r <username>@<cluster_address>:Temple-Malaria-Simulation-Analysis/output <local_path>\n",
    "```\n",
    "This will download the output files to the specified local path. I recommend simply copying them to your desktop. Once you have downloaded the output files, copy them to this repo (or download directly) and place them in the `output/<country>/calibration` directory. This will allow you to run the analysis on the simulated data locally.\n",
    "\n",
    "Assuming that all or most of the calibration simulation executed successfull, delete all job and commands files in the local repository. Do not delete the files on the cluster! This will cause a git tracking issue as the repository branches will be out of sync.\n",
    "\n",
    "Prior to running the full calibration analysis, check for any missing data files. Sometimes Owl's Nest gets hung, the database file didn't write correctly, the job times out, or some other bug happened. First, a sanity check. We should have the following number of output calibration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec4eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "needed_files = len(population_bins) * len(access_rates) * len(betas) * reps\n",
    "output_dir = os.path.join(\"output\", name, \"calibration\")\n",
    "output_files = os.listdir(output_dir)\n",
    "completed_files = len(output_files)\n",
    "\n",
    "print(f\"Total calibration files: {needed_files}\")\n",
    "print(f\"Completed calibration files: {completed_files}\")\n",
    "print(f\"Missing calibration files: {needed_files - completed_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68726cfa",
   "metadata": {},
   "source": [
    "If there are missing files use the below block to check for the missing output and create the appropriate job and commands files. You should also double check that each permutation is completed using `process_missing_jobs` prior to moving onto the calibration curve fitting. If there are no missing files proceed to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7753b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from masim_analysis import calibrate\n",
    "\n",
    "base_file_path = os.path.join(\"output\", name, \"calibration\")\n",
    "summary = pd.DataFrame(columns=[\"population\", \"access_rate\", \"beta\", \"iteration\", \"pfprunder5\", \"pfpr2to10\", \"pfprall\"])\n",
    "\n",
    "comparison = date(calibration_year, 1, 1)\n",
    "year_start = comparison.strftime(\"%Y-%m-%d\")\n",
    "year_end = (comparison + pd.DateOffset(years=1)).strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f428e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrate.process_missing_jobs(\n",
    "    name,\n",
    "    population_bins,\n",
    "    access_rates,\n",
    "    betas,\n",
    "    os.path.join(\"output\"),\n",
    "    reps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e642286f",
   "metadata": {},
   "source": [
    "If there are any missing data files run them on the cluster. This can be done by running the following command:\n",
    "\n",
    "```bash\n",
    "qsub missing_calibration_runs_<pop>_job.sh\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8ab387",
   "metadata": {},
   "source": [
    "### Summarize data\n",
    "\n",
    "Once all the appropriate calibration data has been collected we need to summarize across all the individual data files. This will write the summarized results to `calibration_summary.csv` file in the `output/<country>/calibration` directory. The summary file will contain the following columns:\n",
    "- `beta`: the beta value used in the simulation\n",
    "- `population`: the population size of the pixel\n",
    "- `access_rate`: the treatment access rate of the pixel\n",
    "- `pfpr2_10`: the pfpr2-10 value of the pixel\n",
    "- `pfprunder5`: the mean pfpr under 5 value of the pixel\n",
    "- `pfprall`: the mean pfpr value of the pixel\n",
    "- `iteration` : the iteration number of the simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81af3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = calibrate.summarize_calibration_results(\n",
    "    name,\n",
    "    population_bins,\n",
    "    access_rates,\n",
    "    betas,\n",
    "    calibration_year,\n",
    "    os.path.join(\"output\"),\n",
    "    reps,\n",
    ")\n",
    "summary.to_csv(f\"{base_file_path}/calibration_means.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09abc4e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Curve fitting\n",
    "\n",
    "Now that we have summarized data that connects population size, treatment access, transmission rate, and prevelence, we can fit the data to a linear and log-sigmoid curve. Generally, the log-sigmoid appears to model the relationship better, but both methods are here for reference. This will allow us to generate a beta map for the experimental simulation. The beta map is generated by taking the beta value that corresponds to the population size and treatment access rate of each pixel. This is done using the `generate_beta_map` function, which takes the summarized data and generates a beta map for the experimental simulation.\n",
    "\n",
    "Again, the point of this fit is to relate the measured prevalence to the beta value by way of population size and treatment access rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b79cb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from masim_analysis import calibrate\n",
    "\n",
    "base_file_path = os.path.join(\"output\", name, \"calibration___\")\n",
    "means = pd.read_csv(f\"{base_file_path}/calibration_means.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5886365",
   "metadata": {},
   "source": [
    "### Linear fit\n",
    "\n",
    "We're looking to fit the pfpr to beta relationship so that we can then use the real pfpr value from the raster data to determine the beta value. So, given a specific pixel's population, pfpr, and access rate (treatmentseeking?) calculate the beta value from this fitting method. We also don't have a decent way to serialize the linear models returned from sklearn, so at the moment this is just here for demonstration purposes.\n",
    "\n",
    "Start using linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de2753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine grid size\n",
    "num_rows = len(population_bins)\n",
    "num_cols = len(access_rates)\n",
    "\n",
    "# Create subplots grid\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(4 * num_cols, 4 * num_rows), sharex=True, sharey=True)\n",
    "\n",
    "# Ensure axes is always a 2D list for consistency\n",
    "if num_rows == 1:\n",
    "    axes = np.array([axes])  # Convert to 2D array\n",
    "if num_cols == 1:\n",
    "    axes = np.array([[ax] for ax in axes])  # Convert to 2D array\n",
    "\n",
    "# Perform regression for each (Population, TreatmentAccess) group\n",
    "for i, population in enumerate(population_bins):\n",
    "    for j, treatment_access in enumerate(access_rates):\n",
    "        ax = axes[i, j]  # Select subplot location\n",
    "\n",
    "        # Filter the data for the current Population and TreatmentAccess\n",
    "        group = means[(means[\"population\"] == population) & (means[\"access_rate\"] == treatment_access)]\n",
    "\n",
    "        if group.empty:\n",
    "            ax.set_visible(False)  # Hide empty plots\n",
    "            continue\n",
    "\n",
    "        group = group.dropna(axis=0)  # drop any row in a nan column\n",
    "\n",
    "        X = group[[\"beta\"]].values\n",
    "        y = group[\"pfpr2to10\"].values\n",
    "\n",
    "        # 1. Linear Regression\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)  # type: ignore\n",
    "\n",
    "        # 2. Polynomial Regression (degree 3)\n",
    "        poly_model3 = make_pipeline(PolynomialFeatures(3), LinearRegression())\n",
    "        poly_model3.fit(X, y)\n",
    "\n",
    "        # 3. Polynomial Regression (degree 5)\n",
    "        poly_model5 = make_pipeline(PolynomialFeatures(5), LinearRegression())\n",
    "        poly_model5.fit(X, y)\n",
    "\n",
    "        # 4. Spline Regression\n",
    "        # spline_model = UnivariateSpline(group['beta'], group['pfpr2to10'], s=50)\n",
    "\n",
    "        # Plot regression\n",
    "        sns.scatterplot(x=group[\"beta\"], y=group[\"pfpr2to10\"], ax=ax, label=\"Data\", color=\"black\")\n",
    "        ax.plot(group[\"beta\"], model.predict(X), color=\"red\", linestyle=\"dashed\", label=\"Linear\")\n",
    "        ax.plot(group[\"beta\"], poly_model3.predict(X), color=\"blue\", linestyle=\"dashed\", label=\"Poly (3)\")\n",
    "        ax.plot(group[\"beta\"], poly_model5.predict(X), color=\"green\", linestyle=\"dashed\", label=\"Poly (5)\")\n",
    "        # ax.plot(group['Beta'], spline_model(X), color='purple', linestyle=\"dashed\", label=\"Spline\")\n",
    "\n",
    "        # Setting titles & labels\n",
    "        ax.set_title(f\"Population : {population}, Access : {treatment_access}\")\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(\"pfpr2to10\")\n",
    "        if i == num_rows - 1:\n",
    "            ax.set_xlabel(\"Beta\")\n",
    "        ax.legend(fontsize=7)\n",
    "\n",
    "# Adjust layout\n",
    "plt.suptitle(\"Curve Fitting by Population & Treatment Access\", fontsize=16)\n",
    "plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.96))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3310a37",
   "metadata": {},
   "source": [
    "Now we'll do a logarithemic fit for the beta value and attempt to fit a sigmoid curve. Again this is the model that typically works best for the data. The fit returned is a list of the parameters for the sigmoid function which can be easily serialized and saved to a file. The sigmoid function is defined as:\n",
    "\n",
    "$$s = \\frac{a}{1 + e^{-b(x - c)}}$$\n",
    "\n",
    "which in code is:\n",
    "\n",
    "```python\n",
    "def sigmoid(x, a, b, c):\n",
    "    return a / (1 + np.exp(-b * (x - c)))\n",
    "```\n",
    "where `a`, `b`, and `c` are the parameters of the sigmoid function. The `x` value is the beta value. The `a` parameter is the maximum value of the sigmoid function, the `b` parameter is the steepness of the curve, and the `c` parameter is the x-value of the sigmoid's midpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda2576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_map = calibrate.log_sigmoid_fit(\n",
    "    population_bins,\n",
    "    access_rates,\n",
    "    means,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd10f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = calibrate.plot_log_sigmoid(population_bins, access_rates, means, models_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a22d4",
   "metadata": {},
   "source": [
    "Write the calibrated log-sigmoid model to a file. The most straightforward way to do this is to serialize the model dictionary to a `.json` file. The built in `json` module easily handles this for Python dictionaries. This way the data is written out in a human readable and cross language format. One change that needs to be made is to convert the numpy arrays to lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca22e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rate in models_map.keys():\n",
    "    for pop in models_map[rate].keys():\n",
    "        model = models_map[rate][pop]\n",
    "        if model is None:\n",
    "            models_map[rate][pop] = [0, 0, 0]\n",
    "        else:\n",
    "            try:\n",
    "                models_map[rate][pop] = model.tolist()\n",
    "            except AttributeError:\n",
    "                pass  # in case the model is not a numpy array\n",
    "\n",
    "with open(os.path.join(\"data\", name, \"calibration\", \"models_map.json\"), \"w\") as f:  # noqa: F811, ruff disabled\n",
    "    json.dump(models_map, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855507fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Keep in mind that the built in JSON serialization and deserialization reads in all the values as strings. To get around this use `load_beta_model` from the `calibrate` module instead of the native `json.load()` function. This will convert the values back to numeric values and lists where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a8c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from src.masim_analysis import calibrate, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b090131",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_map = calibrate.load_beta_model(os.path.join(\"data\", name, \"calibration\", \"models_map.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f721fd",
   "metadata": {},
   "source": [
    "We can then use this model to generate the beta map for the experimental simulation. The beta map is generated by taking the beta value that corresponds to the population size and treatment access rate of each pixel. This is done using the `create_beta_map` function, which takes the summarized data and generates a beta map for the experimental simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e425e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "population, _ = utils.read_raster(\"data/moz/moz_population.asc\")\n",
    "prevalence, _ = utils.read_raster(\"data/moz/moz_pfpr210.asc\")\n",
    "treatment, _ = utils.read_raster(\"data/moz/moz_treatmentseeking.asc\")\n",
    "districts = utils.read_raster(\"data/moz/moz_districts.asc\")\n",
    "beta_map = calibrate.create_beta_map(models_map, population, treatment, prevalence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37277b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(beta_map, cmap=\"hot\")\n",
    "plt.colorbar(label=\"Beta\")\n",
    "plt.title(\"Pixel-wise Beta Map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7bbe13",
   "metadata": {},
   "source": [
    "Now we'll use the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d2bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7286aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.write_raster(beta_map, os.path.join(\"data\", name, f\"{name}_beta.asc\"), 195196.26821073, 7004178.4200866)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a878ac6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Validation\n",
    "\n",
    "The validation process is similar to the calibration process except that we now use the fitted modelled beta map to attempt to recreate the prevelance map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00354546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "\n",
    "from masim_analysis import commands, configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the previously saved strategy and event files\n",
    "strategy_db = yaml.load(open(os.path.join(\"conf\", name, \"test\", \"strategy_db.yaml\"), \"r\"))\n",
    "events = yaml.load(open(os.path.join(\"conf\", name, \"test\", \"events.yaml\"), \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d033df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = configure.configure(\n",
    "    country_code=name,\n",
    "    birth_rate=birth_rate,\n",
    "    initial_age_structure=initial_age_structure,\n",
    "    age_distribution=age_distribution,\n",
    "    death_rates=death_rate,\n",
    "    starting_date=date(calibration_year - 11, 1, 1),\n",
    "    start_of_comparison_period=date(calibration_year, 1, 1),\n",
    "    ending_date=date(calibration_year + 1, 1, 1),\n",
    "    strategy_db=strategy_db,\n",
    "    # calibration_str=f\"validation\",\n",
    "    calibration=False,\n",
    ")\n",
    "params[\"events\"].extend(events)\n",
    "# params['artificial_rescaling_of_population_size'] = 0\n",
    "\n",
    "yaml.dump(params, open(os.path.join(\"conf\", name, f\"{name}_validation.yml\"), \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3121ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "file, cmds = commands.generate_commands(\n",
    "    os.path.join(\"conf\", name, f\"{name}_validation.yml\"), os.path.join(\"output\", name, f\"{name}_validation\"), 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58e2335",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"moz_validation.txt\", \"w\") as f:\n",
    "    for cmd in cmds:\n",
    "        f.write(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93ebba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "commands.generate_job_file(\n",
    "    \"moz_validation.txt\",\n",
    "    \"validation\",\n",
    "    cores_override=4,\n",
    "    nodes_override=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7856312",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next up plot the predicted vs true prevelence values. Several plots, sort by population size. There will be some misfit sections of the modeled data where there are zero population in the model but actual population in the data. The Malaria Atlas project sometimes project prevelence where there is no population. Use median simulation validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f824cb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from masim_analysis import analysis\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "021b4d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_path = os.path.join(\"output\", name, f\"{name}_validation\")\n",
    "data = analysis.get_table(os.path.join(validation_path, f\"{name}_validation_monthly_data_0.db\"), \"monthlysitedata\")\n",
    "target_month = data[\"monthlydataid\"].unique()[-13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0a9d011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfprunder5 = pd.DataFrame()\n",
    "pfpr2to10 = pd.DataFrame()\n",
    "pfprall = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7fad6eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for (root, dirs, files) in os.walk(validation_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".db\"):\n",
    "            site_data = analysis.get_table(os.path.join(root, file), \"monthlysitedata\")\n",
    "            last_month = site_data.loc[site_data[\"monthlydataid\"] == target_month]\n",
    "            try:\n",
    "                pfprunder5 = pd.merge(pfprunder5, last_month[[\"locationid\", \"pfprunder5\"]], how=\"outer\", on=\"locationid\", suffixes=(\"\", f\"_{i}\"))\n",
    "                pfpr2to10 = pd.merge(pfpr2to10, last_month[[\"locationid\", \"pfpr2to10\"]], how=\"outer\", on=\"locationid\", suffixes=(\"\", f\"_{i}\"))\n",
    "                pfprall = pd.merge(pfprall, last_month[[\"locationid\", \"pfprall\"]], how=\"outer\", on=\"locationid\", suffixes=(\"\", f\"_{i}\"))\n",
    "            except KeyError as e:\n",
    "                pfprunder5 = pd.concat([pfprunder5, last_month[[\"locationid\", \"pfprunder5\"]]])\n",
    "                pfpr2to10 =  pd.concat([pfpr2to10,   last_month[[\"locationid\", \"pfpr2to10\"]]])\n",
    "                pfprall =    pd.concat([pfprall,       last_month[[\"locationid\", \"pfprall\"]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b93766ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfprunder5[\"mean\"] = pfprunder5.mean(axis=1)\n",
    "pfpr2to10[\"mean\"] = pfpr2to10.mean(axis=1)\n",
    "pfprall[\"mean\"] = pfprall.mean(axis=1)\n",
    "\n",
    "pfprunder5[\"median\"] = pfprunder5.median(axis=1)\n",
    "pfpr2to10[\"median\"] = pfpr2to10.median(axis=1)\n",
    "pfprall[\"median\"] = pfprall.median(axis=1)\n",
    "\n",
    "pfprunder5[\"std\"] = pfprunder5.std(axis=1)\n",
    "pfpr2to10[\"std\"] = pfpr2to10.std(axis=1)\n",
    "pfprall[\"std\"] = pfprall.std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b730dc9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locationid</th>\n",
       "      <th>pfprall</th>\n",
       "      <th>pfprall_1</th>\n",
       "      <th>pfprall_1</th>\n",
       "      <th>pfprall_1</th>\n",
       "      <th>pfprall_1</th>\n",
       "      <th>pfprall_1</th>\n",
       "      <th>pfprall_1</th>\n",
       "      <th>pfprall_1</th>\n",
       "      <th>pfprall_1</th>\n",
       "      <th>...</th>\n",
       "      <th>pfprall_1</th>\n",
       "      <th>pfprall_1</th>\n",
       "      <th>pfprall_1</th>\n",
       "      <th>pfprall_1</th>\n",
       "      <th>pfprall_1</th>\n",
       "      <th>pfprall_1</th>\n",
       "      <th>pfprall_1</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>28.517236</td>\n",
       "      <td>28.596956</td>\n",
       "      <td>28.516836</td>\n",
       "      <td>28.557751</td>\n",
       "      <td>28.241610</td>\n",
       "      <td>28.495485</td>\n",
       "      <td>28.477068</td>\n",
       "      <td>28.337503</td>\n",
       "      <td>28.646586</td>\n",
       "      <td>...</td>\n",
       "      <td>28.599856</td>\n",
       "      <td>28.640713</td>\n",
       "      <td>28.324174</td>\n",
       "      <td>28.319008</td>\n",
       "      <td>28.403634</td>\n",
       "      <td>28.439760</td>\n",
       "      <td>28.456569</td>\n",
       "      <td>27.172049</td>\n",
       "      <td>28.469564</td>\n",
       "      <td>5.725373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>25.801999</td>\n",
       "      <td>25.832657</td>\n",
       "      <td>25.807747</td>\n",
       "      <td>25.806773</td>\n",
       "      <td>25.859168</td>\n",
       "      <td>25.763727</td>\n",
       "      <td>25.953647</td>\n",
       "      <td>25.772787</td>\n",
       "      <td>25.701411</td>\n",
       "      <td>...</td>\n",
       "      <td>25.770648</td>\n",
       "      <td>25.886655</td>\n",
       "      <td>25.893478</td>\n",
       "      <td>25.816601</td>\n",
       "      <td>25.908532</td>\n",
       "      <td>25.895429</td>\n",
       "      <td>25.901597</td>\n",
       "      <td>24.701148</td>\n",
       "      <td>25.812174</td>\n",
       "      <td>4.965206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>27.108430</td>\n",
       "      <td>27.236892</td>\n",
       "      <td>27.038211</td>\n",
       "      <td>27.155310</td>\n",
       "      <td>27.139339</td>\n",
       "      <td>27.275840</td>\n",
       "      <td>27.044998</td>\n",
       "      <td>27.190922</td>\n",
       "      <td>26.852700</td>\n",
       "      <td>...</td>\n",
       "      <td>27.069930</td>\n",
       "      <td>27.196758</td>\n",
       "      <td>26.932213</td>\n",
       "      <td>27.123503</td>\n",
       "      <td>26.970997</td>\n",
       "      <td>27.128845</td>\n",
       "      <td>27.379377</td>\n",
       "      <td>25.974773</td>\n",
       "      <td>27.115967</td>\n",
       "      <td>5.026235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>30.488550</td>\n",
       "      <td>30.343599</td>\n",
       "      <td>30.597751</td>\n",
       "      <td>30.553598</td>\n",
       "      <td>30.417350</td>\n",
       "      <td>30.632757</td>\n",
       "      <td>30.406016</td>\n",
       "      <td>30.272205</td>\n",
       "      <td>30.537311</td>\n",
       "      <td>...</td>\n",
       "      <td>30.665287</td>\n",
       "      <td>30.318320</td>\n",
       "      <td>30.593457</td>\n",
       "      <td>30.292033</td>\n",
       "      <td>30.438432</td>\n",
       "      <td>30.537675</td>\n",
       "      <td>30.319151</td>\n",
       "      <td>29.209801</td>\n",
       "      <td>30.450551</td>\n",
       "      <td>5.514789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>27.008734</td>\n",
       "      <td>26.856882</td>\n",
       "      <td>26.896012</td>\n",
       "      <td>26.938881</td>\n",
       "      <td>26.969489</td>\n",
       "      <td>26.989415</td>\n",
       "      <td>26.960759</td>\n",
       "      <td>26.987917</td>\n",
       "      <td>27.165633</td>\n",
       "      <td>...</td>\n",
       "      <td>26.928561</td>\n",
       "      <td>27.087664</td>\n",
       "      <td>27.016276</td>\n",
       "      <td>26.885644</td>\n",
       "      <td>27.157802</td>\n",
       "      <td>27.008917</td>\n",
       "      <td>27.001893</td>\n",
       "      <td>25.947119</td>\n",
       "      <td>26.978703</td>\n",
       "      <td>4.582142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>23.400144</td>\n",
       "      <td>23.355071</td>\n",
       "      <td>23.253252</td>\n",
       "      <td>23.346554</td>\n",
       "      <td>23.312341</td>\n",
       "      <td>23.462219</td>\n",
       "      <td>23.434154</td>\n",
       "      <td>23.328135</td>\n",
       "      <td>23.312332</td>\n",
       "      <td>...</td>\n",
       "      <td>23.306697</td>\n",
       "      <td>23.337872</td>\n",
       "      <td>23.414352</td>\n",
       "      <td>23.434574</td>\n",
       "      <td>23.284166</td>\n",
       "      <td>23.396202</td>\n",
       "      <td>23.440592</td>\n",
       "      <td>22.525408</td>\n",
       "      <td>23.342213</td>\n",
       "      <td>3.615135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>22.948439</td>\n",
       "      <td>22.985109</td>\n",
       "      <td>23.100676</td>\n",
       "      <td>22.986689</td>\n",
       "      <td>23.029688</td>\n",
       "      <td>23.076893</td>\n",
       "      <td>22.978601</td>\n",
       "      <td>22.989869</td>\n",
       "      <td>23.057139</td>\n",
       "      <td>...</td>\n",
       "      <td>23.007111</td>\n",
       "      <td>22.993351</td>\n",
       "      <td>23.124835</td>\n",
       "      <td>22.994154</td>\n",
       "      <td>23.039184</td>\n",
       "      <td>22.985111</td>\n",
       "      <td>23.021653</td>\n",
       "      <td>22.250603</td>\n",
       "      <td>22.993752</td>\n",
       "      <td>3.335661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>28.088698</td>\n",
       "      <td>27.737764</td>\n",
       "      <td>27.863763</td>\n",
       "      <td>27.864231</td>\n",
       "      <td>28.150840</td>\n",
       "      <td>28.009811</td>\n",
       "      <td>27.936932</td>\n",
       "      <td>27.809604</td>\n",
       "      <td>28.128597</td>\n",
       "      <td>...</td>\n",
       "      <td>27.815252</td>\n",
       "      <td>28.007099</td>\n",
       "      <td>28.127361</td>\n",
       "      <td>28.153599</td>\n",
       "      <td>28.119323</td>\n",
       "      <td>27.950706</td>\n",
       "      <td>27.997878</td>\n",
       "      <td>27.017194</td>\n",
       "      <td>27.943819</td>\n",
       "      <td>4.160900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>24.936371</td>\n",
       "      <td>24.675650</td>\n",
       "      <td>24.596876</td>\n",
       "      <td>24.697312</td>\n",
       "      <td>24.784304</td>\n",
       "      <td>24.734409</td>\n",
       "      <td>24.680401</td>\n",
       "      <td>24.622086</td>\n",
       "      <td>24.796324</td>\n",
       "      <td>...</td>\n",
       "      <td>24.698142</td>\n",
       "      <td>24.576381</td>\n",
       "      <td>24.675793</td>\n",
       "      <td>24.693685</td>\n",
       "      <td>24.761548</td>\n",
       "      <td>24.664087</td>\n",
       "      <td>24.728865</td>\n",
       "      <td>23.958801</td>\n",
       "      <td>24.687043</td>\n",
       "      <td>3.272417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>28.334386</td>\n",
       "      <td>28.258453</td>\n",
       "      <td>28.401589</td>\n",
       "      <td>28.366398</td>\n",
       "      <td>28.396208</td>\n",
       "      <td>28.353634</td>\n",
       "      <td>28.406043</td>\n",
       "      <td>28.347613</td>\n",
       "      <td>28.292424</td>\n",
       "      <td>...</td>\n",
       "      <td>28.323435</td>\n",
       "      <td>28.257072</td>\n",
       "      <td>28.262743</td>\n",
       "      <td>28.282077</td>\n",
       "      <td>28.333320</td>\n",
       "      <td>28.369133</td>\n",
       "      <td>28.323714</td>\n",
       "      <td>27.445242</td>\n",
       "      <td>28.318088</td>\n",
       "      <td>3.815899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   locationid    pfprall  pfprall_1  pfprall_1  pfprall_1  pfprall_1  \\\n",
       "0           1  28.517236  28.596956  28.516836  28.557751  28.241610   \n",
       "1           2  25.801999  25.832657  25.807747  25.806773  25.859168   \n",
       "2           3  27.108430  27.236892  27.038211  27.155310  27.139339   \n",
       "3           4  30.488550  30.343599  30.597751  30.553598  30.417350   \n",
       "4           5  27.008734  26.856882  26.896012  26.938881  26.969489   \n",
       "5           6  23.400144  23.355071  23.253252  23.346554  23.312341   \n",
       "6           7  22.948439  22.985109  23.100676  22.986689  23.029688   \n",
       "7           8  28.088698  27.737764  27.863763  27.864231  28.150840   \n",
       "8           9  24.936371  24.675650  24.596876  24.697312  24.784304   \n",
       "9          10  28.334386  28.258453  28.401589  28.366398  28.396208   \n",
       "\n",
       "   pfprall_1  pfprall_1  pfprall_1  pfprall_1  ...  pfprall_1  pfprall_1  \\\n",
       "0  28.495485  28.477068  28.337503  28.646586  ...  28.599856  28.640713   \n",
       "1  25.763727  25.953647  25.772787  25.701411  ...  25.770648  25.886655   \n",
       "2  27.275840  27.044998  27.190922  26.852700  ...  27.069930  27.196758   \n",
       "3  30.632757  30.406016  30.272205  30.537311  ...  30.665287  30.318320   \n",
       "4  26.989415  26.960759  26.987917  27.165633  ...  26.928561  27.087664   \n",
       "5  23.462219  23.434154  23.328135  23.312332  ...  23.306697  23.337872   \n",
       "6  23.076893  22.978601  22.989869  23.057139  ...  23.007111  22.993351   \n",
       "7  28.009811  27.936932  27.809604  28.128597  ...  27.815252  28.007099   \n",
       "8  24.734409  24.680401  24.622086  24.796324  ...  24.698142  24.576381   \n",
       "9  28.353634  28.406043  28.347613  28.292424  ...  28.323435  28.257072   \n",
       "\n",
       "   pfprall_1  pfprall_1  pfprall_1  pfprall_1  pfprall_1       mean  \\\n",
       "0  28.324174  28.319008  28.403634  28.439760  28.456569  27.172049   \n",
       "1  25.893478  25.816601  25.908532  25.895429  25.901597  24.701148   \n",
       "2  26.932213  27.123503  26.970997  27.128845  27.379377  25.974773   \n",
       "3  30.593457  30.292033  30.438432  30.537675  30.319151  29.209801   \n",
       "4  27.016276  26.885644  27.157802  27.008917  27.001893  25.947119   \n",
       "5  23.414352  23.434574  23.284166  23.396202  23.440592  22.525408   \n",
       "6  23.124835  22.994154  23.039184  22.985111  23.021653  22.250603   \n",
       "7  28.127361  28.153599  28.119323  27.950706  27.997878  27.017194   \n",
       "8  24.675793  24.693685  24.761548  24.664087  24.728865  23.958801   \n",
       "9  28.262743  28.282077  28.333320  28.369133  28.323714  27.445242   \n",
       "\n",
       "      median       std  \n",
       "0  28.469564  5.725373  \n",
       "1  25.812174  4.965206  \n",
       "2  27.115967  5.026235  \n",
       "3  30.450551  5.514789  \n",
       "4  26.978703  4.582142  \n",
       "5  23.342213  3.615135  \n",
       "6  22.993752  3.335661  \n",
       "7  27.943819  4.160900  \n",
       "8  24.687043  3.272417  \n",
       "9  28.318088  3.815899  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfprall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c696f46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
