{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac53736-3a00-443d-9120-a37ee038a592",
   "metadata": {},
   "source": [
    "# MaSim Country Calibration\n",
    "\n",
    "This note book is used for running the country calibration processes for eventual use in the experimental simulation process. This notebook and the accompanying toolkit was developed by James Brodovsky and Sarit Adhikari as part of the calibration efforts for Burkino Faso and Mozambique in 2025.\n",
    "\n",
    "## Installation\n",
    "\n",
    "The prefered use case is to install the toolbox package locally into a virtual environement. If you have cloned this repository, you can install the package using pip. \n",
    "\n",
    "```bash\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "If you are using `uv` install via\n",
    "\n",
    "```bash\n",
    "uv pip install -e .\n",
    "```\n",
    "\n",
    "This will install the package in editable mode, allowing you to make changes to the code and have them reflected in your local environment. If for some reason you are not seeing the changes reflected, try re-installing the package locally.\n",
    "\n",
    "Alternatively, and this should be done only if you are developing additional toolbox features, you can run this notebook from the root (top level) directory and direct import calls to the toolbox via \n",
    "\n",
    "```python \n",
    "from src.masim_analysis import *\n",
    "```\n",
    "where `*` is the specific module you are interested in.\n",
    "\n",
    "## Packge and repo structure\n",
    "\n",
    "Please make note of the following directory structures: `conf` and `data`. These are the primary two directories for experimental country data and configuration files. The `conf` directory contains the configuration (.yml) files for the simulation, while the `data` directory contains the data files used in the simulation (typically raster files).\n",
    "\n",
    "Each of these folders is organized by country. For example, if you are working with Mozambique, the directory structure would look like this:\n",
    "\n",
    "```\n",
    "data/\n",
    "    moz/\n",
    "        calibration/\n",
    "        ...\n",
    "conf/\n",
    "    moz/\n",
    "        calibration/\n",
    "        ...\n",
    "```\n",
    "Additionally the templates folder contains the template files for the configuration files. These are used to generate the .yml files for the simulation.\n",
    "\n",
    "## How to use this notebook\n",
    "\n",
    "This notebook should be thought of as a structured interactivie prompt. The notes sections guide you through the process of calibrating and validating a country. The code sections are generally organized into sections that can be run independently. The notebook breaks up individual workflows by using markdown headings and note blocks. Due to the some what long-running nature of the tasks that calibration and validation invlove, you'll sometimes have to wait and shutdown the kernel the notebook is running on and pick up where you left off another time.\n",
    "\n",
    "To that end, this notebook is designed to make things organized but also segmented. Keep calibration constants (name, population, etc.) in a block below this and make sure to run that block every time you start the notebook. Module and library imports should be handled in the workflow segement you are currently working on. It is recommend (for speed of execution) to seperate out imports from code execution to save time on re-importing.\n",
    "\n",
    "Workflows are generally separated by a horizontal rule:\n",
    "\n",
    "---\n",
    "\n",
    "## Calibration efforts\n",
    "\n",
    "The primary calibration point is to relate the beta parameter (rate of infection/biting) with the population size of a given map pixel given that pixel's treatment access rate. This involves a few steps. As a preliminary step, obtain the relevant raster files that contain population data, district mapping values, treatment, and prevlence (pfpr2-10, or a similar name) and place it under `data/<country>`. Ficticious calibration data will be stored under `data/<country>/calibration`.\n",
    "\n",
    "Calibration then occurs in two phases and should be done on a seperate git branch. The first phase is generating the simulated data for beta calibration. This creates the fictious configuration and data files. This concludes with several command and job files to be run on a cluster. At the momenet this is configured to work on Temple University's OwlsNest cluster, but the resulting `*_cmds.txt` files simply contain a list of shell commands that execute the simulation and should be generalizable to whatever parallel computing system you are using.\n",
    "\n",
    "The second phase is started when the batch processing is completed and downloaded locally to the `output/<country>/calibration` directory. These files are then summarized and the prevelence and beta values are fit using a log-sigmoid curve fit when broken down by pixel population and treatment access rate. These fits are then used to generate the beta map for eventual use in the experimental simulation.\n",
    "\n",
    "Validation phase?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d72e28",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "These are the country specific parameters you are working within. Please run the below code block every time you start this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b425d994",
   "metadata": {},
   "source": [
    "### Read Rasters \n",
    "\n",
    "The first step is to read in the relevant rasters. The rasters are stored in the `data/<country>` directory. This is then used to get some basic statistics to run calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3bea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from masim_analysis import utils\n",
    "import os\n",
    "\n",
    "# Input parameters (sonstants for each calibration permutation)\n",
    "name = \"moz\"\n",
    "birth_rate = 31.2 / 1000\n",
    "death_rate = [0.049744, 0.064331, 0.064331, 0.064331, 0.064331, 0.00359, 0.00361, 0.00365, 0.00379, 0.00379, 0.133, 0.133, 0.0174, 0.0174]\n",
    "age_distribution = [ 0.037, 0.132, 0.161, 0.142, 0.090, 0.086, 0.070, 0.052, 0.044, 0.044, 0.031, 0.041, 0.024, 0.017, 0.013, 0.017]\n",
    "\n",
    "# Calibration parameters (variables for each calibration permutation)\n",
    "betas = [0.001, 0.005, 0.01, 0.0125, 0.015, 0.02, 0.03, 0.04, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.8, 1]\n",
    "calibration_year = 2022  # This is the year from which we have pfpr data\n",
    "reps = 20\n",
    "\n",
    "# Read in the raster data\n",
    "districts, _ =  utils.read_raster(os.path.join(\"data\", name, \"moz_districts.asc\"))\n",
    "population, _ = utils.read_raster(os.path.join(\"data\", name, \"moz_population.asc\"))\n",
    "prevalence, _ = utils.read_raster(os.path.join(\"data\", name, \"moz_pfpr210.asc\"))\n",
    "treatment, _ =  utils.read_raster(os.path.join(\"data\", name, \"moz_treatmentseeking.asc\"))\n",
    "\n",
    "# Get the unique treatment access rates\n",
    "access_rates = np.unique(treatment)\n",
    "access_rates = access_rates[~np.isnan(access_rates)]\n",
    "access_rates = np.sort(access_rates)\n",
    "access_rates = access_rates.tolist()\n",
    "\n",
    "# Set population bins\n",
    "population_bins = [10, 20, 30, 40, 50, 75, 100, 250, 500, 1000, 2000, 5000, 10000, 15000, 20000]\n",
    "\n",
    "# Get districts\n",
    "district_names = pd.read_csv(os.path.join(\"data\", name, f\"{name}_mapping.csv\"), index_col=\"ID\")\n",
    "names = district_names.to_dict()[\"DISTRICT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9779510d",
   "metadata": {},
   "source": [
    "#### Plot districts\n",
    "\n",
    "Plot the rasters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8629d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_fig = utils.plot_districts(districts, names, \"Mozambique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcae1e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_fig = utils.plot_population(population, \"Mozambique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295e37da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfpr_plot = utils.plot_prevalence(prevalence, \"Mozambique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b5d675",
   "metadata": {},
   "source": [
    "## Seasonality Calibration\n",
    "\n",
    "Seasonality is a something of a manual process to fit precisely. The goal is to fit a curve that gives a scalar parameter that modifies the incidence rate for each day of the year. This is written to a `.csv` file call \"adjustment\" or \"rainfall\" and is a simple row-wise list of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfffd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "cuamba = pd.read_csv(os.path.join(\"data\", name, \"calibration\", \"incidence cuamba.csv\"))\n",
    "inharrime = pd.read_csv(os.path.join(\"data\", name, \"calibration\", \"incidence inharrime.csv\"))\n",
    "c = cuamba[['month_num', 'cases/thousand']].copy()\n",
    "i = inharrime[['month_num', 'cases/thousand']].copy()\n",
    "s = pd.concat([c, i], axis=0)\n",
    "s = s.dropna()\n",
    "\n",
    "x = s['month_num'].to_numpy()\n",
    "y = s['cases/thousand'].to_numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,3))\n",
    "ax.plot(x, y, 'ob', label='Data')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Cases/thousand')\n",
    "ax.set_title('Seasonal signal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9cc04b",
   "metadata": {},
   "source": [
    "As we can see, and as we know from previous work, the seasonality of malaria (and most diseases) is roughly sinusoidal. The goal is to fit a sinusoidal curve to the data. This is done using the `scipy.optimize.curve_fit` function. The function takes in a model function and the data to fit. \n",
    "\n",
    "The model function is a sinusoidal function with a phase shift, period, amplitude, and offset.\n",
    "\n",
    "$$\n",
    "s(x) = A \\sin\\left(\\frac{2 \\pi x}{P} + \\phi\\right) + B\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74f4f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonality(x, amplitude, period, phase, offset):\n",
    "    \"\"\"\n",
    "    Generate a seasonal signal according to a sinusoidal model.\n",
    "    \"\"\"\n",
    "    return amplitude * np.sin((2 * np.pi / period) * (x - phase)) + offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b690dc97",
   "metadata": {},
   "source": [
    "Additionally we are looking to fit a scalar multiplier of the base incidence data not a curve over the specific incidence. We can do this by normalizing the incidence data about the median, mean, or mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acecdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode as spmode\n",
    "\n",
    "median = np.median(y)\n",
    "mean = np.mean(y)\n",
    "mode = spmode(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3609386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,3))\n",
    "ax.plot(x, y, 'ob', label='Data')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Cases/thousand')\n",
    "ax.set_title('Seasonal signal')\n",
    "ax.axhline(median, color='r', linestyle='--', label='Median')\n",
    "ax.axhline(mean, color='g', linestyle='--', label='Mean')\n",
    "ax.axhline(mode.mode, color='y', linestyle='--', label='Mode')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ef990",
   "metadata": {},
   "source": [
    "Choose the one that appears to fit the best, probably median. Next normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367c0d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_norm = y / median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113224f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,3))\n",
    "ax.plot(x, y_norm, 'ob', label='Normalized Data')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Cases/thousand')\n",
    "ax.set_title('Seasonal signal')\n",
    "ax.axhline(median / median, color='r', linestyle='--', label='Normalized Median')\n",
    "ax.axhline(mean / median, color='g', linestyle='--', label='Normalized Mean')\n",
    "ax.axhline(mode.mode / median, color='y', linestyle='--', label='Normalized Mode')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e797f054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "fit = curve_fit(seasonality, x, y_norm, p0=[1, 365, 0, 1])\n",
    "\n",
    "t = np.linspace(0, 12, 1000)\n",
    "r2 = 1 - (np.sum((y_norm - seasonality(x, *fit[0]))**2) / np.sum((y_norm - np.mean(y_norm))**2))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,3))\n",
    "ax.plot(x, y_norm, 'ob', label='Data (scaled)')\n",
    "ax.plot(t, seasonality(t, *fit[0]), 'r-', label='Fitted curve; R2 = %.2f' % r2)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd788099",
   "metadata": {},
   "source": [
    "The fit is not great, but this is a relatively minor aspect of the simulation. The point is to introduce some degree of seasonal variation that is reasonable. Now let's transform the fit back to the incidence rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f871e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,3))\n",
    "ax.plot(x, y, 'ob', label='Data (scaled)')\n",
    "ax.plot(t, seasonality(t, *fit[0]) * median, 'r-', label='Fitted curve; R2 = %.2f' % r2)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e4f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the fit to a file for 365 days\n",
    "fit_x = np.arange(1, 366)\n",
    "fit_y = seasonality(fit_x, *fit[0])\n",
    "data = pd.DataFrame({\"day\": fit_x, \"cases/thousand\": fit_y})\n",
    "data.index = data[\"day\"]\n",
    "data = data.drop(columns=[\"day\"])\n",
    "data.to_csv(os.path.join(\"data\", name, f\"{name}_seasonality.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ca7ce",
   "metadata": {},
   "source": [
    "## Run calibration data generation\n",
    "\n",
    "The unknown that we are trying to solve for is the beta value(s). We have _real_ pixel-wise _prevalence_ (pfpr2-10) data that arrises from a given beta. The goal is to generate data that matches closely the real prevalence data by varying the beta value, population size, and access rate for a simulated single pixel. We will first generate the configuration files for the calibration runs here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893310f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country calibration script\n",
    "import os\n",
    "from datetime import date\n",
    "from ruamel.yaml import YAML\n",
    "from masim_analysis import calibrate\n",
    "\n",
    "yaml = YAML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c9779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration files\n",
    "calibrate.generate_configuration_files(\n",
    "    name,\n",
    "    calibration_year,\n",
    "    population_bins,\n",
    "    access_rates,\n",
    "    betas,\n",
    "    birth_rate,\n",
    "    death_rate,\n",
    "    age_distribution,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "775c551a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [00:00<00:00, 58.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commands file: moz_10_cmds.txt\n",
      "Job script: moz_10_jobs.sh\n",
      "To submit the job, run: qsub moz_10_jobs.sh\n",
      "Commands file: moz_20_cmds.txt\n",
      "Job script: moz_20_jobs.sh\n",
      "To submit the job, run: qsub moz_20_jobs.sh\n",
      "Commands file: moz_30_cmds.txt\n",
      "Job script: moz_30_jobs.sh\n",
      "To submit the job, run: qsub moz_30_jobs.sh\n",
      "Commands file: moz_40_cmds.txt\n",
      "Job script: moz_40_jobs.sh\n",
      "To submit the job, run: qsub moz_40_jobs.sh\n",
      "Commands file: moz_50_cmds.txt\n",
      "Job script: moz_50_jobs.sh\n",
      "To submit the job, run: qsub moz_50_jobs.sh\n",
      "Commands file: moz_75_cmds.txt\n",
      "Job script: moz_75_jobs.sh\n",
      "To submit the job, run: qsub moz_75_jobs.sh\n",
      "Commands file: moz_100_cmds.txt\n",
      "Job script: moz_100_jobs.sh\n",
      "To submit the job, run: qsub moz_100_jobs.sh\n",
      "Commands file: moz_250_cmds.txt\n",
      "Job script: moz_250_jobs.sh\n",
      "To submit the job, run: qsub moz_250_jobs.sh\n",
      "Commands file: moz_500_cmds.txt\n",
      "Job script: moz_500_jobs.sh\n",
      "To submit the job, run: qsub moz_500_jobs.sh\n",
      "Commands file: moz_1000_cmds.txt\n",
      "Job script: moz_1000_jobs.sh\n",
      "To submit the job, run: qsub moz_1000_jobs.sh\n",
      "Commands file: moz_2000_cmds.txt\n",
      "Job script: moz_2000_jobs.sh\n",
      "To submit the job, run: qsub moz_2000_jobs.sh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 49.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commands file: moz_5000_cmds.txt\n",
      "Job script: moz_5000_jobs.sh\n",
      "To submit the job, run: qsub moz_5000_jobs.sh\n",
      "Commands file: moz_10000_cmds.txt\n",
      "Job script: moz_10000_jobs.sh\n",
      "To submit the job, run: qsub moz_10000_jobs.sh\n",
      "Commands file: moz_15000_cmds.txt\n",
      "Job script: moz_15000_jobs.sh\n",
      "To submit the job, run: qsub moz_15000_jobs.sh\n",
      "Commands file: moz_20000_cmds.txt\n",
      "Job script: moz_20000_jobs.sh\n",
      "To submit the job, run: qsub moz_20000_jobs.sh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the command and job files\n",
    "calibrate.generate_command_and_job_files(\n",
    "    name,\n",
    "    population_bins,\n",
    "    access_rates,\n",
    "    betas,\n",
    "    reps,\n",
    "    cores=28,\n",
    "    nodes=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392c525b",
   "metadata": {},
   "source": [
    "At this point you should have all the data and configuration files you need in order to run the calibration. The next step is to commit the changes (added files) to the local branch and push to the remote branch. Log into the cluster and switch to the branch you just created:\n",
    "\n",
    "```bash\n",
    "git checkout <branch_name>\n",
    "git pull\n",
    "```\n",
    "Create the required output directories on the cluster: \n",
    "\n",
    "```bash\n",
    "mkdir -p output/<country>/calibration\n",
    "```\n",
    "\n",
    "Ensure that the jobs files (typically `.sh`) are only the specific jobs that you want to run. Delete any other job or `.sh` files. Keep in mind that any given user may only have at most 50 jobs queued on Owl's Nest at a time. Queue up the calibration jobs using the following command:\n",
    "\n",
    "```bash\n",
    "for i in $(ls *.sh); do\n",
    "    echo \"Submitting job $i\"\n",
    "    qsub $i\n",
    "done\n",
    "```\n",
    "\n",
    "This will submit all the jobs in the current directory. Give it a few minutes to queue up and then log into your cluster account and check the status of the jobs. At steady state, each job should be running approximately 28 processess simultaneously. The lower population size pixels do not take very long to run when spread out over several nodes (4-8, approximately 15-60 minutes). The larger population sizes (10k-20k) take much longer (10-12 hours)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cfc524",
   "metadata": {},
   "source": [
    "### Process simulated data\n",
    "\n",
    "Once the jobs are completed, you will need to download the output files from the cluster. This can be done using `scp` or `rsync`. For example, to download the output files from the cluster to your local machine, you can use the following command:\n",
    "\n",
    "```bash\n",
    "scp -r <username>@<cluster_address>:Temple-Malaria-Simulation-Analysis/output <local_path>\n",
    "```\n",
    "This will download the output files to the specified local path. I recommend simply copying them to your desktop. Once you have downloaded the output files, copy them to this repo (or download directly) and place them in the `output/<country>/calibration` directory. This will allow you to run the analysis on the simulated data locally.\n",
    "\n",
    "Assuming that all or most of the calibration simulation executed successfull, delete all job and commands files.\n",
    "\n",
    "---\n",
    "\n",
    "## Check for missing data\n",
    "\n",
    "Prior to running the full analysis, check for any missing data files. Sometimes Owl's Nest get hung, the database file didn't write correctly or some other bug happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7753b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base_file_path = os.path.join(\"output\", name, \"calibration\")\n",
    "summary = pd.DataFrame(columns=[\"population\", \"access_rate\", \"beta\", \"iteration\", \"pfprunder5\", \"pfpr2to10\", \"pfprall\"])\n",
    "\n",
    "comparison = date(calibration_year, 1, 1)\n",
    "year_start = comparison.strftime(\"%Y-%m-%d\")\n",
    "year_end = (comparison + pd.DateOffset(years=1)).strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f428e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrate.process_missing_jobs(\n",
    "    name,\n",
    "    population_bins,\n",
    "    access_rates,\n",
    "    betas,\n",
    "    os.path.join(\"output\", name, \"calibration\"),\n",
    "    reps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e642286f",
   "metadata": {},
   "source": [
    "If there are any missing data files run them on the cluster. This can be done by running the following command:\n",
    "\n",
    "```bash\n",
    "qsub missing_calibration_runs_<pop>_<access>.sh\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8ab387",
   "metadata": {},
   "source": [
    "### Summarize data\n",
    "\n",
    "Once all the appropriate calibration data has been collected we need to summarize across all the individual data files. This will write the summarized results to `calibration_summary.csv` file in the `output/<country>/calibration` directory. The summary file will contain the following columns:\n",
    "- `beta`: the beta value used in the simulation\n",
    "- `population`: the population size of the pixel\n",
    "- `access_rate`: the treatment access rate of the pixel\n",
    "- `pfpr2_10`: the pfpr2-10 value of the pixel\n",
    "- `pfprunder5`: the mean pfpr under 5 value of the pixel\n",
    "- `pfprall`: the mean pfpr value of the pixel\n",
    "- `iteration` : the iteration number of the simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81af3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrate.summarize_calibration_results(\n",
    "    name,\n",
    "    population_bins,\n",
    "    access_rates,\n",
    "    betas,\n",
    "    calibration_year,\n",
    "    os.path.join(\"output\", name, \"calibration\"),\n",
    "    reps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09abc4e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Curve fitting\n",
    "\n",
    "Now that we have summarized data that connects population size, treatment access, transmission rate, and prevelence, we can fit the data to a linear and log-sigmoid curve. Generally, the log-sigmoid appears to model the relationship better, but both methods are here for reference. This will allow us to generate a beta map for the experimental simulation. The beta map is generated by taking the beta value that corresponds to the population size and treatment access rate of each pixel. This is done using the `generate_beta_map` function, which takes the summarized data and generates a beta map for the experimental simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b79cb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from masim_analysis import calibrate\n",
    "\n",
    "base_file_path = os.path.join(\"output\", name, \"calibration\")\n",
    "means = pd.read_csv(f\"{base_file_path}/calibration_means.csv\")\n",
    "populations = means[\"population\"].unique()\n",
    "access_rates = means[\"access_rate\"].unique()\n",
    "betas = means[\"beta\"].unique()\n",
    "#group = means[(means[\"population\"] == 100) & (means[\"access_rate\"] == 0.65)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5886365",
   "metadata": {},
   "source": [
    "### Linear fit\n",
    "\n",
    "We're looking to fit the pfpr to beta relationship so that we can then use the real pfpr value from the raster data to determine the beta value. So, given a specific pixel's population, pfpr, and access rate (treatmentseeking?) calculate the beta value from this fitting method. We also don't have a decent way to serialize the linear models returned from sklearn, so at the moment this is just here for demonstration purposes.\n",
    "\n",
    "Start using linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de2753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine grid size\n",
    "num_rows = len(populations)\n",
    "num_cols = len(access_rates)\n",
    "\n",
    "# Create subplots grid\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(4 * num_cols, 4 * num_rows), sharex=True, sharey=True)\n",
    "\n",
    "# Ensure axes is always a 2D list for consistency\n",
    "if num_rows == 1:\n",
    "    axes = np.array([axes])  # Convert to 2D array\n",
    "if num_cols == 1:\n",
    "    axes = np.array([[ax] for ax in axes])  # Convert to 2D array\n",
    "\n",
    "# Perform regression for each (Population, TreatmentAccess) group\n",
    "for i, population in enumerate(populations):\n",
    "    for j, treatment_access in enumerate(access_rates):\n",
    "        ax = axes[i, j]  # Select subplot location\n",
    "\n",
    "        # Filter the data for the current Population and TreatmentAccess\n",
    "        group = means[(means[\"population\"] == population) & (means[\"access_rate\"] == treatment_access)]\n",
    "\n",
    "        if group.empty:\n",
    "            ax.set_visible(False)  # Hide empty plots\n",
    "            continue\n",
    "\n",
    "        group = group.dropna(axis=0)  # drop any row in a nan column\n",
    "\n",
    "        X = group[[\"beta\"]].values\n",
    "        y = group[\"pfpr2to10\"].values\n",
    "\n",
    "        # 1. Linear Regression\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # 2. Polynomial Regression (degree 3)\n",
    "        poly_model3 = make_pipeline(PolynomialFeatures(3), LinearRegression())\n",
    "        poly_model3.fit(X, y)\n",
    "\n",
    "        # 3. Polynomial Regression (degree 5)\n",
    "        poly_model5 = make_pipeline(PolynomialFeatures(5), LinearRegression())\n",
    "        poly_model5.fit(X, y)\n",
    "\n",
    "        # 4. Spline Regression\n",
    "        # spline_model = UnivariateSpline(group['beta'], group['pfpr2to10'], s=50)\n",
    "\n",
    "        # Plot regression\n",
    "        sns.scatterplot(x=group[\"beta\"], y=group[\"pfpr2to10\"], ax=ax, label=\"Data\", color=\"black\")\n",
    "        ax.plot(group[\"beta\"], model.predict(X), color=\"red\", linestyle=\"dashed\", label=\"Linear\")\n",
    "        ax.plot(group[\"beta\"], poly_model3.predict(X), color=\"blue\", linestyle=\"dashed\", label=\"Poly (3)\")\n",
    "        ax.plot(group[\"beta\"], poly_model5.predict(X), color=\"green\", linestyle=\"dashed\", label=\"Poly (5)\")\n",
    "        # ax.plot(group['Beta'], spline_model(X), color='purple', linestyle=\"dashed\", label=\"Spline\")\n",
    "\n",
    "        # Setting titles & labels\n",
    "        ax.set_title(f\"Population : {population}, Access : {treatment_access}\")\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(\"pfpr2to10\")\n",
    "        if i == num_rows - 1:\n",
    "            ax.set_xlabel(\"Beta\")\n",
    "        ax.legend(fontsize=7)\n",
    "\n",
    "# Adjust layout\n",
    "plt.suptitle(\"Curve Fitting by Population & Treatment Access\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3310a37",
   "metadata": {},
   "source": [
    "Now we'll do a logarithemic fit for the beta value and attempt to fit a sigmoid curve. Again this is the model that typically works best for the data. The fit returned is a list of the parameters for the sigmoid function which can be easily serialized and saved to a file. The sigmoid function is defined as:\n",
    "```python\n",
    "def sigmoid(x, a, b, c):\n",
    "    return a / (1 + np.exp(-b * (x - c)))\n",
    "```\n",
    "where `a`, `b`, and `c` are the parameters of the sigmoid function. The `x` value is the beta value. The `a` parameter is the maximum value of the sigmoid function, the `b` parameter is the steepness of the curve, and the `c` parameter is the x-value of the sigmoid's midpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bf5ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_map = {\n",
    "        access_rate: {population: None for population in populations} for access_rate in access_rates\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda2576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_map = calibrate.log_sigmoid_fit(\n",
    "    populations,\n",
    "    access_rates,\n",
    "    means,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd10f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = calibrate.plot_log_sigmoid_fit(models_map, populations[:5], access_rates[:5], means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc373e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"data\", name, \"calibration\", \"models_map.json\"), 'w') as f:\n",
    "    json.dump(models_map, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855507fa",
   "metadata": {},
   "source": [
    "Keep in mind that the built in JSON serialization and deserialization reads in all the values as strings. To get around this use `load_beta_model` from the `calibrate` module instead of the native `json.load()` function. This will convert the values back to numeric values and lists where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b090131",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_map = calibrate.load_beta_model(os.path.join(\"data\", name, \"calibration\", \"models_map.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f721fd",
   "metadata": {},
   "source": [
    "For an example of how to use the model mapping use the `get_beta` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36f80fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.masim_analysis import calibrate, utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e425e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts, _ =  utils.read_raster(\"data/moz/moz_districts.asc\")\n",
    "population, _ = utils.read_raster(\"data/moz/moz_population.asc\")\n",
    "prevalence, _ = utils.read_raster(\"data/moz/moz_pfpr210.asc\")\n",
    "treatment, _ =  utils.read_raster(\"data/moz/moz_treatmentseeking.asc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82293536",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_map = calibrate.create_beta_map(models_map, population, treatment, prevalence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45048afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37277b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(beta_map, cmap=\"hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e54ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5b9841",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_a = pd.read_csv(\"data/moz/incidence inharrime.csv\")\n",
    "seasons_b = pd.read_csv(\"data/moz/incidence cuamba.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bde6983",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf27cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = seasons[\"cases/thousand\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90ec83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = seasons[\"month_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914334cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(months, cases, 'k.')\n",
    "plt.plot(seasons_b['month_num'], seasons_b['cases/thousand'], 'r.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb9aab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
