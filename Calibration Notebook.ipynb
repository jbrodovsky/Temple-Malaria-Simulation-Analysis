{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac53736-3a00-443d-9120-a37ee038a592",
   "metadata": {},
   "source": [
    "# MaSim Country Calibration\n",
    "\n",
    "This note book is used for running the country calibration processes for eventual use in the experimental simulation process. This notebook and the accompanying toolkit was developed by James Brodovsky and Sarit Adhikari as part of the calibration efforts for Burkino Faso and Mozambique in 2025.\n",
    "\n",
    "## Installation\n",
    "\n",
    "The preferred use case is to install the toolbox package locally into a virtual environment. If you have cloned this repository, you can install the package using pip. \n",
    "\n",
    "```bash\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "If you are using `uv` (and you should!) install via\n",
    "\n",
    "```bash\n",
    "uv pip install -e .\n",
    "```\n",
    "\n",
    "This will install the package in editable mode, allowing you to make changes to the code and have them reflected in your local environment. If for some reason you are not seeing the changes reflected, try re-installing the package locally using the above commands again.\n",
    "\n",
    "Alternatively, and this should be done only if you are developing additional toolbox features, you can run this notebook from the root (top level) directory and direct import calls to the toolbox via \n",
    "\n",
    "```python \n",
    "from src.masim_analysis import *\n",
    "```\n",
    "where `*` is the specific module you are interested in.\n",
    "\n",
    "## Package and repo structure\n",
    "\n",
    "Please make note of the following directory structures: `conf` and `data`. These are the primary two directories for experimental country data and configuration files. The `conf` directory contains the configuration (.yml) files for the simulation, while the `data` directory contains the data files used in the simulation (typically raster files).\n",
    "\n",
    "Each of these folders is organized by country. For example, if you are working with Mozambique, the directory structure would look like this:\n",
    "\n",
    "```\n",
    "data/\n",
    "    moz/\n",
    "        calibration/\n",
    "        ...\n",
    "conf/\n",
    "    moz/\n",
    "        calibration/\n",
    "        ...\n",
    "```\n",
    "Additionally the templates folder contains the template files for the configuration files. These are used to generate the .yml files for the simulation.\n",
    "\n",
    "## How to use this notebook\n",
    "\n",
    "This notebook should be thought of as a structured interactive prompt. The notes sections guide you through the process of calibrating and validating a country. The code sections are generally organized into sections that can be run independently. The notebook breaks up individual workflows by using markdown headings and note blocks. Due to the some what long-running nature of the tasks that calibration and validation involve, you'll sometimes have to wait and shutdown the kernel the notebook is running on and pick up where you left off another time.\n",
    "\n",
    "To that end, this notebook is designed to make things organized but also segmented. Keep calibration constants (name, population, etc.) in a block below this and make sure to run that block every time you start the notebook. Module and library imports should be handled in the workflow segment you are currently working on. It is recommend (for speed of execution) to separate out imports from code execution to save time on re-importing.\n",
    "\n",
    "Workflows are generally separated by a horizontal rule:\n",
    "\n",
    "---\n",
    "\n",
    "## Calibration efforts\n",
    "\n",
    "The primary calibration point is to relate the beta parameter (rate of infection/biting) with the population size of a given map pixel given that pixel's treatment access rate. This involves a few steps. As a preliminary step, obtain the relevant raster files that contain population data, district mapping values, treatment, and prevalence (pfpr2-10, or a similar name) and place it under `data/<country>`. fictitious calibration data will be stored under `data/<country>/calibration`. Create a new branch in the git repository for the calibration process. This is important to keep track of changes and to avoid conflicts with the main branch. The main branch should be reserved for strategy and treatment analysis. The workflow there should be to branch off from the main branch, implement the strategies and treatments, and then merge back in any useful changes. Strategy and treament analysis shouldn't be a main contribution to the main branch. The calibration branch name should be descriptive and include the country name, e.g., `calibration-moz`.\n",
    "\n",
    "Calibration then occurs in two phases and should be done on a separate git branch. The first phase is generating the simulated data for beta calibration. This creates the fictitious configuration and data files. This concludes with several command and job files to be run on a cluster. At the moment this is configured to work on Temple University's OwlsNest cluster, but the resulting `*_cmds.txt` files simply contain a list of shell commands that execute the simulation and should be generalizable to whatever parallel computing cluster system you are using.\n",
    "\n",
    "The second phase is started when the batch processing is completed and downloaded locally to the `output/<country>/calibration` directory. These files are then summarized and the prevalence and beta values are fit using a log-sigmoid curve fit when broken down by pixel population and treatment access rate. These fits are then used to generate the beta map for eventual use in the experimental simulation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d72e28",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "These are the country specific parameters you are working within. Please run the below code block every time you start this notebook.\n",
    "\n",
    "- `name`: The country code name. Usually two or three letters. Examples: `moz` for Mozambique, `bf` for Burkino Faso, `rwa` for Rwanda, `tz` for Tanzania, etc.\n",
    "- `birth_rate`: The birth rate of the country. This is used to calculate the population growth rate. This is usually a constant value for the country. Data is typically given in births per 1000 people. Normalize that data to a decimal value.\n",
    "- `target_population`: The population of the country in the calibration target year.\n",
    "- `initial_age_structure`: The initial age bins of the country. \n",
    "- `age_distribution`: The age distribution of the country as percentages corresponding to the age bins.\n",
    "- `death_rate`: The death rate of the country corresponding to the age bins.\n",
    "- `access_rates`: the treatment access rates for the country. This is determined by the unique values in the raster file typically called `<name>_treatementseeking.asc`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfb1055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from masim_analysis import utils, configure\n",
    "import os\n",
    "from datetime import date\n",
    "from ruamel.yaml import YAML\n",
    "\n",
    "yaml = YAML()\n",
    "\n",
    "# --- Input parameters ---\n",
    "# These are theconstants for each calibration permutation that we know from population data\n",
    "name = \"moz\"\n",
    "\n",
    "# Population structure\n",
    "initial_age_structure = [1, 5, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 69, 100] # Different from the MaSim age_structure\n",
    "age_distribution = [0.037, 0.132, 0.161, 0.142, 0.090, 0.086, 0.070, 0.052, 0.044, 0.044, 0.031, 0.041, 0.024, 0.017, 0.013, 0.017]\n",
    "\n",
    "# Growth rate and population parameters\n",
    "birth_rate = 55.5 / 1000 \n",
    "target_population = 33_635_160  # Total population for the current year (2023)\n",
    "initial_population, _ = utils.read_raster(os.path.join(\"data\", name, f\"{name}_initialpopulation.asc\"))\n",
    "initial_population = np.nansum(initial_population)  # Initial population for the target year\n",
    "target_growth_rate = (target_population - initial_population) / initial_population\n",
    "#death_rate = [0.049744, 0.064331, 0.064331, 0.064331, 0.064331, 0.00359, 0.00361, 0.00365, 0.00379, 0.00379, 0.133, 0.133, 0.0174, 0.0174, 0.0174] # original\n",
    "death_rate = [0.048140, 0.041605, 0.052070, 0.048057, 0.048057, 0.00497, 0.00497, 0.00497, 0.00497, 0.003540, 0.00354, 0.00758, 0.01113, 0.01113, 0.01113]\n",
    "\n",
    "# Get the unique treatment access rates\n",
    "treatment, _ =  utils.read_raster(os.path.join(\"data\", name, f\"{name}_treatmentseeking.asc\"))\n",
    "access_rates = np.unique(treatment)\n",
    "access_rates = access_rates[~np.isnan(access_rates)]\n",
    "access_rates = np.sort(access_rates)\n",
    "access_rates = access_rates.tolist()\n",
    "\n",
    "# --- Tuneable parameters ---\n",
    "# These are the parameters that we will be tuning in the calibration process to develop a model that\n",
    "# relates the prevelence data to population, access rates, and betas.\n",
    "betas = [0.001, 0.005, 0.01, 0.0125, 0.015, 0.02, 0.03, 0.04, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.8, 1]\n",
    "population_bins = [10, 20, 30, 40, 50, 75, 100, 250, 500, 1000, 2000, 5000, 10000, 15000, 20000]\n",
    "\n",
    "# --- Calibration parameters ---\n",
    "# These are the target year for the data available as well as the number \n",
    "# of repetitions for each calibration combination. These variables \n",
    "# are constant for each calibration permutation\n",
    "calibration_year = 2022 \n",
    "reps = 20\n",
    "\n",
    "# --- Derived dates ---\n",
    "starting_date=date(calibration_year-11, 1, 1).strftime(\"%Y/%m/%d\")\n",
    "ending_date=date(calibration_year+1, 12, 31).strftime(\"%Y/%m/%d\")\n",
    "start_of_comparison_period=date(calibration_year, 1, 1).strftime(\"%Y/%m/%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb02686",
   "metadata": {},
   "source": [
    "### Raster data exploration\n",
    "\n",
    "The first step is to read in the relevant rasters to get a sense of some of the big picture data. The rasters are stored in the `data/<country>` directory. This is then used to get some basic statistics to run calibration. Of primary concern is getting the value of the initial population, calibration year population, `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47944fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts, _ =  utils.read_raster(os.path.join(\"data\", name, f\"{name}_districts.asc\"))\n",
    "population, _ = utils.read_raster(os.path.join(\"data\", name, f\"{name}_initialpopulation.asc\"))\n",
    "prevalence, _ = utils.read_raster(os.path.join(\"data\", name, f\"{name}_pfpr210.asc\"))\n",
    "district_names = pd.read_csv(os.path.join(\"data\", name, f\"{name}_mapping.csv\"), index_col=\"ID\")\n",
    "names = district_names.to_dict()[\"DISTRICT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_fig = utils.plot_districts(districts, names, \"Mozambique\", fig_size=(12, 6), loc=\"lower right\", bbox_to_anchor=(1.55, 0))\n",
    "pop_fig = utils.plot_population(population, \"Mozambique\", fig_size=(12, 6))\n",
    "pfpr_plot = utils.plot_prevalence(prevalence, \"Mozambique\", fig_size=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778892ee",
   "metadata": {},
   "source": [
    "### Drug distribution rates\n",
    "\n",
    "Each country has a different drug distribution rate. This will factor into the calibration file. For instances, Mozambique's distribution is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bcc7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_distribution = pd.read_csv(os.path.join(\"data\", name, f\"{name}_drugdistribution.csv\"), index_col=0, na_values=-99)\n",
    "drug_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809cbe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_distribution.drop(2003, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f15d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_distribution.drop(columns=['AL', 'aspirin'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2d4741",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72a5c4c",
   "metadata": {},
   "source": [
    "As a santiy check, the sum of the distribution rates should equal 1.0 for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba9d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_distribution[\"sum\"] = drug_distribution.sum(axis=1)\n",
    "drug_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d004d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = drug_distribution[\"sum\"]\n",
    "drug_distribution.drop(columns=[\"sum\"], inplace=True)\n",
    "drug_distribution = drug_distribution.div(totals, axis=0)\n",
    "drug_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036a87c5",
   "metadata": {},
   "source": [
    "We then relate this to the `DRUG_DB` and `THERAPY_DB` dictionary in `masim_analysis.configure`. Note that the drug distribution may be either a single drug or a combination of drugs. For instance, `al` is a combination of artemether and lumefantrine. This would correspond to a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68b4dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from masim_analysis.configure import DRUG_DB, THERAPY_DB\n",
    "print(\"Drugs:\")\n",
    "for idx in DRUG_DB.keys():\n",
    "    print(f\"Drug {idx}: {DRUG_DB[idx]['name']}\")\n",
    "print(\"Therapies:\")\n",
    "for idx in THERAPY_DB.keys():\n",
    "    drug_ids = THERAPY_DB[idx]['drug_id']\n",
    "    print(f\"Therapy {idx}: {[DRUG_DB[drug_id]['name'] for drug_id in drug_ids]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be18a449",
   "metadata": {},
   "source": [
    "From this information we must manually construct the baseline strategy. Similar to the drug and therapy \"databases\" we store the strategies in a dictionary with integer keys. Each key corresponds to a strategy which itself is a dictionary with the following keys:\n",
    "- `name`: The name of the strategy. This is used to identify the strategy in the simulation.\n",
    "- `type`: The type of strategy. Leave this as \"MFT\" until further notice.\n",
    "- `therapy_ids`: The therapy ids used in the strategy. This is a list of integers corresponding to the therapy ids in the `THERAPY_DB` dictionary.\n",
    "- `distribution`: The distribution of the strategy. This is a list of floats corresponding to the distribution of the strategy that we derived from the drug distribution rates from DHS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b487a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: maciek thinks that ASAQ should really be AL; going to queue up this for a temporary calibration (i.e. therapy 1 instead of 0)\n",
    "os.makedirs(os.path.join(\"conf\", name, \"test\"), exist_ok=True)\n",
    "\n",
    "strategy_db = {\n",
    "    0: {\n",
    "        \"name\": \"2011\",\n",
    "        \"type\": \"MFT\",\n",
    "        \"therapy_ids\":  [7, 8, 6, 4, 5, 1],\n",
    "        \"distribution\": [0.083661, 0.234252, 0.027559, 0.019685, 0, 0.589567]\n",
    "    },\n",
    "    1: {\n",
    "        \"name\": \"2015\",\n",
    "        \"type\": \"MFT\",\n",
    "        \"therapy_ids\":  [7, 8, 6, 4, 5, 1],\n",
    "        \"distribution\": [0.003914, 0.07045, 0.002935, 0.001957, 0, 0.014677, 0.906067]\n",
    "    },\n",
    "    2: {\n",
    "        \"name\": \"2018\",\n",
    "        \"type\": \"MFT\",\n",
    "        \"therapy_ids\":  [7, 8, 6, 4, 5, 1],\n",
    "        \"distribution\": [0.3005970, 0.006965, 0.002985, 0.00199, 0.004975, 0.00199, 0.975124]\n",
    "    },\n",
    "    3: {\n",
    "        \"name\": \"2022\",\n",
    "        \"type\": \"MFT\",\n",
    "        \"therapy_ids\":  [7, 8, 6, 4, 5, 1],\n",
    "        \"distribution\": [0.025819, 0.028798, 0, 0.027805, 0.059583, 0.013903, 0.844091]\n",
    "    },\n",
    "    4: {\n",
    "        \"name\": \"2023\",\n",
    "        \"type\": \"MFT\",\n",
    "        \"therapy_ids\":  [7, 8, 6, 4, 5, 1],\n",
    "        \"distribution\": [0.025819, 0.028798, 0, 0.027805, 0.059583, 0.013903, 0.844091]\n",
    "    },\n",
    "}\n",
    "yaml.dump(strategy_db, open(os.path.join(\"conf\", name, \"test\", f\"strategy_db.yaml\"), \"w\"))\n",
    "\n",
    "events = [\n",
    "    {\"name\": \"2011_strategy\", \n",
    "     \"info\": [{\"day\": starting_date, \"strategy_id\": 0}]   \n",
    "    },\n",
    "    {\"name\": \"2015_strategy\", \n",
    "     \"info\": [{\"day\": date(2015, 1, 1).strftime(\"%Y/%m/%d\"), \"strategy_id\": 1}]   \n",
    "    },\n",
    "    {\"name\": \"2018_strategy\", \n",
    "     \"info\": [{\"day\": date(2018, 1, 1).strftime(\"%Y/%m/%d\"), \"strategy_id\": 2}]   \n",
    "    },\n",
    "    {\"name\": \"2022_strategy\", \n",
    "     \"info\": [{\"day\": date(2022, 1, 1).strftime(\"%Y/%m/%d\"), \"strategy_id\": 3}]   \n",
    "    },\n",
    "    {\"name\": \"2023_strategy\", \n",
    "     \"info\": [{\"day\": date(2023, 1, 1).strftime(\"%Y/%m/%d\"), \"strategy_id\": 4}]   \n",
    "    }\n",
    "]\n",
    "yaml.dump(events, open(os.path.join(\"conf\", name, \"test\", f\"events.yaml\"), \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f6e22",
   "metadata": {},
   "source": [
    "## Birth Rate Check\n",
    "\n",
    "The important calibration point is the final year population. We back out the population data from this using a birth rate. To check that the initialization raster is correct, we need to run an initial simulation run to verify this growth rate considering malria deaths as well. This is really just a sanity check to make sure the input data is correct.\n",
    "\n",
    "Run an initial check to make sure the growth rate is working correctly. Given the input file for whatever year, run a single simulation and check that the population for the target year is correct. Make sure to scale the ending population by the simulation scale factor as well. Ball park the initial beta value around 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a335cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from masim_analysis import analysis, calibrate\n",
    "\n",
    "# load the previously saved strategy and event files\n",
    "strategy_db = yaml.load(open(os.path.join(\"conf\", name, \"test\", f\"strategy_db.yaml\"), \"r\"))\n",
    "events = yaml.load(open(os.path.join(\"conf\", name, \"test\", f\"events.yaml\"), \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a463f50c",
   "metadata": {},
   "source": [
    "Next we'll create a simple simulation run to verify the population growth rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d2e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = 100_000\n",
    "params = configure.configure(\n",
    "    country_code=name,\n",
    "    birth_rate=birth_rate,\n",
    "    initial_age_structure=initial_age_structure,\n",
    "    age_distribution=age_distribution,\n",
    "    death_rates=death_rate,\n",
    "    starting_date=date(calibration_year - 11, 1, 1),\n",
    "    start_of_comparison_period=date(calibration_year, 1, 1),\n",
    "    ending_date = date(calibration_year + 1, 1, 1),\n",
    "    strategy_db=strategy_db,\n",
    "    calibration_str=f\"growth_validation_{pop}\",\n",
    "    beta_override=0.00,\n",
    "    population_override=pop,\n",
    "    calibration=True,\n",
    ")\n",
    "params['events'].extend(events)\n",
    "params['artificial_rescaling_of_population_size'] = 1.0\n",
    "\n",
    "yaml.dump(params, open(os.path.join(\"conf\", name, \"test\", f\"{name}_growth_validation_{pop}.yaml\"), \"w\"))\n",
    "\n",
    "calibrate.write_pixel_data_files(params[\"raster_db\"], pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb6f91",
   "metadata": {},
   "source": [
    "Now we'll run the simulation to verify the population growth rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ed93d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(\"conf\", name, \"test\", f\"{name}_growth_validation_{pop}.yaml\")\n",
    "os.makedirs(os.path.join(\"output\", name, \"test\"), exist_ok=True)\n",
    "output_file = os.path.join(\"output\", name, \"test\", f\"{name}_growth_validation_{pop}\")\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\n",
    "    f\"./bin/MaSim -i ./{filename} -o ./{output_file} -r SQLiteDistrictReporter\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd61af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4490f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = analysis.get_table(f\"{output_file}monthly_data_0.db\", \"monthlysitedata\")\n",
    "starting_pop = data[data[\"monthlydataid\"] == 1][\"population\"].sum()\n",
    "last_month = data[\"monthlydataid\"].unique()[-1] \n",
    "population_by_month = [data[data[\"monthlydataid\"] == month][\"population\"].sum() for month in data[\"monthlydataid\"].unique()]\n",
    "population_by_month = np.array(population_by_month)\n",
    "ending_population = population_by_month[-1]\n",
    "growth_rate = (ending_population - starting_pop) / starting_pop\n",
    "projected_population = initial_population * (1 + growth_rate)\n",
    "print(f\"Starting population: {starting_pop}\")\n",
    "print(f\"Ending population: {ending_population}\")\n",
    "print(f\"Growth rate: {growth_rate}\")\n",
    "print(f\"Projected 2023 population: {projected_population}\")\n",
    "print(f\"Percent error: {100 * (projected_population - target_population) / target_population:.4f}%\")\n",
    "\n",
    "# plots\n",
    "population_scalar = population_by_month / starting_pop\n",
    "plt.plot(data[\"monthlydataid\"].unique(), (initial_population * population_scalar) / 1_000_000, linestyle='-', color='b')\n",
    "plt.axhline(y=target_population / 1_000_000, color='r', linestyle='--', label='Target Population (32.64 million)')\n",
    "plt.axvline(x=last_month, color='g', linestyle='--', label='Calibration year (2023)')\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Population (millions)\")\n",
    "plt.title(\"Population by Month\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b5d675",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Seasonality Calibration\n",
    "\n",
    "Seasonality is a something of a manual process to fit precisely. The goal is to fit a curve that gives a scalar parameter that modifies the incidence rate for each day of the year. This is written to a `.csv` file historically called \"adjustment\" or \"rainfall\" and is a simple row-wise list of scalar values that effect the overall incidence or biting rate. This notebook choose to save this output as `seasonality.csv` in the `data/<country>/calibration` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfffd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "cuamba = pd.read_csv(os.path.join(\"data\", name, \"calibration\", \"incidence cuamba.csv\"))\n",
    "inharrime = pd.read_csv(os.path.join(\"data\", name, \"calibration\", \"incidence inharrime.csv\"))\n",
    "c = cuamba[['month_num', 'cases/thousand']].copy()\n",
    "i = inharrime[['month_num', 'cases/thousand']].copy()\n",
    "s = pd.concat([c, i], axis=0)\n",
    "s = s.dropna()\n",
    "\n",
    "x = s['month_num'].to_numpy()\n",
    "y = s['cases/thousand'].to_numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,2))\n",
    "ax.plot(x, y, '.b', label='Data')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Cases/thousand')\n",
    "ax.set_title('Seasonal signal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9cc04b",
   "metadata": {},
   "source": [
    "As we know from previous work, the seasonality of malaria (and most diseases) is roughly sinusoidal. The goal is to fit a sinusoidal curve to the data. With that as well the working assumption of the Boni Lab is to use the positive half of the sine curve. There is not a strong stance on this as seasonal incidence modeling is not a major research concern of the lab. Simply put, taking the positive half of the sine curve is what previous publications from the lab has done and to avoid unneccesary problems in the peer review process, we will continue to do this. This notebook will walk through the process for the full sine curve for completeness' sake.\n",
    "\n",
    "The fitting is done using the `scipy.optimize.curve_fit` function. The function takes in a model function and the data to fit. The model function is a sinusoidal function with a phase shift, period, amplitude, and offset.\n",
    "\n",
    "$$\n",
    "s(x) = A \\sin\\left(\\frac{2 \\pi x}{P} + \\phi\\right) + B\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74f4f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonality(x, amplitude, period, phase, offset):\n",
    "    \"\"\"\n",
    "    Generate a seasonal signal according to a sinusoidal model.\n",
    "    \"\"\"\n",
    "    return amplitude * np.sin((2 * np.pi / period) * (x - phase)) + offset\n",
    "\n",
    "def seasonality_positive_sine(x, amplitude, period, phase, offset):\n",
    "    \"\"\"\n",
    "    Generate a seasonal signal according to a sinusoidal model.\n",
    "    \"\"\"\n",
    "    s = seasonality(x, amplitude, period, phase, offset)\n",
    "    s[s <= offset] = offset\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b690dc97",
   "metadata": {},
   "source": [
    "We are looking to fit a scalar multiplier of the base incidence data not a curve over the specific incidence. We can do this by normalizing the incidence data about the median, mean, or mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acecdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode as spmode\n",
    "\n",
    "median = np.median(y)\n",
    "mean = np.mean(y)\n",
    "mode = spmode(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3609386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,2))\n",
    "ax.plot(x, y, '.b', label='Data')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Cases/thousand')\n",
    "ax.set_title('Seasonal signal')\n",
    "ax.axhline(median, color='r', linestyle='--', label='Median')\n",
    "ax.axhline(mean, color='g', linestyle='--', label='Mean')\n",
    "ax.axhline(mode.mode, color='y', linestyle='--', label='Mode')\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.12, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ef990",
   "metadata": {},
   "source": [
    "Normailze the data with respect to the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113224f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_norm = y / mean\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,2))\n",
    "ax.plot(x, y_norm, '.b', label='Normalized Data')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Cases / thousand')\n",
    "ax.set_title('Seasonal signal')\n",
    "ax.axhline(median / mean, color='r', linestyle='--', label='Normalized Median')\n",
    "ax.axhline(mean / mean, color='g', linestyle='--', label='Normalized Mean')\n",
    "ax.axhline(mode.mode / mean, color='y', linestyle='--', label='Normalized Mode')\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.22, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fa5d3e",
   "metadata": {},
   "source": [
    "Run the curve fit function and analyze with resepct to both the full sinusoid and the positive half of the sine curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e797f054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "fit = curve_fit(seasonality, x, y_norm, p0=[1, 365, 0, 1], maxfev=10000)\n",
    "\n",
    "coefs = fit[0]\n",
    "\n",
    "t = np.linspace(1, 12, 1000)\n",
    "\n",
    "full_sine = seasonality(x, *coefs)\n",
    "r2 = 1 - (np.sum((y_norm - full_sine)**2) / np.sum((y_norm - np.mean(y_norm))**2))\n",
    "std_dev = np.std(y_norm - full_sine)\n",
    "\n",
    "positive_sine = seasonality_positive_sine(x, *coefs)\n",
    "r2_positive = 1 - (np.sum((y_norm - positive_sine)**2) / np.sum((y_norm - np.mean(y_norm))**2))\n",
    "std_dev_positive = np.std(y_norm - positive_sine)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 2))\n",
    "ax.plot(x, y_norm, '.b', label='Data (scaled)')\n",
    "ax.plot(t, seasonality(t, *coefs), 'r-', label=f'Fitted curve\\n$r^2$ = {r2:0.2f}\\n$\\sigma$: {std_dev:0.2f}')\n",
    "ax.plot(t, seasonality_positive_sine(t, *coefs), 'g--', label=f'Fitted curve (positive sine)\\n$r^2$ = {r2_positive:0.2f}\\n$\\sigma$: {std_dev_positive:0.2f}')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Incidence Scalar')\n",
    "ax.legend(loc=\"upper right\", bbox_to_anchor=(1.28, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd788099",
   "metadata": {},
   "source": [
    "The fit is not great, but this is a relatively minor aspect of the simulation. The point is to introduce some degree of seasonal variation that is reasonable. Now let's transform the fit back to the incidence rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f871e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,3))\n",
    "ax.plot(x, y, '.b', label='Data (scaled)')\n",
    "ax.plot(t, seasonality_positive_sine(t, *coefs) * mean, 'g--', label='Fitted curve; $r^2$ = %.2f' % r2)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e4f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the fit to a file for 365 days\n",
    "fit_x = np.arange(1, 366)\n",
    "fit_y = seasonality_positive_sine(fit_x, *fit[0])\n",
    "data = pd.DataFrame({\"day\": fit_x, \"cases/thousand\": fit_y})\n",
    "data.index = data[\"day\"]\n",
    "data = data.drop(columns=[\"day\"])\n",
    "data.to_csv(os.path.join(\"data\", name, f\"{name}_seasonality.csv\"), index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ca7ce",
   "metadata": {},
   "source": [
    "## Run calibration data generation\n",
    "\n",
    "The unknown that we are trying to solve for is the beta value(s). We have _real_ pixel-wise _prevalence_ (pfpr2-10) data that arrises from a given beta. The goal is to generate data that matches closely the real prevalence data by varying the beta value, population size, and access rate for a simulated single pixel. We will first generate the configuration files for the calibration runs here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893310f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country calibration script\n",
    "import os\n",
    "from datetime import date\n",
    "from ruamel.yaml import YAML\n",
    "from masim_analysis import calibrate\n",
    "\n",
    "yaml = YAML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449c63df",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrate.generate_configuration_files(\n",
    "    name,\n",
    "    calibration_year,\n",
    "    population_bins,\n",
    "    access_rates,\n",
    "    betas,\n",
    "    birth_rate,\n",
    "    death_rate,\n",
    "    initial_age_structure,\n",
    "    age_distribution,\n",
    "    strategy_db=strategy_db,\n",
    "    events=events,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd31fb6d",
   "metadata": {},
   "source": [
    "Create the command and job files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrate.generate_command_and_job_files(\n",
    "    name,\n",
    "    population_bins,\n",
    "    access_rates,\n",
    "    betas,\n",
    "    reps,\n",
    "    cores=28,\n",
    "    nodes=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392c525b",
   "metadata": {},
   "source": [
    "At this point you should have all the data and configuration files you need in order to run the calibration. The next step is to commit the changes (added files) to the local branch and push to the remote branch. Log into the cluster and switch to the branch you just created:\n",
    "\n",
    "```bash\n",
    "git checkout <branch_name>\n",
    "git pull\n",
    "```\n",
    "Create the required output directories on the cluster: \n",
    "\n",
    "```bash\n",
    "mkdir -p output/<country>/calibration\n",
    "```\n",
    "\n",
    "Ensure that the jobs files (typically `.sh`) are only the specific jobs that you want to run. Delete any other job or `.sh` files. Keep in mind that any given user may only have at most 50 jobs queued on Owl's Nest at a time. Queue up the calibration jobs using the following command:\n",
    "\n",
    "```bash\n",
    "for i in $(ls *.sh); do\n",
    "    echo \"Submitting job $i\"\n",
    "    qsub $i\n",
    "done\n",
    "```\n",
    "\n",
    "This will submit all the jobs in the current directory. Give it a few minutes to queue up and then log into your cluster account and check the status of the jobs. At steady state, each job should be running approximately 28 processess simultaneously. The lower population size pixels do not take very long to run when spread out over several nodes (4-8, approximately 15-60 minutes). The larger population sizes (10k-20k) take much longer (10-12 hours).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cfc524",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Check for missing data\n",
    "\n",
    "Once the jobs are completed, you will need to download the output files from the cluster. This can be done using `scp` or `rsync`. For example, to download the output files from Owl's Nest to your local machine, you can use the following command:\n",
    "\n",
    "```bash\n",
    "scp -r <username>@<cluster_address>:Temple-Malaria-Simulation-Analysis/output <local_path>\n",
    "```\n",
    "This will download the output files to the specified local path. I recommend simply copying them to your desktop. Once you have downloaded the output files, copy them to this repo (or download directly) and place them in the `output/<country>/calibration` directory. This will allow you to run the analysis on the simulated data locally.\n",
    "\n",
    "Assuming that all or most of the calibration simulation executed successfull, delete all job and commands files in the local repository. Do not delete the files on the cluster! This will cause a git tracking issue as the repository branches will be out of sync.\n",
    "\n",
    "Prior to running the full calibration analysis, check for any missing data files. Sometimes Owl's Nest gets hung, the database file didn't write correctly, the job times out, or some other bug happened. First, a sanity check. We should have the following number of output calibration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec4eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "needed_files = len(population_bins) * len(access_rates) * len(betas) * reps\n",
    "output_dir = os.path.join(\"output\", name, \"calibration\")\n",
    "output_files = os.listdir(output_dir)\n",
    "completed_files = len(output_files)\n",
    "\n",
    "print(f\"Total calibration files: {needed_files}\")\n",
    "print(f\"Completed calibration files: {completed_files}\")\n",
    "print(f\"Missing calibration files: {needed_files - completed_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68726cfa",
   "metadata": {},
   "source": [
    "If there are missing files use the below block to check for the missing output and create the appropriate job and commands files. You should also double check that each permutation is completed using `process_missing_jobs` prior to moving onto the calibration curve fitting. If there are no missing files proceed to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7753b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from masim_analysis import calibrate\n",
    "\n",
    "base_file_path = os.path.join(\"output\", name, \"calibration\")\n",
    "summary = pd.DataFrame(columns=[\"population\", \"access_rate\", \"beta\", \"iteration\", \"pfprunder5\", \"pfpr2to10\", \"pfprall\"])\n",
    "\n",
    "comparison = date(calibration_year, 1, 1)\n",
    "year_start = comparison.strftime(\"%Y-%m-%d\")\n",
    "year_end = (comparison + pd.DateOffset(years=1)).strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f428e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrate.process_missing_jobs(\n",
    "    name,\n",
    "    population_bins,\n",
    "    access_rates,\n",
    "    betas,\n",
    "    os.path.join(\"output\"),\n",
    "    reps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e642286f",
   "metadata": {},
   "source": [
    "If there are any missing data files run them on the cluster. This can be done by running the following command:\n",
    "\n",
    "```bash\n",
    "qsub missing_calibration_runs_<pop>_job.sh\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8ab387",
   "metadata": {},
   "source": [
    "### Summarize data\n",
    "\n",
    "Once all the appropriate calibration data has been collected we need to summarize across all the individual data files. This will write the summarized results to `calibration_summary.csv` file in the `output/<country>/calibration` directory. The summary file will contain the following columns:\n",
    "- `beta`: the beta value used in the simulation\n",
    "- `population`: the population size of the pixel\n",
    "- `access_rate`: the treatment access rate of the pixel\n",
    "- `pfpr2_10`: the pfpr2-10 value of the pixel\n",
    "- `pfprunder5`: the mean pfpr under 5 value of the pixel\n",
    "- `pfprall`: the mean pfpr value of the pixel\n",
    "- `iteration` : the iteration number of the simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81af3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = calibrate.summarize_calibration_results(\n",
    "    name,\n",
    "    population_bins,\n",
    "    access_rates,\n",
    "    betas,\n",
    "    calibration_year,\n",
    "    os.path.join(\"output\"),\n",
    "    reps,\n",
    ")\n",
    "summary.to_csv(f\"{base_file_path}/calibration_means.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09abc4e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Curve fitting\n",
    "\n",
    "Now that we have summarized data that connects population size, treatment access, transmission rate, and prevelence, we can fit the data to a linear and log-sigmoid curve. Generally, the log-sigmoid appears to model the relationship better, but both methods are here for reference. This will allow us to generate a beta map for the experimental simulation. The beta map is generated by taking the beta value that corresponds to the population size and treatment access rate of each pixel. This is done using the `generate_beta_map` function, which takes the summarized data and generates a beta map for the experimental simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b79cb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from masim_analysis import calibrate\n",
    "\n",
    "base_file_path = os.path.join(\"output\", name, \"calibration\")\n",
    "means = pd.read_csv(f\"{base_file_path}/calibration_means.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5886365",
   "metadata": {},
   "source": [
    "### Linear fit\n",
    "\n",
    "We're looking to fit the pfpr to beta relationship so that we can then use the real pfpr value from the raster data to determine the beta value. So, given a specific pixel's population, pfpr, and access rate (treatmentseeking?) calculate the beta value from this fitting method. We also don't have a decent way to serialize the linear models returned from sklearn, so at the moment this is just here for demonstration purposes.\n",
    "\n",
    "Start using linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de2753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine grid size\n",
    "num_rows = len(population_bins)\n",
    "num_cols = len(access_rates)\n",
    "\n",
    "# Create subplots grid\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(4 * num_cols, 4 * num_rows), sharex=True, sharey=True)\n",
    "\n",
    "# Ensure axes is always a 2D list for consistency\n",
    "if num_rows == 1:\n",
    "    axes = np.array([axes])  # Convert to 2D array\n",
    "if num_cols == 1:\n",
    "    axes = np.array([[ax] for ax in axes])  # Convert to 2D array\n",
    "\n",
    "# Perform regression for each (Population, TreatmentAccess) group\n",
    "for i, population in enumerate(population_bins):\n",
    "    for j, treatment_access in enumerate(access_rates):\n",
    "        ax = axes[i, j]  # Select subplot location\n",
    "\n",
    "        # Filter the data for the current Population and TreatmentAccess\n",
    "        group = means[(means[\"population\"] == population) & (means[\"access_rate\"] == treatment_access)]\n",
    "\n",
    "        if group.empty:\n",
    "            ax.set_visible(False)  # Hide empty plots\n",
    "            continue\n",
    "\n",
    "        group = group.dropna(axis=0)  # drop any row in a nan column\n",
    "\n",
    "        X = group[[\"beta\"]].values\n",
    "        y = group[\"pfpr2to10\"].values\n",
    "\n",
    "        # 1. Linear Regression\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # 2. Polynomial Regression (degree 3)\n",
    "        poly_model3 = make_pipeline(PolynomialFeatures(3), LinearRegression())\n",
    "        poly_model3.fit(X, y)\n",
    "\n",
    "        # 3. Polynomial Regression (degree 5)\n",
    "        poly_model5 = make_pipeline(PolynomialFeatures(5), LinearRegression())\n",
    "        poly_model5.fit(X, y)\n",
    "\n",
    "        # 4. Spline Regression\n",
    "        # spline_model = UnivariateSpline(group['beta'], group['pfpr2to10'], s=50)\n",
    "\n",
    "        # Plot regression\n",
    "        sns.scatterplot(x=group[\"beta\"], y=group[\"pfpr2to10\"], ax=ax, label=\"Data\", color=\"black\")\n",
    "        ax.plot(group[\"beta\"], model.predict(X), color=\"red\", linestyle=\"dashed\", label=\"Linear\")\n",
    "        ax.plot(group[\"beta\"], poly_model3.predict(X), color=\"blue\", linestyle=\"dashed\", label=\"Poly (3)\")\n",
    "        ax.plot(group[\"beta\"], poly_model5.predict(X), color=\"green\", linestyle=\"dashed\", label=\"Poly (5)\")\n",
    "        # ax.plot(group['Beta'], spline_model(X), color='purple', linestyle=\"dashed\", label=\"Spline\")\n",
    "\n",
    "        # Setting titles & labels\n",
    "        ax.set_title(f\"Population : {population}, Access : {treatment_access}\")\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(\"pfpr2to10\")\n",
    "        if i == num_rows - 1:\n",
    "            ax.set_xlabel(\"Beta\")\n",
    "        ax.legend(fontsize=7)\n",
    "\n",
    "# Adjust layout\n",
    "plt.suptitle(\"Curve Fitting by Population & Treatment Access\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3310a37",
   "metadata": {},
   "source": [
    "Now we'll do a logarithemic fit for the beta value and attempt to fit a sigmoid curve. Again this is the model that typically works best for the data. The fit returned is a list of the parameters for the sigmoid function which can be easily serialized and saved to a file. The sigmoid function is defined as:\n",
    "\n",
    "$$s = \\frac{a}{1 + e^{-b(x - c)}}$$\n",
    "\n",
    "which in code is:\n",
    "\n",
    "```python\n",
    "def sigmoid(x, a, b, c):\n",
    "    return a / (1 + np.exp(-b * (x - c)))\n",
    "```\n",
    "where `a`, `b`, and `c` are the parameters of the sigmoid function. The `x` value is the beta value. The `a` parameter is the maximum value of the sigmoid function, the `b` parameter is the steepness of the curve, and the `c` parameter is the x-value of the sigmoid's midpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda2576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_map = calibrate.log_sigmoid_fit(\n",
    "    population_bins,\n",
    "    access_rates,\n",
    "    means,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd10f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = calibrate.plot_log_sigmoid(population_bins, access_rates, means, models_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a22d4",
   "metadata": {},
   "source": [
    "Write the calibrated log-sigmoid model to a file. The most straightforward way to do this is to serialize the model dictionary to a `.json` file. The built in `json` module easily handles this for Python dictionaries. This way the data is written out in a human readable and cross language format. One change that needs to be made is to convert the numpy arrays to lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca22e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rate in models_map.keys():\n",
    "    for pop in models_map[rate].keys():\n",
    "        model = models_map[rate][pop]\n",
    "        if model is None:\n",
    "            models_map[rate][pop] = [0, 0, 0]\n",
    "        else:\n",
    "            try:\n",
    "                models_map[rate][pop] = model.tolist()\n",
    "            except AttributeError:\n",
    "                pass  # in case the model is not a numpy array\n",
    "\n",
    "with open(os.path.join(\"data\", name, \"calibration\", \"models_map.json\"), 'w') as f:\n",
    "    json.dump(models_map, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855507fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Keep in mind that the built in JSON serialization and deserialization reads in all the values as strings. To get around this use `load_beta_model` from the `calibrate` module instead of the native `json.load()` function. This will convert the values back to numeric values and lists where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a8c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.masim_analysis import calibrate, utils\n",
    "import os\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b090131",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_map = calibrate.load_beta_model(os.path.join(\"data\", name, \"calibration\", \"models_map.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f721fd",
   "metadata": {},
   "source": [
    "We can then use this model to generate the beta map for the experimental simulation. The beta map is generated by taking the beta value that corresponds to the population size and treatment access rate of each pixel. This is done using the `create_beta_map` function, which takes the summarized data and generates a beta map for the experimental simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e425e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "population, _ = utils.read_raster(\"data/moz/moz_population.asc\")\n",
    "prevalence, _ = utils.read_raster(\"data/moz/moz_pfpr210.asc\")\n",
    "treatment, _ =  utils.read_raster(\"data/moz/moz_treatmentseeking.asc\")\n",
    "beta_map = calibrate.create_beta_map(models_map, population, treatment, prevalence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37277b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(beta_map, cmap=\"hot\")\n",
    "plt.colorbar(label=\"Beta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7286aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.write_raster(beta_map, os.path.join(\"data\", name, f\"{name}_beta.asc\"), 195196.26821073, 7004178.4200866)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a878ac6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Validation\n",
    "\n",
    "The validation process is similar to the calibration process except that we now use the fitted modelled beta map to attempt to recreate the prevelance map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00354546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from masim_analysis import configure, commands\n",
    "import os\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "125c4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the previously saved strategy and event files\n",
    "strategy_db = yaml.load(open(os.path.join(\"conf\", name, \"test\", \"strategy_db.yaml\"), \"r\"))\n",
    "events = yaml.load(open(os.path.join(\"conf\", name, \"test\", \"events.yaml\"), \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d033df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = configure.configure(\n",
    "    country_code=name,\n",
    "    birth_rate=birth_rate,\n",
    "    initial_age_structure=initial_age_structure,\n",
    "    age_distribution=age_distribution,\n",
    "    death_rates=death_rate,\n",
    "    starting_date=date(calibration_year - 11, 1, 1),\n",
    "    start_of_comparison_period=date(calibration_year, 1, 1),\n",
    "    ending_date = date(calibration_year + 1, 1, 1),\n",
    "    strategy_db=strategy_db,\n",
    "    #calibration_str=f\"validation\",\n",
    "    calibration=False,\n",
    ")\n",
    "params['events'].extend(events)\n",
    "#params['artificial_rescaling_of_population_size'] = 0\n",
    "\n",
    "yaml.dump(params, open(os.path.join(\"conf\", name, f\"{name}_validation.yml\"), \"w\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e3121ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from masim_analysis import commands\n",
    "file, cmds = commands.generate_commands(os.path.join(\"conf\", name, f\"{name}_validation.yml\"), os.path.join(\"output\", name, f\"{name}_validation\"), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58e2335",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"moz_validation.txt\", \"w\") as f:\n",
    "    for cmd in cmds:\n",
    "        f.write(cmd + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93ebba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "commands.generate_job_file(\n",
    "    \"moz_validation.txt\",\n",
    "    \"validation\",\n",
    "    cores_override=4,\n",
    "    nodes_override=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7762ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(cmds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8680bc26",
   "metadata": {},
   "source": [
    "Alright I'm having some issues with the validation yaml file. I'm going to use the section below to test out the configuration process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ff7d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_db = configure.load_yaml(\"templates/drug_db.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d079785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "therapy_db = configure.load_yaml(\"templates/therapy_db.yml\")\n",
    "therapy_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac63d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = configure.load_yaml(\"conf/moz/validation2.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d8d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e9e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val4 = {'days_between_notifications': 30, 'initial_seed_number': 0, 'connection_string': 'host=masimdb.vmhost.psu.edu dbname=moznda user=sim password=sim connect_timeout=60', 'record_genome_db': True, 'report_frequency': 30, 'starting_date': '2003/1/1', 'start_of_comparison_period': '2020/1/1', 'ending_date': '2035/1/1', 'start_collect_data_day': 1826, 'number_of_tracking_days': 11, 'transmission_parameter': 0.55, 'number_of_age_classes': 15, 'age_structure': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 15, 20, 60, 100], 'initial_age_structure': [1, 5, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 69, 100], 'artificial_rescaling_of_population_size': 0.25, 'raster_db': {'population_raster': './data/moz/moz_population.asc', 'district_raster': './data/moz/moz_districts.asc', 'pr_treatment_under5': './data/moz/moz_treatmentseeking.asc', 'pr_treatment_over5': './data/moz/moz_treatmentseeking.asc', 'beta_raster': './data/moz/moz_beta.asc', 'cell_size': 5, 'age_distribution_by_location': [[0.037, 0.132, 0.161, 0.142, 0.09, 0.086, 0.07, 0.052, 0.044, 0.044, 0.031, 0.041, 0.024, 0.017, 0.013, 0.017]], 'p_treatment_for_less_than_5_by_location': [-1], 'p_treatment_for_more_than_5_by_location': [-1], 'beta_by_location': [0.55]}, 'seasonal_info': {'enable': False, 'mode': 'rainfall', 'rainfall': {'filename': './data/moz/moz_seasonality.csv', 'period': 365}}, 'spatial_model': {'name': 'Wesolowski', 'Wesolowski': {'kappa': 0.01093251, 'alpha': 0.22268982, 'beta': 0.14319618, 'gamma': 0.83741484}}, 'birth_rate': 0.0288, 'death_rate_by_age_class': [0.049744, 0.064331, 0.064331, 0.064331, 0.064331, 0.00359, 0.00361, 0.00365, 0.00379, 0.00379, 0.133, 0.133, 0.0174, 0.0174, 0.0174], 'mortality_when_treatment_fail_by_age_class': [0.04, 0.02, 0.02, 0.02, 0.02, 0.004, 0.004, 0.004, 0.004, 0.004, 0.004, 0.001, 0.001, 0.001, 0.001], 'parasite_density_level': {'log_parasite_density_cured': -4.699, 'log_parasite_density_from_liver': -2.0, 'log_parasite_density_asymptomatic': 3, 'log_parasite_density_clinical': 4.301, 'log_parasite_density_clinical_from': 3.301, 'log_parasite_density_clinical_to': 5.301, 'log_parasite_density_detectable': 1.0, 'log_parasite_density_detectable_pfpr': 1.699, 'log_parasite_density_pyrogenic': 3.398}, 'immune_system_information': {'b1': 0.00125, 'b2': 0.0025, 'duration_for_naive': 300, 'duration_for_fully_immune': 60, 'mean_initial_condition': 0.1, 'sd_initial_condition': 0.1, 'immune_inflation_rate': 0.01, 'max_clinical_probability': 0.99, 'immune_effect_on_progression_to_clinical': 4, 'age_mature_immunity': 10, 'factor_effect_age_mature_immunity': 1, 'midpoint': 0.4}, 'circulation_info': {'max_relative_moving_value': 35, 'number_of_moving_levels': 100, 'moving_level_distribution': {'distribution': 'Gamma', 'Exponential': {'scale': 0.17}, 'Gamma': {'mean': 5, 'sd': 10}}, 'circulation_percent': 0.00336, 'length_of_stay': {'mean': 5, 'sd': 10}}, 'initial_parasite_info': [{'location_id': -1, 'parasite_info': [{'parasite_type_id': 32, 'prevalence': 0.05}, {'parasite_type_id': 36, 'prevalence': 0.05}]}], 'events': [{'name': 'turn_off_mutation', 'info': [{'day': '2003/1/1'}]}], 'drug_db': {0: {'name': 'ART', 'half_life': 0.0, 'maximum_parasite_killing_rate': 0.999, 'n': 25, 'age_specific_drug_concentration_sd': [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4], 'mutation_probability': 0.005, 'affecting_loci': [2], 'selecting_alleles': [[1]], 'k': 4, 'EC50': {'..0..': 0.75, '..1..': 1.2}}, 1: {'name': 'LUM', 'half_life': 4.5, 'maximum_parasite_killing_rate': 0.99, 'n': 20, 'age_specific_drug_concentration_sd': [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4], 'mutation_probability': 0.005, 'affecting_loci': [0, 1], 'selecting_alleles': [[0], [0, 2, 3, 4, 6, 7]], 'k': 4, 'EC50': {'00...': 0.8, '01...': 0.67, '02...': 0.9, '03...': 0.8, '04...': 1.0, '05...': 0.87, '06...': 1.1, '07...': 1.0, '10...': 0.75, '11...': 0.6, '12...': 0.85, '13...': 0.75, '14...': 0.95, '15...': 0.8, '16...': 1.05, '17...': 0.95}}, 2: {'name': 'QUIN', 'half_life': 18, 'maximum_parasite_killing_rate': 0.9, 'n': 3, 'age_specific_drug_concentration_sd': [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4], 'mutation_probability': 0.0, 'affecting_loci': [], 'selecting_alleles': [], 'k': 4, 'EC50': {'0....': 1.41, '1....': 1.41}}, 3: {'name': 'PQ', 'half_life': 28.0, 'maximum_parasite_killing_rate': 0.9, 'n': 15, 'age_specific_drug_concentration_sd': [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4], 'mutation_probability': 0.005, 'affecting_loci': [3], 'selecting_alleles': [[1]], 'resistant_factor': [[1]], 'k': 4, 'EC50': {'...0.': 0.58, '...1.': 1.4}}, 4: {'name': 'amodiaquine', 'half_life': 9.0, 'maximum_parasite_killing_rate': 0.95, 'n': 19, 'age_specific_drug_concentration_sd': [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4], 'mutation_probability': 0.005, 'affecting_loci': [0, 1], 'selecting_alleles': [[1], [0, 1, 3, 4, 5, 7]], 'k': 4, 'EC50': {'00...': 0.62, '01...': 0.85, '02...': 0.5, '03...': 0.775, '04...': 0.62, '05...': 0.85, '06...': 0.5, '07...': 0.775, '10...': 0.7, '11...': 0.9, '12...': 0.65, '13...': 0.82, '14...': 0.7, '15...': 0.9, '16...': 0.65, '17...': 0.82}}}, 'tf_testing_day': 28, 'therapy_db': {0: {'drug_id': [0, 1], 'dosing_days': [3]}, 1: {'drug_id': [0], 'dosing_days': [3]}, 2: {'drug_id': [2], 'dosing_days': [7]}, 3: {'drug_id': [0, 3], 'dosing_days': [3]}, 4: {'drug_id': [0, 1], 'dosing_days': [4]}, 5: {'drug_id': [0, 1], 'dosing_days': [5]}, 6: {'drug_id': [0, 4], 'dosing_days': [3]}, 7: {'drug_id': [0, 1, 4], 'dosing_days': [3]}}, 'strategy_db': {0: {'name': 'moznda-Baseline', 'type': 'MFT', 'therapy_ids': [0], 'distribution': [1]}, 1: {'name': 'moznda-AL4', 'type': 'MFT', 'therapy_ids': [6], 'distribution': [1]}, 2: {'name': 'moznda-AL5', 'type': 'MFT', 'therapy_ids': [5], 'distribution': [1]}, 3: {'name': 'moznda-ASAQ', 'type': 'MFT', 'therapy_ids': [6], 'distribution': [1]}, 4: {'name': 'moznda-DHA-PPQ', 'type': 'MFT', 'therapy_ids': [3], 'distribution': [1]}, 5: {'name': 'AL25-ASAQ75', 'type': 'MFT', 'therapy_ids': [0, 6], 'distribution': [0.25, 0.75]}, 6: {'name': 'AL50-ASAQ50', 'type': 'MFT', 'therapy_ids': [0, 6], 'distribution': [0.5, 0.5]}, 7: {'name': 'AL75-ASAQ25', 'type': 'MFT', 'therapy_ids': [0, 6], 'distribution': [0.75, 0.25]}, 8: {'name': 'AL25-DHA-PPQ75', 'type': 'MFT', 'therapy_ids': [0, 3], 'distribution': [0.25, 0.75]}, 9: {'name': 'AL50-DHA-PPQ50', 'type': 'MFT', 'therapy_ids': [0, 3], 'distribution': [0.5, 0.5]}, 10: {'name': 'AL75-DHA-PPQ25', 'type': 'MFT', 'therapy_ids': [0, 3], 'distribution': [0.75, 0.25]}, 11: {'name': 'ASAQ25-DHA-PPQ75', 'type': 'MFT', 'therapy_ids': [6, 3], 'distribution': [0.25, 0.75]}, 12: {'name': 'ASAQ50-DHA-PPQ50', 'type': 'MFT', 'therapy_ids': [6, 3], 'distribution': [0.5, 0.5]}, 13: {'name': 'ASAQ75-DHA-PPQ25', 'type': 'MFT', 'therapy_ids': [6, 3], 'distribution': [0.75, 0.25]}}, 'initial_strategy_id': 0, 'days_to_clinical_under_five': 4, 'days_to_clinical_over_five': 6, 'days_mature_gametocyte_under_five': 4, 'days_mature_gametocyte_over_five': 6, 'relative_bitting_info': {'max_relative_biting_value': 35, 'number_of_biting_levels': 100, 'biting_level_distribution': {'distribution': 'Gamma', 'Exponential': {'scale': 0.17}, 'Gamma': {'mean': 5, 'sd': 10}}}, 'gametocyte_level_under_artemisinin_action': 1.0, 'gametocyte_level_full': 1.0, 'relative_infectivity': {'sigma': 3.91, 'ro': 0.00031, 'blood_meal_volume': 3}, 'p_relapse': 0.01, 'relapse_duration': 30, 'relapseRate': 4.4721, 'update_frequency': 7, 'allow_new_coinfection_to_cause_symtoms': True, 'using_free_recombination': True, 'tf_window_size': 60, 'fraction_mosquitoes_interrupted_feeding': 0.0, 'inflation_factor': 0.01, 'genotype_info': {'loci': [{'locus_name': 'pfcrt', 'position': 0, 'alleles': [{'value': 0, 'allele_name': 'K76', 'short_name': 'K', 'can_mutate_to': [1], 'mutation_level': 0, 'daily_cost_of_resistance': 0.0}, {'value': 1, 'allele_name': '76T', 'short_name': 'T', 'can_mutate_to': [0], 'mutation_level': 1, 'daily_cost_of_resistance': 0.0005}]}, {'locus_name': 'pfmdr1', 'position': 1, 'alleles': [{'value': 0, 'allele_name': 'N86 Y184 one copy of pfmdr1', 'short_name': 'NY--', 'can_mutate_to': [1, 2, 4], 'mutation_level': 0, 'daily_cost_of_resistance': 0.0}, {'value': 1, 'allele_name': '86Y Y184 one copy of pfmdr1', 'short_name': 'YY--', 'can_mutate_to': [3, 0, 5], 'mutation_level': 1, 'daily_cost_of_resistance': 0.0005}, {'value': 2, 'allele_name': 'N86 184F one copy of pfmdr1', 'short_name': 'NF--', 'can_mutate_to': [3, 0, 6], 'mutation_level': 1, 'daily_cost_of_resistance': 0.0005}, {'value': 3, 'allele_name': '86Y 184F one copy of pfmdr1', 'short_name': 'YF--', 'can_mutate_to': [1, 2, 7], 'mutation_level': 2, 'daily_cost_of_resistance': 0.00099975}, {'value': 4, 'allele_name': 'N86 Y184 2 copies of pfmdr1', 'short_name': 'NYNY', 'can_mutate_to': [0], 'mutation_level': 1, 'daily_cost_of_resistance': 0.005}, {'value': 5, 'allele_name': '86Y Y184 2 copies of pfmdr1', 'short_name': 'YYYY', 'can_mutate_to': [1], 'mutation_level': 2, 'daily_cost_of_resistance': 0.0055}, {'value': 6, 'allele_name': 'N86 184F 2 copies of pfmdr1', 'short_name': 'NFNF', 'can_mutate_to': [2], 'mutation_level': 2, 'daily_cost_of_resistance': 0.0055}, {'value': 7, 'allele_name': '86Y 184F 2 copies of pfmdr1', 'short_name': 'YFYF', 'can_mutate_to': [3], 'mutation_level': 3, 'daily_cost_of_resistance': 0.006}]}, {'locus_name': 'K13 Propeller', 'position': 2, 'alleles': [{'value': 0, 'allele_name': 'R561', 'short_name': 'R', 'can_mutate_to': [1], 'mutation_level': 0, 'daily_cost_of_resistance': 0.0}, {'value': 1, 'allele_name': '561H', 'short_name': 'H', 'can_mutate_to': [0], 'mutation_level': 1, 'daily_cost_of_resistance': 0.0005}]}, {'locus_name': 'Plasmepsin 2-3', 'position': 3, 'alleles': [{'value': 0, 'allele_name': 'Plasmepsin 2-3 one copy', 'short_name': '1', 'can_mutate_to': [1], 'mutation_level': 0, 'daily_cost_of_resistance': 0.0}, {'value': 1, 'allele_name': 'Plasmepsin 2-3 2 copies', 'short_name': '2', 'can_mutate_to': [0], 'mutation_level': 1, 'daily_cost_of_resistance': 0.0005}]}]}, 'using_age_dependent_bitting_level': False, 'using_variable_probability_infectious_bites_cause_infection': False, 'mda_therapy_id': 8, 'age_bracket_prob_individual_present_at_mda': [10, 40], 'mean_prob_individual_present_at_mda': [0.85, 0.75, 0.85], 'sd_prob_individual_present_at_mda': [0.3, 0.3, 0.3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb9779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"conf/moz/validation4.yml\", \"w\") as f:\n",
    "    yaml.dump(val4, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7856312",
   "metadata": {},
   "source": [
    "Next up plot the predicted vs true prevelence values. Several plots, sort by population size. There will be some misfit sections of the modeled data where there are zero population in the model but actual population in the data. The Malaria Atlas project sometimes project prevelence where there is no population. Use median simulation validation results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
